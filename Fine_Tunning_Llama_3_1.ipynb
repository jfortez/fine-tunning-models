{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1JuZMRZfpI5h"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unsloth in ./.venv/lib/python3.11/site-packages (2025.8.1)\n",
            "Requirement already satisfied: datasets in ./.venv/lib/python3.11/site-packages (3.6.0)\n",
            "Requirement already satisfied: dotenv in ./.venv/lib/python3.11/site-packages (0.9.9)\n",
            "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.11/site-packages (0.34.3)\n",
            "Requirement already satisfied: torch in ./.venv/lib/python3.11/site-packages (2.7.1)\n",
            "Requirement already satisfied: unsloth_zoo>=2025.8.1 in ./.venv/lib/python3.11/site-packages (from unsloth) (2025.8.1)\n",
            "Requirement already satisfied: xformers>=0.0.27.post2 in ./.venv/lib/python3.11/site-packages (from unsloth) (0.0.31.post1)\n",
            "Requirement already satisfied: bitsandbytes in ./.venv/lib/python3.11/site-packages (from unsloth) (0.46.1)\n",
            "Requirement already satisfied: triton>=3.0.0 in ./.venv/lib/python3.11/site-packages (from unsloth) (3.3.1)\n",
            "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from unsloth) (25.0)\n",
            "Requirement already satisfied: tyro in ./.venv/lib/python3.11/site-packages (from unsloth) (0.9.27)\n",
            "Requirement already satisfied: transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,>=4.51.3 in ./.venv/lib/python3.11/site-packages (from unsloth) (4.55.0)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in ./.venv/lib/python3.11/site-packages (from unsloth) (0.2.0)\n",
            "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from unsloth) (4.67.1)\n",
            "Requirement already satisfied: psutil in ./.venv/lib/python3.11/site-packages (from unsloth) (7.0.0)\n",
            "Requirement already satisfied: wheel>=0.42.0 in ./.venv/lib/python3.11/site-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from unsloth) (2.3.2)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in ./.venv/lib/python3.11/site-packages (from unsloth) (1.9.0)\n",
            "Requirement already satisfied: trl!=0.15.0,!=0.19.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in ./.venv/lib/python3.11/site-packages (from unsloth) (0.21.0)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in ./.venv/lib/python3.11/site-packages (from unsloth) (0.17.0)\n",
            "Requirement already satisfied: protobuf in ./.venv/lib/python3.11/site-packages (from unsloth) (6.31.1)\n",
            "Requirement already satisfied: hf_transfer in ./.venv/lib/python3.11/site-packages (from unsloth) (0.1.9)\n",
            "Requirement already satisfied: diffusers in ./.venv/lib/python3.11/site-packages (from unsloth) (0.34.0)\n",
            "Requirement already satisfied: torchvision in ./.venv/lib/python3.11/site-packages (from unsloth) (0.22.1)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.11/site-packages (from datasets) (21.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (from datasets) (2.3.1)\n",
            "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.11/site-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: xxhash in ./.venv/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in ./.venv/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.11/site-packages (from dotenv) (1.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (1.1.7)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.11/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./.venv/lib/python3.11/site-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./.venv/lib/python3.11/site-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./.venv/lib/python3.11/site-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./.venv/lib/python3.11/site-packages (from torch) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./.venv/lib/python3.11/site-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./.venv/lib/python3.11/site-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./.venv/lib/python3.11/site-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./.venv/lib/python3.11/site-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./.venv/lib/python3.11/site-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./.venv/lib/python3.11/site-packages (from torch) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./.venv/lib/python3.11/site-packages (from torch) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./.venv/lib/python3.11/site-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./.venv/lib/python3.11/site-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./.venv/lib/python3.11/site-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in ./.venv/lib/python3.11/site-packages (from triton>=3.0.0->unsloth) (80.9.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.11/site-packages (from accelerate>=0.34.1->unsloth) (0.6.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,>=4.51.3->unsloth) (2025.7.34)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.11/site-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,>=4.51.3->unsloth) (0.21.4)\n",
            "Requirement already satisfied: cut_cross_entropy in ./.venv/lib/python3.11/site-packages (from unsloth_zoo>=2025.8.1->unsloth) (25.1.1)\n",
            "Requirement already satisfied: pillow in ./.venv/lib/python3.11/site-packages (from unsloth_zoo>=2025.8.1->unsloth) (11.3.0)\n",
            "Requirement already satisfied: msgspec in ./.venv/lib/python3.11/site-packages (from unsloth_zoo>=2025.8.1->unsloth) (0.19.0)\n",
            "Requirement already satisfied: importlib_metadata in ./.venv/lib/python3.11/site-packages (from diffusers->unsloth) (8.7.0)\n",
            "Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.11/site-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in ./.venv/lib/python3.11/site-packages (from tyro->unsloth) (0.17.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in ./.venv/lib/python3.11/site-packages (from tyro->unsloth) (14.1.0)\n",
            "Requirement already satisfied: shtab>=1.5.6 in ./.venv/lib/python3.11/site-packages (from tyro->unsloth) (1.7.2)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in ./.venv/lib/python3.11/site-packages (from tyro->unsloth) (4.4.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.11/site-packages (from rich>=11.1.0->tyro->unsloth) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.11/site-packages (from rich>=11.1.0->tyro->unsloth) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install unsloth datasets dotenv huggingface_hub torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316,
          "referenced_widgets": [
            "df8cb7c4b09d4b1ab1d49591f89b23fd",
            "f1969ec790764fb7878ec570464cf844",
            "9560fbbb2020494284cfa52f3aba0ced",
            "241be6f77a2247999a6adb196047b6fd",
            "556bbe564f4543b7a63ef1cf058623b7",
            "ab204469a3754ed2b9f220b0d74e0d63",
            "5801c065fa6e415d97ef3891d5c3d89a",
            "87748d6d8c6c4fd7abb62ffe71bb126a",
            "acef14ef3ea74d389265c95cec7dd5f7",
            "77cfc917e3f4439d978f9290ad2d1ffd",
            "57a5483bf4fd4726a4d0d434e7aca338",
            "818881c0a28e4ca6a325dcc3c2caa14f",
            "617169c3b0af4b00a66d621469e87b94",
            "364d047cf1044594a5336d4659f24074",
            "4d48b315656443ccb7e5656ea940f017",
            "1d4995e0f8684ee7acefebd6fd1caca3",
            "4531109e7a27424b87ba16fe473a7e22",
            "1ff388a8c8444254b1b218225af1e506",
            "b9bda7063e364fdc8f371fe6f60ab425",
            "f8f3ca71aba6495fa9ebb32f3bd60ab3",
            "e7548e757996416e99109a3b6c55fd16",
            "d18f9537929e4b5fb5009ce426791da5",
            "b21d6f7ebf6a4273b1d31645ca13ebf1",
            "e65fdd6951334a2a8f42d431254da207",
            "28dee16d89d248a1ac7606faba9de6cd",
            "d5359f910f6f497494fe09d54d3c80eb",
            "082c7bcffd274215a1a3b03a56c0a303",
            "713e05a8285c4b338921a63e82a30694",
            "42412db9b76f4bbfa67056c8acb9bab5",
            "59056bccf795490d98664aadabc619c0",
            "49ff4a08713f49f49b687a3dd6ca1f0d",
            "4981fbe58c31449dad4864b8ba2c151c",
            "f29c365803104ffbaa3b50f167fd08ec",
            "257afb34467c464daf3df45a42f697c3",
            "6d90e48cf4bd46239267f2515133d3d1",
            "483c113f25a845d392ccc456d3046a40",
            "493abb845f1344f28675d00bdb6e02f5",
            "8047fc11d6dc4635b42dd0ea6fb69bde",
            "d40768f42f004960adb76a6aea583da6",
            "1e6127c6367340f1b4954499dd163b41",
            "964d4515de7945b5ac82c9372a08031a",
            "186daa6e79e9464aa04107b4776702d8",
            "4dfa74df37a34185ac5bdf1a2fc881e5",
            "fdb9ff25e5054095ad4a784a5b0099ee",
            "6598808733c9480eaf9623b1e4ce0182",
            "a81cb0643de7456890f9e272c59fb318",
            "3092d6eb5a2d4bfc93704bf02e46afb2",
            "4a0e30b9837640b7afc17fd47075a4f8",
            "709aebbe21a54336a907b3dab1872996",
            "2ea103fd703c4917ace8df3ce4dc4bcd",
            "8bcf6c8b0701413696ac7af3a6febeed",
            "9d852f519cb449e993d54c9d3ac47d48",
            "85daa7a105c64380a682620a9cb2ee96",
            "f8d5d268520e41e1a2d9799282b26e9d",
            "768258bc409b4e9d808a97198cd173e0"
          ]
        },
        "id": "SAF4wyKMriWq",
        "outputId": "b1371e9e-776f-444c-d085-aa3f78349c08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.8.1: Fast Llama patching. Transformers: 4.55.0.\n",
            "   \\\\   /|    NVIDIA A40. Num GPUs = 1. Max memory: 44.339 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.27s/it]\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model_name = \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n",
        "# model_name = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(   \n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75Liszct9aZs",
        "outputId": "e24d605e-a44c-4bc0-9953-c89e42aba24b"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save a copy because apply_chat_template() has in-place modifications\n",
        "import copy\n",
        "\n",
        "tokenizer_orig = copy.deepcopy(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Download Dataset And Formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "81bcbbc9d7574220bf8f9b3ed40c7467",
            "b86bb63b80fb41c0bcdbd4fcfca1b6fe",
            "314fe0becd0248648b6c39cc3eb839d8",
            "17bbd1d8e24c4c2494d3b1e5efc9a7c3",
            "6b096db4c2454b9eafaa35c6e4b05566",
            "f59d99fd1d0541d08b2d75528c6dcf0c",
            "f7bc9724ea4e4592a3374724078b13aa",
            "113ba718a2c146e4970546a3312e6230",
            "c9364c8b32fd40168e6735f39b80a210",
            "68acfb4bf95547848a0fced9e6c0aa29",
            "62c9849bf9fe41e3afa4f528bfa321f0",
            "2121b6cbeb08488db8f1c4ddd068593c",
            "8ff22b6aec10444d852dc071374ecd8e",
            "ef180749a010442e8c54a9b68e2c47cc",
            "fafc4021a849469fb59b5df0987fd0b3",
            "f001e1bd936b4c93bb5328f09ca208c1",
            "01353edbf0534514a8762146b84f55e5",
            "32183ededf184d2da61eb6529c36f4ec",
            "afa1b973e435498e9e4d4cad1b5ebfc2",
            "021a36dd1b7f49729060be7457e40bf2",
            "efd013183fd64920b0295e5a479e0868",
            "5f07a4774b0d4863b333dc5b4ea3555a",
            "bd014ce911cc4905bffa72939dca8cbc",
            "49df1af933914d1aab887f876b78012a",
            "5cdc6d3a8d274913ada4ee4cb2b0715e",
            "805610edc25e4de1a48dbb6f67f27a11",
            "a2f55a0b48894a46af1ee75cf8ae55dc",
            "0cb7c78341d94916b03a73a18a90d124",
            "34bac034017a43dba58c28c7e520f8b1",
            "060dbb641b324c7092f1aa46268ae354",
            "f5b42bc85c374d5bb49cdfd77a636d73",
            "0e13d385ea134c2ea25eb5b0d4a4578b",
            "854f5c279f8b4984bd036df1df3d9c92"
          ]
        },
        "id": "uBULf6z9pW4G",
        "outputId": "8a7f3e83-027d-4a3a-c206-4e7acde0a274"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['messages', 'topic'],\n",
              "    num_rows: 9938\n",
              "})"
            ]
          },
          "execution_count": 154,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_orig = load_dataset(\"jeanmcm/b_risks\", split=\"train\")\n",
        "dataset_orig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpC2ysGJ97I8",
        "outputId": "6cfbac2b-018a-4067-cf79-9a29ca42627c"
      },
      "outputs": [],
      "source": [
        "def validate_messages_format(messages):\n",
        "    \"\"\"\n",
        "    Validates the format of the messages list and counts interactions.\n",
        "\n",
        "    Args:\n",
        "        messages (list): A list of dictionaries representing messages.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (bool, int) - True if the format is valid and the number of interactions (pairs of user/assistant messages), False and 0 otherwise.\n",
        "    \"\"\"\n",
        "    if len(messages) % 2 != 0:\n",
        "        return False, 0  # Length must be even\n",
        "\n",
        "    user_count = 0\n",
        "    assistant_count = 0\n",
        "    for i, message in enumerate(messages):\n",
        "        if i % 2 == 0:\n",
        "            if message.get('role') != 'user':\n",
        "                return False, 0  # Even indexed messages should have 'user' role\n",
        "            user_count += 1\n",
        "        else:\n",
        "            if message.get('role') != 'assistant':\n",
        "                return False, 0  # Odd indexed messages should have 'assistant' role\n",
        "            assistant_count += 1\n",
        "\n",
        "    if user_count == assistant_count:\n",
        "        return True, user_count  # Return True and the number of interactions\n",
        "    else:\n",
        "        return False, 0\n",
        "\n",
        "def clean_invalid_messages(messages):\n",
        "    \"\"\"\n",
        "    Cleans invalid message sequences by keeping only the valid alternating pairs.\n",
        "\n",
        "    Args:\n",
        "        messages (list): A list of dictionaries representing messages.\n",
        "\n",
        "    Returns:\n",
        "        list: A list containing only the valid alternating user/assistant message pairs.\n",
        "    \"\"\"\n",
        "    cleaned_messages = []\n",
        "    for i in range(0, len(messages) - 1, 2):\n",
        "        if messages[i].get('role') == 'user' and messages[i+1].get('role') == 'assistant':\n",
        "            cleaned_messages.append(messages[i])\n",
        "            cleaned_messages.append(messages[i+1])\n",
        "        else:\n",
        "            # Stop if the pattern is broken\n",
        "            break\n",
        "    return cleaned_messages\n",
        "\n",
        "\n",
        "def validate_dataset(dataset):\n",
        "    valid_dataset = []\n",
        "    invalid_dataset = []\n",
        "\n",
        "    # Iterate through the dataset and validate the messages\n",
        "    for example in dataset:\n",
        "        is_valid, _ = validate_messages_format(example['messages'])\n",
        "        if is_valid:\n",
        "            valid_dataset.append(example)\n",
        "        else:\n",
        "            invalid_dataset.append(example)\n",
        "\n",
        "    print(f\"Total number of valid examples: {len(valid_dataset)}\")\n",
        "    print(f\"Total number of invalid examples: {len(invalid_dataset)}\")\n",
        "\n",
        "    cleaned_invalid_dataset = []\n",
        "    # Clean the invalid messages and re-validate\n",
        "    for example in invalid_dataset:\n",
        "        cleaned_messages = clean_invalid_messages(example['messages'])\n",
        "        is_valid, interactions = validate_messages_format(cleaned_messages)\n",
        "        if is_valid:\n",
        "            cleaned_invalid_dataset.append({'messages': cleaned_messages, 'topic': example['topic']})\n",
        "\n",
        "\n",
        "    print(f\"\\nTotal number of cleaned invalid examples that are now valid: {len(cleaned_invalid_dataset)}\")\n",
        "\n",
        "    combined_dataset = valid_dataset + cleaned_invalid_dataset\n",
        "\n",
        "    print(f\"Total number of examples in the combined dataset: {len(combined_dataset)}\")\n",
        "\n",
        "    return combined_dataset        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "Eres un asistente con amplios conocimientos en el tema: {}. \n",
        "Responde de forma clara, precisa y útil, adaptando la complejidad según el nivel del usuario.\n",
        "\"\"\"\n",
        "\n",
        "def format_dataset(dataset,split_pairs=False, show_assistant_message=True):\n",
        "    \n",
        "    valid_dataset = validate_dataset(dataset)\n",
        "    \n",
        "    if not show_assistant_message and not split_pairs:\n",
        "        return Dataset.from_list([{'messages': conversation} for conversation in valid_dataset])\n",
        "    \n",
        "    conversations = []\n",
        "\n",
        "    for example in valid_dataset:\n",
        "        messages = example['messages']\n",
        "        topic = example['topic'] \n",
        "        system_message = {\"role\": \"system\", \"content\": system_prompt.format(topic)} \n",
        "\n",
        "        if split_pairs:\n",
        "            for i in range(0, len(messages), 2):\n",
        "                if i + 1 < len(messages): # Ensure there's a next message for a pair\n",
        "                    # Prepend the system message to the interaction pair\n",
        "                    retrievals = [system_message, messages[i], messages[i+1]]\n",
        "                    if not show_assistant_message:\n",
        "                        retrievals.pop(0)\n",
        "                    conversations.append(retrievals)\n",
        "        else:\n",
        "            conversations.append([system_message,*messages])\n",
        "    \n",
        "     \n",
        "    return Dataset.from_list([{'messages': conversation} for conversation in conversations])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of valid examples: 7578\n",
            "Total number of invalid examples: 2360\n",
            "\n",
            "Total number of cleaned invalid examples that are now valid: 2360\n",
            "Total number of examples in the combined dataset: 9938\n"
          ]
        }
      ],
      "source": [
        "dataset = format_dataset(dataset_orig,\n",
        "                         show_assistant_message=True,\n",
        "                         split_pairs=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'content': '\\nEres un asistente con amplios conocimientos en el tema: Calificaciones de peritos valuadores en finanzas. \\nResponde de forma clara, precisa y útil, adaptando la complejidad según el nivel del usuario.\\n',\n",
              "  'role': 'system'},\n",
              " {'content': 'He estado estudiando normas de control en el sector financiero, especialmente las relacionadas con los peritos valuadores. ¿Podrías explicarme las calificaciones que otorga la Superintendencia de Bancos y por qué son importantes?',\n",
              "  'role': 'user'},\n",
              " {'content': 'Claro, la Superintendencia de Bancos otorga calificaciones a los peritos valuadores que son esenciales para garantizar que las evaluaciones de activos sean precisas y confiables. Estas calificaciones aseguran que los valuadores cumplan con ciertos requisitos y estándares, lo que es crucial en el contexto financiero, donde las decisiones pueden depender del valor estimado de los activos. Una buena calificación indica que el perito tiene la formación y la experiencia necesarias para realizar valuaciones correctas.',\n",
              "  'role': 'assistant'}]"
            ]
          },
          "execution_count": 158,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[0]['messages']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Llama 3.1 Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vsqiHWoJjkgt"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\", # change this to the right chat_template name\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "4bf4dee1ea944b2ea71ba30925639cf6",
            "d27ed6e473ee4f8f97ada5a6a181dd16",
            "175c9d72cc0249cc9e2ab1bc0d86405a",
            "4b067e2fbddb4d36824e44914b686898",
            "d419c950a6b94e8c9141e8e690148c83",
            "bebc043d89a74a3bb9f9f812807b8c84",
            "7e0ce4ea8d6243318f72cbfe1727c37b",
            "e1fd61cc64eb453394d31ecfbeedf9f4",
            "6c66d2d831774e749ccc6263a5339e7d",
            "328d5557de3c43ceb18a2cc3ca451025",
            "3dd0f9ac6bae447f94637fbb7a5362d2"
          ]
        },
        "id": "hFyp-a_ydtkt",
        "outputId": "55b22b46-fd13-410d-a38d-358c17cc0dae"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4bf4dee1ea944b2ea71ba30925639cf6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/3392 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "from unsloth import standardize_sharegpt\n",
        "\n",
        "\n",
        "# Convert the list of interaction sets back to a Dataset\n",
        "dataset = Dataset.from_list([{'messages': interaction_set} for interaction_set in interaction_pairs_with_system])\n",
        "\n",
        "# Adjust the formatting function to handle the new structure\n",
        "def formatting_prompts_func(examples):\n",
        "    # Each element in 'examples'['messages'] is now a list [system_message, user_message, assistant_message]\n",
        "    convos = examples[\"messages\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "# Apply the formatting function to the new dataset\n",
        "dataset = standardize_sharegpt(dataset)\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2fT1l69gZFC",
        "outputId": "acacdf83-2259-4891-f29b-5ea4bf2fd2ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'content': 'This conversation is about: Lavado de activos y sus métodos. ',\n",
              "  'role': 'system'},\n",
              " {'content': \"¿Qué es un 'embudo' en este contexto?\", 'role': 'user'},\n",
              " {'content': \"En este contexto, una 'cuenta de embudo' se refiere a la creación de una estructura financiera compleja diseñada para ocultar el origen ilícito de los fondos. Esta estructura puede incluir múltiples cuentas en diferentes países o entidades financieras, lo que dificulta su rastreo y seguimiento.\",\n",
              "  'role': 'assistant'}]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[5]['messages']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "Q3KgYe1wLQ8B",
        "outputId": "91747017-1ae0-4ad7-982c-631def39edb5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\nThis conversation is about: Lavado de activos y sus métodos. <|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n¿Qué es un 'embudo' en este contexto?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nEn este contexto, una 'cuenta de embudo' se refiere a la creación de una estructura financiera compleja diseñada para ocultar el origen ilícito de los fondos. Esta estructura puede incluir múltiples cuentas en diferentes países o entidades financieras, lo que dificulta su rastreo y seguimiento.<|eot_id|>\""
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[5]['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxZYiPp15YxK"
      },
      "source": [
        "## Alpaca Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8154794457f74509b2a5c6ea18c8027e",
            "33d0ff7b3dad4a719d5414cc77dd7250",
            "97525916b6464a19998a1ff62e810c1c",
            "6337963a2bd24138892255335ac44128",
            "888c066045fc44c19ca8c7c26b9b6ea4",
            "739dd44420774a4e89d3f4162c448186",
            "d4d21f5053ab44e5beaf6dda775b7e63",
            "b7f0b3dbc128479389f38d8b34dce034",
            "622204f814074b249422134e8d144f1f",
            "02fd407b25914c98993a4ac3d46eff8e",
            "bb27d759f4c54c30b5bc4cdb03902622"
          ]
        },
        "id": "FIBDGgVg5Wqb",
        "outputId": "4570b299-3671-4e14-d00e-054f1304d70e"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b1e35f91bc8e438f9fb839ab582387de",
            "1d6b0fe6e7454620b9380e1632b90e3d",
            "46bedf08f65644b58237ee1f6e18cc84",
            "fd2ebdc3e11844d99bbe1c22b022fc62",
            "90b604e36b2c4a639bead4d06e8a34e5",
            "eb30f78f3bc94e40a46574c546c9fd93",
            "587488a53a0c4d10a93836ad36bfe60a",
            "9d7eb01c0e5d44b0a1c06a6a2a59a2bf",
            "510c2f7c0f8f44fdab1eb7a1fe0d6799",
            "8a00f259cf9a48059b31bfaa2386dc17",
            "59bf80d05f654c44b87d3bfa1e2364a9"
          ]
        },
        "id": "4DNK_sPV6u6f",
        "outputId": "261f4826-bfa5-46ba-9a82-cc009f942de6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 3392/3392 [00:00<00:00, 27783.25 examples/s]\n"
          ]
        }
      ],
      "source": [
        "from unsloth import standardize_sharegpt\n",
        "\n",
        "\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"messages\"]\n",
        "    #texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    texts = [alpaca_prompt.format(convo[0][\"content\"], convo[1][\"content\"], convo[2]['content']) + EOS_TOKEN for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "# Convert the list of interaction sets back to a Dataset\n",
        "dataset = standardize_sharegpt(dataset)\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Chat Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 15124/15124 [00:00<00:00, 16930.49 examples/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['messages', 'conversations'],\n",
              "    num_rows: 15124\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def custom_conversations(example):\n",
        "  messages = example[\"messages\"]\n",
        "  messages_without_system = messages[1:]\n",
        "  example[\"conversations\"] = messages_without_system\n",
        "  return example\n",
        "\n",
        "dataset = dataset.map(custom_conversations)\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: We automatically added an EOS token to stop endless generations.\n",
            "Map: 100%|██████████| 15124/15124 [00:00<00:00, 21303.57 examples/s]\n"
          ]
        }
      ],
      "source": [
        "chat_template = \"\"\"Below are some instructions that describe some tasks. Write responses that appropriately complete each request.\n",
        "\n",
        "### Instruction:\n",
        "{INPUT}\n",
        "\n",
        "### Response:\n",
        "{OUTPUT}\"\"\"\n",
        "\n",
        "from unsloth import apply_chat_template\n",
        "\n",
        "\n",
        "dataset = apply_chat_template(\n",
        "    dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    chat_template=chat_template,\n",
        "    default_system_message = \"You are a helpful assistant\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'content': 'He estado estudiando normas de control en el sector financiero, especialmente las relacionadas con los peritos valuadores. ¿Podrías explicarme las calificaciones que otorga la Superintendencia de Bancos y por qué son importantes?',\n",
              "  'role': 'user'},\n",
              " {'content': 'Claro, la Superintendencia de Bancos otorga calificaciones a los peritos valuadores que son esenciales para garantizar que las evaluaciones de activos sean precisas y confiables. Estas calificaciones aseguran que los valuadores cumplan con ciertos requisitos y estándares, lo que es crucial en el contexto financiero, donde las decisiones pueden depender del valor estimado de los activos. Una buena calificación indica que el perito tiene la formación y la experiencia necesarias para realizar valuaciones correctas.',\n",
              "  'role': 'assistant'}]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[\"conversations\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-rVNGzn7beL",
        "outputId": "dcb9d546-a1c1-46d2-e82e-83e990e9c416"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<|begin_of_text|>Below are some instructions that describe some tasks. Write responses that appropriately complete each request.',\n",
              " '',\n",
              " '### Instruction:',\n",
              " 'He estado estudiando normas de control en el sector financiero, especialmente las relacionadas con los peritos valuadores. ¿Podrías explicarme las calificaciones que otorga la Superintendencia de Bancos y por qué son importantes?',\n",
              " '',\n",
              " '### Response:',\n",
              " 'Claro, la Superintendencia de Bancos otorga calificaciones a los peritos valuadores que son esenciales para garantizar que las evaluaciones de activos sean precisas y confiables. Estas calificaciones aseguran que los valuadores cumplan con ciertos requisitos y estándares, lo que es crucial en el contexto financiero, donde las decisiones pueden depender del valor estimado de los activos. Una buena calificación indica que el perito tiene la formación y la experiencia necesarias para realizar valuaciones correctas.<|end_of_text|>']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[0]['text'].split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQJXuW4g77lP",
        "outputId": "842918c6-78b6-4581-d33a-bdc9d2439601"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['messages', 'conversations', 'text'],\n",
              "    num_rows: 15124\n",
              "})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ChatML Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer_orig,\n",
        "    chat_template=\"chatml\",\n",
        "    #mapping={\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}\n",
        ")\n",
        "\n",
        "def apply_template(examples):\n",
        "    messages = examples[\"messages\"]\n",
        "    text = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False) for message in messages]\n",
        "    return {\"text\": text}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map:   8%|▊         | 2000/23968 [00:00<00:01, 15984.36 examples/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 23968/23968 [00:01<00:00, 18921.36 examples/s]\n"
          ]
        }
      ],
      "source": [
        "dataset = dataset.map(apply_template, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<|im_start|>system',\n",
              " '',\n",
              " 'Eres un asistente con amplios conocimientos en el tema: Calificaciones de peritos valuadores en finanzas. ',\n",
              " 'Responde de forma clara, precisa y útil, adaptando la complejidad según el nivel del usuario.',\n",
              " '<|im_end|>',\n",
              " '<|im_start|>user',\n",
              " 'He estado estudiando normas de control en el sector financiero, especialmente las relacionadas con los peritos valuadores. ¿Podrías explicarme las calificaciones que otorga la Superintendencia de Bancos y por qué son importantes?<|im_end|>',\n",
              " '<|im_start|>assistant',\n",
              " 'Claro, la Superintendencia de Bancos otorga calificaciones a los peritos valuadores que son esenciales para garantizar que las evaluaciones de activos sean precisas y confiables. Estas calificaciones aseguran que los valuadores cumplan con ciertos requisitos y estándares, lo que es crucial en el contexto financiero, donde las decisiones pueden depender del valor estimado de los activos. Una buena calificación indica que el perito tiene la formación y la experiencia necesarias para realizar valuaciones correctas.<|im_end|>',\n",
              " '']"
            ]
          },
          "execution_count": 161,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['text'][0].split('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tool Calling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_vector_sum(a: list[float], b: list[float]) -> list[float]:\n",
        "    \"\"\"\n",
        "    Performs element-wise addition of two numerical vectors.\n",
        "\n",
        "    Both vectors must be of the same length and contain numerical values.\n",
        "\n",
        "    Args:\n",
        "        a: First vector containing numerical values\n",
        "        b: Second vector containing numerical values\n",
        "\n",
        "    Returns:\n",
        "        Resulting vector where each element is the sum of corresponding elements in a and b\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If vectors have different lengths\n",
        "\n",
        "    Example:\n",
        "        >>> get_vector_sum([1, 2], [3, 4])\n",
        "        [4, 6]\n",
        "    \"\"\"\n",
        "    if len(a) != len(b):\n",
        "        raise ValueError(\"Vectors must be of the same length\")\n",
        "\n",
        "    return [x + y for x, y in zip(a, b)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_query = {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"Find the sum of a = [1, -1, 2] and b = [3, 0, -4].\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\", # change this to the right chat_template name\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Environment: ipython\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 26 July 2024\n",
            "\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Given the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\n",
            "\n",
            "Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.Do not use variables.\n",
            "\n",
            "{\n",
            "    \"type\": \"function\",\n",
            "    \"function\": {\n",
            "        \"name\": \"get_vector_sum\",\n",
            "        \"description\": \"Performs element-wise addition of two numerical vectors.\\n\\nBoth vectors must be of the same length and contain numerical values.\",\n",
            "        \"parameters\": {\n",
            "            \"type\": \"object\",\n",
            "            \"properties\": {\n",
            "                \"a\": {\n",
            "                    \"type\": \"array\",\n",
            "                    \"items\": {\n",
            "                        \"type\": \"number\"\n",
            "                    },\n",
            "                    \"description\": \"First vector containing numerical values\"\n",
            "                },\n",
            "                \"b\": {\n",
            "                    \"type\": \"array\",\n",
            "                    \"items\": {\n",
            "                        \"type\": \"number\"\n",
            "                    },\n",
            "                    \"description\": \"Second vector containing numerical values\"\n",
            "                }\n",
            "            },\n",
            "            \"required\": [\n",
            "                \"a\",\n",
            "                \"b\"\n",
            "            ]\n",
            "        },\n",
            "        \"return\": {\n",
            "            \"type\": \"array\",\n",
            "            \"items\": {\n",
            "                \"type\": \"number\"\n",
            "            },\n",
            "            \"description\": \"Resulting vector where each element is the sum of corresponding elements in a and b\"\n",
            "        }\n",
            "    }\n",
            "}\n",
            "\n",
            "Find the sum of a = [1, -1, 2] and b = [3, 0, -4].<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "messages = []\n",
        "\n",
        "messages.append(user_query)\n",
        "\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    add_special_tokens=False,\n",
        "    padding=True,\n",
        "    tools=[get_vector_sum],\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "print(tokenizer.decode(input_ids[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers_cfg\n",
            "  Downloading transformers_cfg-0.2.7-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: torch>=2.0.0 in ./.venv/lib/python3.11/site-packages (from transformers_cfg) (2.7.1)\n",
            "Requirement already satisfied: numpy>=1.24.2 in ./.venv/lib/python3.11/site-packages (from transformers_cfg) (2.3.2)\n",
            "Requirement already satisfied: transformers>=4.37.2 in ./.venv/lib/python3.11/site-packages (from transformers_cfg) (4.55.0)\n",
            "Requirement already satisfied: tokenizers>=0.19.0 in ./.venv/lib/python3.11/site-packages (from transformers_cfg) (0.21.4)\n",
            "Collecting termcolor>=2.4.0 (from transformers_cfg)\n",
            "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: sentencepiece>=0.1.99 in ./.venv/lib/python3.11/site-packages (from transformers_cfg) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=4.25.2 in ./.venv/lib/python3.11/site-packages (from transformers_cfg) (6.31.1)\n",
            "Collecting setuptools>=69.0.3 (from transformers_cfg)\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./.venv/lib/python3.11/site-packages (from tokenizers>=0.19.0->transformers_cfg) (0.34.3)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.19.0->transformers_cfg) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.19.0->transformers_cfg) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.19.0->transformers_cfg) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.19.0->transformers_cfg) (6.0.2)\n",
            "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.19.0->transformers_cfg) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.19.0->transformers_cfg) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.19.0->transformers_cfg) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.19.0->transformers_cfg) (1.1.7)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->transformers_cfg) (1.14.0)\n",
            "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->transformers_cfg) (3.5)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->transformers_cfg) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->transformers_cfg) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->transformers_cfg) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->transformers_cfg) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->transformers_cfg) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->transformers_cfg) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->transformers_cfg) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->transformers_cfg) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->transformers_cfg) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->transformers_cfg) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->transformers_cfg) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->transformers_cfg) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->transformers_cfg) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->transformers_cfg) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->transformers_cfg) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.1 in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->transformers_cfg) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.0.0->transformers_cfg) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers>=4.37.2->transformers_cfg) (2025.7.34)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.11/site-packages (from transformers>=4.37.2->transformers_cfg) (0.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->transformers_cfg) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.19.0->transformers_cfg) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.19.0->transformers_cfg) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.19.0->transformers_cfg) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.19.0->transformers_cfg) (2025.8.3)\n",
            "Downloading transformers_cfg-0.2.7-py3-none-any.whl (67 kB)\n",
            "Downloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
            "Installing collected packages: termcolor, setuptools, transformers_cfg\n",
            "\u001b[2K  Attempting uninstall: setuptools\n",
            "\u001b[2K    Found existing installation: setuptools 65.5.0\n",
            "\u001b[2K    Uninstalling setuptools-65.5.0:\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [setuptools]\n",
            "\u001b[2K      Successfully uninstalled setuptools-65.5.0━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [setuptools]\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [transformers_cfg][transformers_cfg]\n",
            "\u001b[1A\u001b[2KSuccessfully installed setuptools-80.9.0 termcolor-3.1.0 transformers_cfg-0.2.7\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers_cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from transformers_cfg.grammar_utils import IncrementalGrammarConstraint\n",
        "from transformers_cfg.generation.logits_process import GrammarConstrainedLogitsProcessor\n",
        "# from transformers import  AutoTokenizer\n",
        "\n",
        "\n",
        "JSON_ARR_GBNF = r\"\"\"\n",
        "# This is the same as json.gbnf but we restrict whitespaces at the end of the root array\n",
        "# Useful for generating JSON arrays\n",
        "root   ::= arr\n",
        "value  ::= object | array | string | number | (\"true\" | \"false\" | \"null\") ws\n",
        "arr  ::=\n",
        "  \"[\\n\" ws (\n",
        "            value\n",
        "    (\",\\n\" ws value)*\n",
        "  )? \"]\"\n",
        "object ::=\n",
        "  \"{\" ws (\n",
        "            string \":\" ws value\n",
        "    (\",\" ws string \":\" ws value)*\n",
        "  )? \"}\" ws\n",
        "array  ::=\n",
        "  \"[\" ws (\n",
        "            value\n",
        "    (\",\" ws value)*\n",
        "  )? \"]\" ws\n",
        "string ::=\n",
        "  \"\\\"\" (\n",
        "    [^\"\\\\\\x7F\\x00-\\x1F] |\n",
        "    \"\\\\\" ([\"\\\\/bfnrt] | \"u\" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F]) # escapes\n",
        "  )* \"\\\"\" ws\n",
        "number ::= (\"-\"? ([0-9] | [1-9] [0-9]*)) (\".\" [0-9]+)? ([eE] [-+]? [0-9]+)? ws\n",
        "# Optional space: by convention, applied in this grammar after literal chars when allowed\n",
        "ws ::= ([ \\t\\n] ws)?\n",
        "\"\"\"\n",
        "\n",
        "def generate_with_grammar(model, input_ids, **kwargs):\n",
        "    # tokenizer = FastLanguageModel.from_pretrained(model.config.name_or_path)\n",
        "    grammar = IncrementalGrammarConstraint(JSON_ARR_GBNF, start_rule_name=\"root\", tokenizer=tokenizer)\n",
        "    grammar_processor = GrammarConstrainedLogitsProcessor(grammar)\n",
        "\n",
        "    partial_generate = partial(\n",
        "        model.generate,\n",
        "        do_sample=False,\n",
        "        repetition_penalty=1.1,\n",
        "        num_return_sequences=1,\n",
        "        logits_processor=[grammar_processor],  # Ensure grammar_processor is accessible\n",
        "        temperature=None,\n",
        "        top_p=None,\n",
        "        top_k=None,\n",
        "        sliding_window=None,\n",
        "    )\n",
        "\n",
        "    # Execute generation with merged parameters\n",
        "    return partial_generate(\n",
        "        input_ids=input_ids,\n",
        "        **kwargs\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "ename": "NotImplementedError",
          "evalue": "Tokenizer not supported: PreTrainedTokenizerFast",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m output = \u001b[43mgenerate_with_grammar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m generated_tokens = output[:, input_ids.shape[\u001b[32m1\u001b[39m]:]\n\u001b[32m      8\u001b[39m decoded_output = tokenizer.batch_decode(generated_tokens, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mgenerate_with_grammar\u001b[39m\u001b[34m(model, input_ids, **kwargs)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_with_grammar\u001b[39m(model, input_ids, **kwargs):\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# tokenizer = FastLanguageModel.from_pretrained(model.config.name_or_path)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     grammar = \u001b[43mIncrementalGrammarConstraint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mJSON_ARR_GBNF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_rule_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mroot\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     grammar_processor = GrammarConstrainedLogitsProcessor(grammar)\n\u001b[32m     42\u001b[39m     partial_generate = partial(\n\u001b[32m     43\u001b[39m         model.generate,\n\u001b[32m     44\u001b[39m         do_sample=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m         sliding_window=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     52\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/fine-tunning-models/.venv/lib/python3.11/site-packages/transformers_cfg/token_grammar_recognizer.py:132\u001b[39m, in \u001b[36mIncrementalTokenRecognizer.__init__\u001b[39m\u001b[34m(self, grammar_str, start_rule_name, tokenizer, trie, homomorphism)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    125\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    126\u001b[39m     grammar_str: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    130\u001b[39m     homomorphism: Optional[TokenizerMiddleMapping] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    131\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrammar_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_rule_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrie\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrie\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhomomorphism\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhomomorphism\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m     \u001b[38;5;28mself\u001b[39m.last_size = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/fine-tunning-models/.venv/lib/python3.11/site-packages/transformers_cfg/token_grammar_recognizer.py:38\u001b[39m, in \u001b[36mAbsTokenRecognizer.__init__\u001b[39m\u001b[34m(self, grammar_str, tokenizer, start_rule_name, trie, homomorphism)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mself\u001b[39m.string_recognizer = StringRecognizer(grammar_encoding, \u001b[38;5;28mself\u001b[39m.start_rule_id)\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trie \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28mself\u001b[39m.byte_trie = \u001b[43mByteTrie\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     40\u001b[39m     \u001b[38;5;28mself\u001b[39m.byte_trie = trie\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/fine-tunning-models/.venv/lib/python3.11/site-packages/transformers_cfg/tokenization/byte_trie.py:55\u001b[39m, in \u001b[36mByteTrie.from_tokenizer\u001b[39m\u001b[34m(cls, tokenizer)\u001b[39m\n\u001b[32m     53\u001b[39m trie = \u001b[38;5;28mcls\u001b[39m()\n\u001b[32m     54\u001b[39m mapping = TokenizerMiddleMapping.from_hf_tokenizer(tokenizer)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m TCFG_tokenizer = \u001b[43mTCFG_Tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_hf_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m token_ids_to_ignore: Set[\n\u001b[32m     58\u001b[39m     \u001b[38;5;28mint\u001b[39m\n\u001b[32m     59\u001b[39m ] = TCFG_tokenizer.get_special_token_ids_to_excluded()\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(TCFG_tokenizer.real_vocab_size()):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/fine-tunning-models/.venv/lib/python3.11/site-packages/transformers_cfg/tokenization/tokenizer.py:71\u001b[39m, in \u001b[36mTCFG_Tokenizer.from_hf_tokenizer\u001b[39m\u001b[34m(cls, hf_tokenizer)\u001b[39m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m TCFG_LlamaTokenizer(hf_tokenizer)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m     72\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTokenizer not supported: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhf_tokenizer.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m     )\n",
            "\u001b[31mNotImplementedError\u001b[39m: Tokenizer not supported: PreTrainedTokenizerFast"
          ]
        }
      ],
      "source": [
        "output = generate_with_grammar(\n",
        "    model=model,\n",
        "    input_ids=input_ids\n",
        ")\n",
        "\n",
        "generated_tokens = output[:, input_ids.shape[1]:]\n",
        "\n",
        "decoded_output = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "for i, message in enumerate(decoded_output):\n",
        "    print(f\"{message}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "21f11fe64628423b86cfd1072510e8f0",
            "f9daabb2fced47849eaeffdab8b586b1",
            "18cf67c84e4e4e9d9d1b4b47ec4125a9",
            "1d4ad81e71c94ce5a54b2aeeb59a0f46",
            "4968ee9ca3564b77af8e3aeceb5abcab",
            "0e017dfdc4ad41afa93e234eaad42dfb",
            "4e92b7c3e8ff4347971267aee205490a",
            "e794823e257646199142d5a2d34322c7",
            "4e27449c8fc740eba93cb175b6c103fc",
            "bfece21a2b704a4e8f4e8c3159fd0a50",
            "ddfdd8e47c454b17b2fd3a7ae751a821"
          ]
        },
        "id": "biZHDF6DCGK6",
        "outputId": "d945885a-0c13-48ce-f785-d1d5bbdff6dd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|██████████| 23968/23968 [00:13<00:00, 1760.19 examples/s]\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 3, # Set this for 1 full training run.\n",
        "        max_steps = 100,\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyHWyScTMK80",
        "outputId": "133f465f-f8d1-4242-fa27-ef241889b6a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['messages', 'text', 'input_ids', 'attention_mask'],\n",
              "    num_rows: 23968\n",
              "})"
            ]
          },
          "execution_count": 163,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0Nu0pH8JMYyR",
        "outputId": "d450f598-0172-40c4-ed20-4d6d83b52fe5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|begin_of_text|><|im_start|>system\\n\\nEres un asistente con amplios conocimientos en el tema: Valuación de bienes y función del perito valuador. \\nResponde de forma clara, precisa y útil, adaptando la complejidad según el nivel del usuario.\\n<|im_end|>\\n<|im_start|>user\\n¿Cómo se diferencia el valor de mercado de la valuación técnica del bien?<|im_end|>\\n<|im_start|>assistant\\nEl valor de mercado se refiere al precio más probable que un bien podría alcanzar en un entorno competitivo, mientras que la valuación técnica es un proceso más detallado que involucra un análisis exhaustivo del bien. Esta última incluye no solo el valor de mercado, sino también aspectos específicos como costos de reparación o mejoras necesarias, así como una evaluación más profunda de los potenciales riesgos que el bien puede presentar. Así, la valuación técnica proporciona un marco más completo para entender el verdadero valor y potencial del bien.<|im_end|>\\n'"
            ]
          },
          "execution_count": 164,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_iG2LbLNkpL",
        "outputId": "9e103298-7cbe-4aad-b1c2-27c314e3d42b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = NVIDIA A40. Max memory = 44.339 GB.\n",
            "21.387 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "Z-z-yRpLN-Xk",
        "outputId": "704ffcad-cf26-4dac-fd65-4ac25e4d4073"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 23,968 | Num Epochs = 1 | Total steps = 100\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 41,943,040 of 8,072,466,432 (0.52% trained)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 04:10, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.088500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.060600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.372900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.941200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.984800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.983700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.944200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.718900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.423000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.496100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.365200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.307000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.361200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.451500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.234700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.282800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.210000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.302200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.455000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.203000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.147500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.184300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.096300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.465100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.093400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.323400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.277500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.117100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.132000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.171300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.299500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.090600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.106100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.298300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.082600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.119200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.967400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.183500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.088100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.121000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.129200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.280100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.065100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.169900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.118700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.046700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.063300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.119100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.221000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.069100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.226600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.214600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.101800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.021600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.209200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.111700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.008800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>1.045100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.129600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>1.098500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>1.204100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>1.321600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>1.206900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>1.025200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>1.060800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>1.096100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>1.098000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>1.217100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.077600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>1.068700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>1.005500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>1.010500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>1.055800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>1.057800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>1.084000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>1.057500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>1.083900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>1.262200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.085800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.987900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>1.308900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>1.201700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>1.332900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.978400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>1.080300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>1.099300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>1.251300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>1.091600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.076300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>1.166700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>1.123800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>1.094000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>1.165900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.967100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.941000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>1.097700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>1.095800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>1.213200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.185200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR_s_7rhN_T6",
        "outputId": "9070397c-4886-410d-8dba-a25b321be826"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "460.5799 seconds used for training.\n",
            "7.68 minutes used for training.\n",
            "Peak reserved memory = 14.203 GB.\n",
            "Peak reserved memory for training = 0.0 GB.\n",
            "Peak reserved memory % of max memory = 32.033 %.\n",
            "Peak reserved memory for training % of max memory = 0.0 %.\n"
          ]
        }
      ],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "log_history = trainer.state.log_history\n",
        "steps = [entry['step'] for entry in log_history if 'loss' in entry]\n",
        "losses = [entry['loss'] for entry in log_history if 'loss' in entry]\n",
        "\n",
        "# Crea la gráfica\n",
        "plt.plot(steps, losses, label='Training Loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r_oQtrdQgtC"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "# Assuming model and tokenizer are already defined\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "\n",
        "\n",
        "def generate_response(messages):\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "    \n",
        "    text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "    _ = model.generate(\n",
        "        input_ids,\n",
        "        streamer=text_streamer,\n",
        "        max_new_tokens=128,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bulk messages for testing\n",
        "bulk_messages = [\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": \"Hola\"}\n",
        "    ],\n",
        "     [\n",
        "        {\"role\": \"user\", \"content\": \"Saludos\"}\n",
        "    ],\n",
        "      [\n",
        "        {\"role\": \"user\", \"content\": \"Bom dia\"}\n",
        "    ],\n",
        "       [\n",
        "        {\"role\": \"user\", \"content\": \"Hello!!\"}\n",
        "    ],\n",
        "    [\n",
        "        {\"role\":\"user\", \"content\":\"2+2\"},\n",
        "        {\"role\":\"assistant\",\"content\":\"el resultado de 2+2 es *4*\"},\n",
        "        {\"role\": \"user\", \"content\": \"Gracias por Ayudarme\"}\n",
        "    ],\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": \"Me gustaría aprender sobre inteligencia artificial.\"}\n",
        "    ],\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": \"Explícame qué es el cambio climático.\"}\n",
        "    ],\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": \"Dime un chiste.\"}\n",
        "    ],\n",
        "    [\n",
        "        {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"¿Qué ocurre con un estafador que paga por dos vehículos mediante transferencias bancarias y luego solicita un crédito de seis meses para la compra de otros dos vehículos?\",\n",
        "        }\n",
        "    ],\n",
        "    [\n",
        "        {\n",
        "            \"role\":\"user\",\n",
        "            \"content\":\"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8\"\n",
        "        }\n",
        "    ]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QUERY: Hola\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Claro, esto fue el punto que me gustaba especialmente. ¿Quieres saber más sobre esto o algo específico?<|im_end|>\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "QUERY: Saludos\n",
            "\n",
            "¿Cuál es el objeto central de los artículos?<|im_end|>\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "QUERY: Bom dia\n",
            "\n",
            "Olá! Como está? Qual é seu problema ou questão?<|im_end|>\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "QUERY: Hello!!\n",
            "\n",
            "What happened last night at Eddystone was insane. The amount of people there was like nothing I’ve ever seen! I was so overwhelmed, and even though we got the tickets after waiting for a couple of hours, I didn’t have the best of nights. But at least the concert was amazing.<|im_end|>\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "QUERY: Gracias por Ayudarme\n",
            "\n",
            "De nada, ¿Puedes responder a la siguiente pregunta o debería enviarla a otro asistente? ¿Quieres conocer más detalles sobre este resultado?<|im_end|>\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "QUERY: Me gustaría aprender sobre inteligencia artificial.\n",
            "\n",
            "La inteligencia artificial es una rama de la informática que está evolucionando rápidamente, y tiene múltiples aplicaciones prácticas.<|im_end|>\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "QUERY: Explícame qué es el cambio climático.\n",
            "\n",
            "El cambio climático es un proceso mediante el cual las condiciones climáticas de un área o de todo el planeta como un todo se modifican, volviendo el ambiente más cálido o más frío, o más húmedo o más seco.<|im_end|>\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "QUERY: Dime un chiste.\n",
            "\n",
            "¿Te lo dije o estás bromeando?<|im_end|>\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "QUERY: ¿Qué ocurre con un estafador que paga por dos vehículos mediante transferencias bancarias y luego solicita un crédito de seis meses para la compra de otros dos vehículos?\n",
            "\n",
            "Esa es un caso interesante. Primero, vamos a analizar cada paso del proceso. A continuación, proporciono una explicación detallada sobre cada parte y cómo se relaciona con la idea del estafador.\n",
            "\n",
            "**Parte 1: Pago de vehículos**\n",
            "\n",
            "En esta parte, el estafador paga por dos vehículos a través de transferencias bancarias, lo que implica que ya ha logrado obtener, de algún modo, el dinero necesario para este gasto. No sabemos si estas transferencias provienen de algún método ilegítimo o simplemente una forma de a\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "QUERY: Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8\n",
            "\n",
            "The next number in the Fibonacci sequence would be 13. This is calculated by adding the previous two numbers (8+5) to get 13.<|im_end|>\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for messages in bulk_messages:\n",
        "  print(f\"QUERY: {messages[-1]['content']}\\n\")\n",
        "  generate_response(messages)\n",
        "  print(\"-\"*100)\n",
        "  print(f\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.8.1: Fast Llama patching. Transformers: 4.55.0.\n",
            "   \\\\   /|    NVIDIA A40. Num GPUs = 1. Max memory: 44.339 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.22s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "One of the most famous towers in Paris is the Eiffel Tower, an iconic monument and popular tourist destination located on the Champ de Mars.<|im_end|>\n"
          ]
        }
      ],
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "        [{\"role\":\"user\", \"content\": \"What is a famous tall tower in Paris?\"}],\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer,skip_prompt=True)\n",
        "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "PreTrainedTokenizerFast has no attribute _ollama_modelfile",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[150]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ollama_modelfile\u001b[49m.split(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/fine-tunning-models/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1099\u001b[39m, in \u001b[36mSpecialTokensMixin.__getattr__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1096\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.convert_tokens_to_ids(attr_as_tokens) \u001b[38;5;28;01mif\u001b[39;00m attr_as_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1098\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1099\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1100\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattr__\u001b[39m(key)\n",
            "\u001b[31mAttributeError\u001b[39m: PreTrainedTokenizerFast has no attribute _ollama_modelfile"
          ]
        }
      ],
      "source": [
        "tokenizer._ollama_modelfile.split('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save to GGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "JutGXCqQ_PHL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130,
          "referenced_widgets": [
            "0b5dc50ae3dd42ad808db9a1b96c9e07",
            "e5b241ec2e01445aa5b3ecb8430c2329",
            "7f87b910edde4b89b89a71542771a06f",
            "e0e224471a24406dbc6ca8e837c91fd1",
            "673b23bc307a4c2baeca7f3d989207d8",
            "da505427887442579ccdad3fd452584e",
            "8486203cb1454c5996944f60ce56e0ef",
            "86f60a2df7c74583856dee0e69d4684d",
            "1c988da8790f484db0a36f21dc34f371",
            "d32fbf3c48cf487088bb8fc612ff633d",
            "f8cc9eaec2b54c859e74b0401493d111",
            "86d5585d1cf047578dd9e9b4c6b2c50a",
            "e86d45ab807c4053bc8bd041d1677c22",
            "b26a8ca5860f4c23ac7a13726b08f78d",
            "f7f23f23751c4ce9af26ce8126d03343",
            "21f9d25a06154a89be7763626bf428be",
            "8c82af98ef85481da01f9f937c37ced2",
            "008dc8b8ea4f4cee91b93fbdea194eb8",
            "a457c94501c24c479bd677d8e5f71317",
            "7c3f9d4c18ba41b980ea9c7e0f7bb486",
            "f09dff6f74fb4445817d155435b3e7c2",
            "78d83c6676a44873af88b871b4be7a92",
            "cb3293c9c37147d1997ab3e6cdc41cbf",
            "0b3a2d641ac549e7b1b64af719abc05e",
            "583702b26d274214a9b5f2e75b383888",
            "6e6944997247422ca532ebc63171e804",
            "fcc4ddc74fd64b7ab4332baf915a47df",
            "f3d739a53fd34090920d1a5c9893f863",
            "0ac386dc6a9041dd9d3b6b9d54d5c4d9",
            "23ecab9a709a4ac786a4e4fc99c2a1a9",
            "2f109f1cc0f04a7ca7d19ea508be1cb0",
            "fb6ca1deeaee47af9c1065a90d1f70ba",
            "39ee9f8ba8e74d7b8b1c41c97427c72f"
          ]
        },
        "id": "_yQWm481TaNs",
        "outputId": "ebeab075-16fb-4573-a7d8-e0bb7085eb0b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/chat_template.jinja',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "execution_count": 128,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "#model.push_to_hub(model_name, token = hf_token) # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "hf_token = os.environ.get(\"HUGGING_FACE_KEY\")\n",
        "model_name =\"jeanmcm/llama3.1-b_risks\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ctqkd75NU37o",
        "outputId": "4b4f8083-58ab-4e18-9ada-fe4d66383b36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 363.87 out of 503.51 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [00:00<00:00, 32.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Converting llama model. Can use fast conversion = False.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: [1] Converting model at jeanmcm/llama3.1-b_risks-2 into bf16 GGUF format.\n",
            "The output location will be /workspace/fine-tunning-models/jeanmcm/llama3.1-b_risks-2/unsloth.BF16.gguf\n",
            "This might take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: llama3.1-b_risks-2\n",
            "INFO:hf-to-gguf:Model architecture: LlamaForCausalLM\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> BF16, shape = {4096, 128288}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:output.weight,               torch.bfloat16 --> BF16, shape = {4096, 128288}\n",
            "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 8192\n",
            "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 14336\n",
            "INFO:hf-to-gguf:gguf: head count = 32\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
            "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
            "INFO:hf-to-gguf:gguf: file type = 32\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "WARNING:gguf.vocab:Unknown separator token '<|begin_of_text|>' in TemplateProcessing<pair>\n",
            "INFO:gguf.vocab:Adding 280147 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type bos to 128000\n",
            "INFO:gguf.vocab:Setting special token type eos to 128003\n",
            "INFO:gguf.vocab:Setting special token type pad to 128001\n",
            "INFO:gguf.vocab:Setting add_bos_token to True\n",
            "INFO:gguf.vocab:Setting add_sep_token to False\n",
            "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if message['role'] == 'user' %}{{'<|im_start|>user\n",
            "' + message['content'] + '<|im_end|>\n",
            "'}}{% elif message['role'] == 'assistant' %}{{'<|im_start|>assistant\n",
            "' + message['content'] + '<|im_end|>\n",
            "' }}{% else %}{{ '<|im_start|>system\n",
            "' + message['content'] + '<|im_end|>\n",
            "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
            "' }}{% endif %}\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/workspace/fine-tunning-models/jeanmcm/llama3.1-b_risks-2/unsloth.BF16.gguf: n_tensors = 291, total_size = 16.1G\n",
            "Writing: 100%|██████████| 16.1G/16.1G [02:24<00:00, 111Mbyte/s] \n",
            "INFO:hf-to-gguf:Model successfully exported to /workspace/fine-tunning-models/jeanmcm/llama3.1-b_risks-2/unsloth.BF16.gguf\n",
            "Unsloth: Conversion completed! Output location: /workspace/fine-tunning-models/jeanmcm/llama3.1-b_risks-2/unsloth.BF16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
            "main: build = 6112 (99acbc99)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/workspace/fine-tunning-models/jeanmcm/llama3.1-b_risks-2/unsloth.BF16.gguf' to '/workspace/fine-tunning-models/jeanmcm/llama3.1-b_risks-2/unsloth.Q4_K_M.gguf' as Q4_K_M using 192 threads\n",
            "llama_model_loader: loaded meta data with 31 key-value pairs and 291 tensors from /workspace/fine-tunning-models/jeanmcm/llama3.1-b_risks-2/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama3.1 B_Risks 2\n",
            "llama_model_loader: - kv   3:                            general.version str              = 2\n",
            "llama_model_loader: - kv   4:                           general.basename str              = llama3.1-b_risks\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8.0B\n",
            "llama_model_loader: - kv   6:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   7:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   8:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   9:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  10:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  11:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  13:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  14:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  15:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  16:                          general.file_type u32              = 32\n",
            "llama_model_loader: - kv  17:                           llama.vocab_size u32              = 128288\n",
            "llama_model_loader: - kv  18:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128288]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128288]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128003\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_sep_token bool             = false\n",
            "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type bf16:  226 tensors\n",
            "[   1/ 291]                        output.weight - [ 4096, 128288,     1,     1], type =   bf16, converting to q6_K .. size =  1002.25 MiB ->   411.08 MiB\n",
            "[   2/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   3/ 291]                    token_embd.weight - [ 4096, 128288,     1,     1], type =   bf16, converting to q4_K .. size =  1002.25 MiB ->   281.88 MiB\n",
            "[   4/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[   5/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   6/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   7/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   8/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[   9/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  10/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  11/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  12/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  13/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  14/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  15/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  16/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  17/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  18/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  19/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  20/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  21/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  22/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  23/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  24/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  25/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  26/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  27/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  28/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  29/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  30/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  31/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  32/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  33/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  34/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  35/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  36/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  37/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  38/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  39/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  40/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  41/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  42/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  43/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  44/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  45/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  46/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  47/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  48/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  49/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  50/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  51/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  52/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  53/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  54/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  55/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  56/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  57/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  58/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  59/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  60/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  61/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  62/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  63/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  64/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  65/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  66/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  67/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  68/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  69/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  70/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  71/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  72/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  73/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  74/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  75/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  76/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  77/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  78/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  79/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  80/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  81/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  82/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  83/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  84/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  85/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  86/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  87/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  88/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  89/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  90/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  91/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  92/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  93/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  94/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  95/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  96/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  97/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  98/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  99/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 100/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 101/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 102/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 103/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 104/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 105/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 106/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 107/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 108/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 109/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 110/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 111/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 112/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 113/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 114/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 115/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 116/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 117/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 118/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 119/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 120/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 121/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 122/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 123/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 124/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 125/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 126/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 127/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 128/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 129/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 130/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 131/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 132/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 133/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 134/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 135/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 136/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 137/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 138/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 139/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 140/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 141/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 142/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 143/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 144/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 145/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 146/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 147/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 148/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 149/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 150/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 151/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 152/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 153/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 154/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 155/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 156/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 157/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 158/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 159/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 160/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 161/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 162/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 163/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 164/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 165/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 166/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 167/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 168/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 169/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 170/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 171/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 172/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 173/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 174/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 175/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 176/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 177/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 178/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 179/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 180/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 181/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 182/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 183/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 184/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 185/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 186/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 187/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 188/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 189/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 190/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 191/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 192/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 193/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 194/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 195/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 196/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 197/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 198/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 199/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 200/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 201/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 202/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 203/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 204/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 205/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 206/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 207/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 208/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 209/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 210/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 211/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 212/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 213/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 214/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 215/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 216/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 217/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 218/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 219/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 220/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 221/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 222/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 223/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 224/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 225/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 226/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 227/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 228/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 229/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 230/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 231/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 232/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 233/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 234/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 235/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 236/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 237/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 238/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 239/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 240/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 241/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 242/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 243/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 244/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 245/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 246/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 247/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 248/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 249/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 250/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 251/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 252/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 253/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 254/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 255/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 256/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 257/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 258/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 259/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 260/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 261/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 262/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 263/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 264/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 265/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 266/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 267/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 268/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 269/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 270/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 271/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 272/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 273/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 274/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 275/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 276/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 277/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 278/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 279/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 280/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 281/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 282/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 283/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 284/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 285/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 286/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 287/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 288/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 289/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 290/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 291/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "llama_model_quantize_impl: model size  = 15317.52 MB\n",
            "llama_model_quantize_impl: quant size  =  4685.48 MB\n",
            "\n",
            "main: quantize time = 439841.50 ms\n",
            "main:    total time = 439841.50 ms\n",
            "Unsloth: Conversion completed! Output location: /workspace/fine-tunning-models/jeanmcm/llama3.1-b_risks-2/unsloth.Q4_K_M.gguf\n",
            "Unsloth: Saved Ollama Modelfile to jeanmcm/llama3.1-b_risks-2/Modelfile\n"
          ]
        }
      ],
      "source": [
        "# Save to 8bit Q8_0\n",
        "# if False: model.save_pretrained_gguf(model_name, tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "# if False: model.push_to_hub_gguf(model_name, tokenizer, token = hf_token)\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "# if False: model.save_pretrained_gguf(model_name, tokenizer, quantization_method = \"f16\")\n",
        "# if False: model.push_to_hub_gguf(model_name, tokenizer, quantization_method = \"f16\", )\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "model.save_pretrained_gguf(model_name, tokenizer, quantization_method = \"q4_k_m\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save to hugging Face Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: ##### The current model auto adds a BOS token.\n",
            "Unsloth: ##### Your chat template has a BOS token. We shall remove it temporarily.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 286.28 out of 503.51 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [00:00<00:00, 36.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Done.\n",
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: [1] Converting model at jeanmcm/llama3.1-b_risks-2 into bf16 GGUF format.\n",
            "The output location will be /workspace/fine-tunning-models/jeanmcm/llama3.1-b_risks-2/unsloth.BF16.gguf\n",
            "This might take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: llama3.1-b_risks-2\n",
            "INFO:hf-to-gguf:Model architecture: LlamaForCausalLM\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {64}\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> BF16, shape = {4096, 128256}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:output.weight,               torch.bfloat16 --> BF16, shape = {4096, 128256}\n",
            "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 131072\n",
            "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 14336\n",
            "INFO:hf-to-gguf:gguf: head count = 32\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
            "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
            "INFO:hf-to-gguf:gguf: file type = 32\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "WARNING:gguf.vocab:Unknown separator token '<|begin_of_text|>' in TemplateProcessing<pair>\n",
            "INFO:gguf.vocab:Adding 280147 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type bos to 128000\n",
            "INFO:gguf.vocab:Setting special token type eos to 128001\n",
            "INFO:gguf.vocab:Setting special token type pad to 128004\n",
            "INFO:gguf.vocab:Setting add_bos_token to True\n",
            "INFO:gguf.vocab:Setting add_sep_token to False\n",
            "INFO:gguf.vocab:Setting chat_template to {{ 'Below are some instructions that describe some tasks. Write responses that appropriately complete each request.' }}{% for message in messages %}{% if message['role'] == 'user' %}{{ '\n",
            "\n",
            "### Instruction:\n",
            "' + message['content'] }}{% elif message['role'] == 'assistant' %}{{ '\n",
            "\n",
            "### Response:\n",
            "' + message['content'] + '<|end_of_text|>' }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '\n",
            "\n",
            "### Response:\n",
            "' }}{% endif %}\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/workspace/fine-tunning-models/jeanmcm/llama3.1-b_risks-2/unsloth.BF16.gguf: n_tensors = 292, total_size = 16.1G\n",
            "Writing: 100%|██████████| 16.1G/16.1G [02:38<00:00, 101Mbyte/s] \n",
            "INFO:hf-to-gguf:Model successfully exported to /workspace/fine-tunning-models/jeanmcm/llama3.1-b_risks-2/unsloth.BF16.gguf\n",
            "Unsloth: Conversion completed! Output location: /workspace/fine-tunning-models/jeanmcm/llama3.1-b_risks-2/unsloth.BF16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
            "main: build = 6112 (99acbc99)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/workspace/fine-tunning-models/jeanmcm/llama3.1-b_risks-2/unsloth.BF16.gguf' to '/workspace/fine-tunning-models/jeanmcm/llama3.1-b_risks-2/unsloth.Q4_K_M.gguf' as Q4_K_M using 192 threads\n",
            "llama_model_loader: loaded meta data with 31 key-value pairs and 292 tensors from /workspace/fine-tunning-models/jeanmcm/llama3.1-b_risks-2/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama3.1 B_Risks 2\n",
            "llama_model_loader: - kv   3:                            general.version str              = 2\n",
            "llama_model_loader: - kv   4:                           general.basename str              = llama3.1-b_risks\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8.0B\n",
            "llama_model_loader: - kv   6:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   7:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   8:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   9:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  10:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  11:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  13:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  14:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  15:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  16:                          general.file_type u32              = 32\n",
            "llama_model_loader: - kv  17:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  18:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004\n",
            "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_sep_token bool             = false\n",
            "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ 'Below are some instructions that ...\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type bf16:  226 tensors\n",
            "[   1/ 292]                        output.weight - [ 4096, 128256,     1,     1], type =   bf16, converting to q6_K .. size =  1002.00 MiB ->   410.98 MiB\n",
            "[   2/ 292]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   3/ 292]                    rope_freqs.weight - [   64,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[   4/ 292]                    token_embd.weight - [ 4096, 128256,     1,     1], type =   bf16, converting to q4_K .. size =  1002.00 MiB ->   281.81 MiB\n",
            "[   5/ 292]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[   6/ 292]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   7/ 292]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   8/ 292]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   9/ 292]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  10/ 292]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  11/ 292]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  12/ 292]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  13/ 292]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  14/ 292]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  15/ 292]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  16/ 292]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  17/ 292]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  18/ 292]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  19/ 292]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  20/ 292]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  21/ 292]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  22/ 292]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  23/ 292]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  24/ 292]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  25/ 292]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  26/ 292]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  27/ 292]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  28/ 292]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  29/ 292]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  30/ 292]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  31/ 292]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  32/ 292]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  33/ 292]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  34/ 292]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  35/ 292]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  36/ 292]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  37/ 292]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  38/ 292]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  39/ 292]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  40/ 292]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  41/ 292]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  42/ 292]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  43/ 292]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  44/ 292]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  45/ 292]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  46/ 292]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  47/ 292]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  48/ 292]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  49/ 292]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  50/ 292]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  51/ 292]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  52/ 292]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  53/ 292]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  54/ 292]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  55/ 292]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  56/ 292]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  57/ 292]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  58/ 292]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  59/ 292]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  60/ 292]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  61/ 292]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  62/ 292]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  63/ 292]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  64/ 292]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  65/ 292]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  66/ 292]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  67/ 292]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  68/ 292]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  69/ 292]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  70/ 292]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  71/ 292]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  72/ 292]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  73/ 292]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  74/ 292]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  75/ 292]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  76/ 292]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  77/ 292]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  78/ 292]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  79/ 292]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  80/ 292]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  81/ 292]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  82/ 292]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  83/ 292]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  84/ 292]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  85/ 292]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  86/ 292]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  87/ 292]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  88/ 292]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  89/ 292]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  90/ 292]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  91/ 292]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  92/ 292]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  93/ 292]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  94/ 292]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  95/ 292]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  96/ 292]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  97/ 292]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  98/ 292]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  99/ 292]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 100/ 292]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 101/ 292]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 102/ 292]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 103/ 292]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 104/ 292]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 105/ 292]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 106/ 292]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 107/ 292]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 108/ 292]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 109/ 292]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 110/ 292]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 111/ 292]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 112/ 292]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 113/ 292]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 114/ 292]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 115/ 292]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 116/ 292]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 117/ 292]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 118/ 292]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 119/ 292]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 120/ 292]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 121/ 292]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 122/ 292]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 123/ 292]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 124/ 292]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 125/ 292]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 126/ 292]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 127/ 292]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 128/ 292]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 129/ 292]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 130/ 292]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 131/ 292]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 132/ 292]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 133/ 292]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 134/ 292]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 135/ 292]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 136/ 292]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 137/ 292]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 138/ 292]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 139/ 292]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 140/ 292]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 141/ 292]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 142/ 292]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 143/ 292]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 144/ 292]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 145/ 292]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 146/ 292]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 147/ 292]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 148/ 292]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 149/ 292]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 150/ 292]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 151/ 292]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 152/ 292]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 153/ 292]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 154/ 292]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 155/ 292]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 156/ 292]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 157/ 292]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 158/ 292]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 159/ 292]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 160/ 292]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 161/ 292]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 162/ 292]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 163/ 292]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 164/ 292]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 165/ 292]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 166/ 292]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 167/ 292]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 168/ 292]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 169/ 292]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 170/ 292]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 171/ 292]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 172/ 292]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 173/ 292]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 174/ 292]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 175/ 292]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 176/ 292]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 177/ 292]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 178/ 292]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 179/ 292]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 180/ 292]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 181/ 292]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 182/ 292]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 183/ 292]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 184/ 292]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 185/ 292]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 186/ 292]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 187/ 292]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 188/ 292]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 189/ 292]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 190/ 292]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 191/ 292]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 192/ 292]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 193/ 292]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 194/ 292]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 195/ 292]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 196/ 292]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 197/ 292]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 198/ 292]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 199/ 292]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 200/ 292]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 201/ 292]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 202/ 292]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 203/ 292]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 204/ 292]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 205/ 292]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 206/ 292]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 207/ 292]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 208/ 292]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 209/ 292]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 210/ 292]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 211/ 292]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 212/ 292]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 213/ 292]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 214/ 292]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 215/ 292]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 216/ 292]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 217/ 292]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 218/ 292]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 219/ 292]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 220/ 292]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 221/ 292]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 222/ 292]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 223/ 292]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 224/ 292]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 225/ 292]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 226/ 292]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 227/ 292]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 228/ 292]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 229/ 292]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 230/ 292]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 231/ 292]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 232/ 292]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 233/ 292]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 234/ 292]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 235/ 292]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 236/ 292]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 237/ 292]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 238/ 292]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 239/ 292]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 240/ 292]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 241/ 292]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 242/ 292]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 243/ 292]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 244/ 292]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 245/ 292]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 246/ 292]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 247/ 292]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 248/ 292]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 249/ 292]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 250/ 292]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 251/ 292]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 252/ 292]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 253/ 292]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 254/ 292]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 255/ 292]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 256/ 292]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 257/ 292]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 258/ 292]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 259/ 292]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 260/ 292]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 261/ 292]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 262/ 292]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 263/ 292]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 264/ 292]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 265/ 292]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 266/ 292]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 267/ 292]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 268/ 292]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 269/ 292]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 270/ 292]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 271/ 292]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 272/ 292]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 273/ 292]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 274/ 292]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 275/ 292]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 276/ 292]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 277/ 292]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 278/ 292]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 279/ 292]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 280/ 292]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 281/ 292]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 282/ 292]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 283/ 292]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 284/ 292]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 285/ 292]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 286/ 292]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 287/ 292]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 288/ 292]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 289/ 292]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 290/ 292]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 291/ 292]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 292/ 292]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "llama_model_quantize_impl: model size  = 15317.02 MB\n",
            "llama_model_quantize_impl: quant size  =  4685.30 MB\n",
            "\n",
            "main: quantize time = 312022.81 ms\n",
            "main:    total time = 312022.81 ms\n",
            "Unsloth: Conversion completed! Output location: /workspace/fine-tunning-models/jeanmcm/llama3.1-b_risks-2/unsloth.Q4_K_M.gguf\n",
            "Unsloth: Saved Ollama Modelfile to jeanmcm/llama3.1-b_risks-2/Modelfile\n",
            "Unsloth: Uploading GGUF to Huggingface Hub...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            \n",
            "\u001b[A\n",
            "Processing Files (0 / 1)                :   1%|          | 33.6MB / 4.92GB, 24.0MB/s  \n",
            "Processing Files (0 / 1)                :   1%|▏         | 67.1MB / 4.92GB, 41.9MB/s  \n",
            "Processing Files (0 / 1)                :   2%|▏         |  109MB / 4.92GB, 60.6MB/s  \n",
            "Processing Files (0 / 1)                :   3%|▎         |  151MB / 4.92GB, 75.5MB/s  \n",
            "Processing Files (0 / 1)                :   4%|▍         |  193MB / 4.92GB, 87.7MB/s  \n",
            "Processing Files (0 / 1)                :   5%|▍         |  235MB / 4.92GB, 97.9MB/s  \n",
            "Processing Files (0 / 1)                :   6%|▌         |  277MB / 4.92GB,  106MB/s  \n",
            "Processing Files (0 / 1)                :   6%|▋         |  319MB / 4.92GB,  114MB/s  \n",
            "Processing Files (0 / 1)                :   8%|▊         |  369MB / 4.92GB,  123MB/s  \n",
            "Processing Files (0 / 1)                :   9%|▊         |  419MB / 4.92GB,  131MB/s  \n",
            "Processing Files (0 / 1)                :   9%|▊         |  422MB / 4.92GB,  124MB/s  \n",
            "\u001b[A\n",
            "\u001b[A\n",
            "Processing Files (0 / 1)                :  10%|▉         |  468MB / 4.92GB,  117MB/s  \n",
            "Processing Files (0 / 1)                :  10%|█         |  506MB / 4.92GB,  120MB/s  \n",
            "Processing Files (0 / 1)                :  11%|█         |  537MB / 4.92GB,  122MB/s  \n",
            "Processing Files (0 / 1)                :  12%|█▏        |  582MB / 4.92GB,  127MB/s  \n",
            "Processing Files (0 / 1)                :  13%|█▎        |  620MB / 4.92GB,  129MB/s  \n",
            "Processing Files (0 / 1)                :  13%|█▎        |  654MB / 4.92GB,  131MB/s  \n",
            "Processing Files (0 / 1)                :  14%|█▎        |  667MB / 4.92GB,  128MB/s  \n",
            "Processing Files (0 / 1)                :  14%|█▍        |  706MB / 4.92GB,  131MB/s  \n",
            "Processing Files (0 / 1)                :  14%|█▍        |  711MB / 4.92GB,  127MB/s  \n",
            "Processing Files (0 / 1)                :  15%|█▍        |  719MB / 4.92GB,  124MB/s  \n",
            "Processing Files (0 / 1)                :  15%|█▍        |  720MB / 4.92GB,  120MB/s  \n",
            "Processing Files (0 / 1)                :  15%|█▍        |  725MB / 4.92GB,  117MB/s  \n",
            "Processing Files (0 / 1)                :  15%|█▍        |  733MB / 4.92GB,  115MB/s  \n",
            "Processing Files (0 / 1)                :  16%|█▌        |  765MB / 4.92GB,  116MB/s  \n",
            "Processing Files (0 / 1)                :  16%|█▌        |  794MB / 4.92GB,  117MB/s  \n",
            "Processing Files (0 / 1)                :  17%|█▋        |  828MB / 4.92GB,  118MB/s  \n",
            "Processing Files (0 / 1)                :  17%|█▋        |  836MB / 4.92GB,  116MB/s  \n",
            "Processing Files (0 / 1)                :  18%|█▊        |  861MB / 4.92GB,  116MB/s  \n",
            "Processing Files (0 / 1)                :  18%|█▊        |  898MB / 4.92GB,  118MB/s  \n",
            "Processing Files (0 / 1)                :  19%|█▉        |  924MB / 4.92GB,  118MB/s  \n",
            "Processing Files (0 / 1)                :  19%|█▉        |  959MB / 4.92GB,  120MB/s  \n",
            "Processing Files (0 / 1)                :  20%|█▉        |  968MB / 4.92GB,  118MB/s  \n",
            "Processing Files (0 / 1)                :  21%|██        | 1.04GB / 4.92GB,  124MB/s  \n",
            "Processing Files (0 / 1)                :  21%|██▏       | 1.05GB / 4.92GB,  122MB/s  \n",
            "Processing Files (0 / 1)                :  22%|██▏       | 1.09GB / 4.92GB,  124MB/s  \n",
            "Processing Files (0 / 1)                :  22%|██▏       | 1.11GB / 4.92GB,  123MB/s  \n",
            "Processing Files (0 / 1)                :  23%|██▎       | 1.12GB / 4.92GB,  122MB/s  \n",
            "\u001b[A\n",
            "Processing Files (0 / 1)                :  23%|██▎       | 1.16GB / 4.92GB,  120MB/s  \n",
            "Processing Files (0 / 1)                :  24%|██▍       | 1.21GB / 4.92GB,  123MB/s  \n",
            "Processing Files (0 / 1)                :  25%|██▍       | 1.23GB / 4.92GB,  123MB/s  \n",
            "Processing Files (0 / 1)                :  25%|██▌       | 1.24GB / 4.92GB,  122MB/s  \n",
            "Processing Files (0 / 1)                :  26%|██▌       | 1.28GB / 4.92GB,  126MB/s  \n",
            "Processing Files (0 / 1)                :  27%|██▋       | 1.31GB / 4.92GB,  128MB/s  \n",
            "Processing Files (0 / 1)                :  27%|██▋       | 1.34GB / 4.92GB,  132MB/s  \n",
            "Processing Files (0 / 1)                :  28%|██▊       | 1.37GB / 4.92GB,  134MB/s  \n",
            "Processing Files (0 / 1)                :  28%|██▊       | 1.37GB / 4.92GB,  134MB/s  \n",
            "Processing Files (0 / 1)                :  28%|██▊       | 1.38GB / 4.92GB,  135MB/s  \n",
            "Processing Files (0 / 1)                :  28%|██▊       | 1.40GB / 4.92GB,  134MB/s  \n",
            "Processing Files (0 / 1)                :  28%|██▊       | 1.40GB / 4.92GB,  131MB/s  \n",
            "Processing Files (0 / 1)                :  29%|██▊       | 1.41GB / 4.92GB,  127MB/s  \n",
            "Processing Files (0 / 1)                :  30%|███       | 1.48GB / 4.92GB,  131MB/s  \n",
            "Processing Files (0 / 1)                :  31%|███       | 1.52GB / 4.92GB,  130MB/s  \n",
            "Processing Files (0 / 1)                :  32%|███▏      | 1.56GB / 4.92GB,  130MB/s  \n",
            "Processing Files (0 / 1)                :  33%|███▎      | 1.61GB / 4.92GB,  131MB/s  \n",
            "Processing Files (0 / 1)                :  33%|███▎      | 1.65GB / 4.92GB,  130MB/s  \n",
            "Processing Files (0 / 1)                :  34%|███▍      | 1.66GB / 4.92GB,  127MB/s  \n",
            "Processing Files (0 / 1)                :  35%|███▍      | 1.71GB / 4.92GB,  126MB/s  \n",
            "Processing Files (0 / 1)                :  35%|███▍      | 1.71GB / 4.92GB,  126MB/s  \n",
            "Processing Files (0 / 1)                :  36%|███▌      | 1.78GB / 4.92GB,  133MB/s  \n",
            "\u001b[A\n",
            "Processing Files (0 / 1)                :  37%|███▋      | 1.84GB / 4.92GB,  134MB/s  \n",
            "Processing Files (0 / 1)                :  38%|███▊      | 1.85GB / 4.92GB,  132MB/s  \n",
            "Processing Files (0 / 1)                :  39%|███▉      | 1.91GB / 4.92GB,  135MB/s  \n",
            "Processing Files (0 / 1)                :  39%|███▉      | 1.92GB / 4.92GB,  131MB/s  \n",
            "Processing Files (0 / 1)                :  40%|███▉      | 1.97GB / 4.92GB,  132MB/s  \n",
            "Processing Files (0 / 1)                :  41%|████▏     | 2.04GB / 4.92GB,  135MB/s  \n",
            "Processing Files (0 / 1)                :  42%|████▏     | 2.05GB / 4.92GB,  136MB/s  \n",
            "Processing Files (0 / 1)                :  42%|████▏     | 2.09GB / 4.92GB,  136MB/s  \n",
            "Processing Files (0 / 1)                :  43%|████▎     | 2.12GB / 4.92GB,  138MB/s  \n",
            "Processing Files (0 / 1)                :  43%|████▎     | 2.14GB / 4.92GB,  139MB/s  \n",
            "Processing Files (0 / 1)                :  44%|████▍     | 2.17GB / 4.92GB,  143MB/s  \n",
            "Processing Files (0 / 1)                :  45%|████▍     | 2.21GB / 4.92GB,  145MB/s  \n",
            "Processing Files (0 / 1)                :  46%|████▌     | 2.26GB / 4.92GB,  150MB/s  \n",
            "Processing Files (0 / 1)                :  46%|████▋     | 2.28GB / 4.92GB,  148MB/s  \n",
            "Processing Files (0 / 1)                :  46%|████▋     | 2.28GB / 4.92GB,  146MB/s  \n",
            "Processing Files (0 / 1)                :  46%|████▋     | 2.28GB / 4.92GB,  142MB/s  \n",
            "Processing Files (0 / 1)                :  48%|████▊     | 2.35GB / 4.92GB,  148MB/s  \n",
            "Processing Files (0 / 1)                :  48%|████▊     | 2.35GB / 4.92GB,  146MB/s  \n",
            "Processing Files (0 / 1)                :  49%|████▉     | 2.40GB / 4.92GB,  148MB/s  \n",
            "Processing Files (0 / 1)                :  49%|████▉     | 2.40GB / 4.92GB,  145MB/s  \n",
            "Processing Files (0 / 1)                :  50%|████▉     | 2.46GB / 4.92GB,  147MB/s  \n",
            "Processing Files (0 / 1)                :  51%|█████     | 2.49GB / 4.92GB,  149MB/s  \n",
            "Processing Files (0 / 1)                :  52%|█████▏    | 2.54GB / 4.92GB,  147MB/s  \n",
            "Processing Files (0 / 1)                :  53%|█████▎    | 2.59GB / 4.92GB,  151MB/s  \n",
            "Processing Files (0 / 1)                :  53%|█████▎    | 2.61GB / 4.92GB,  149MB/s  \n",
            "Processing Files (0 / 1)                :  54%|█████▍    | 2.66GB / 4.92GB,  152MB/s  \n",
            "Processing Files (0 / 1)                :  55%|█████▍    | 2.69GB / 4.92GB,  154MB/s  \n",
            "Processing Files (0 / 1)                :  56%|█████▌    | 2.74GB / 4.92GB,  159MB/s  \n",
            "Processing Files (0 / 1)                :  56%|█████▌    | 2.74GB / 4.92GB,  156MB/s  \n",
            "Processing Files (0 / 1)                :  57%|█████▋    | 2.81GB / 4.92GB,  158MB/s  \n",
            "Processing Files (0 / 1)                :  57%|█████▋    | 2.81GB / 4.92GB,  155MB/s  \n",
            "Processing Files (0 / 1)                :  58%|█████▊    | 2.83GB / 4.92GB,  156MB/s  \n",
            "Processing Files (0 / 1)                :  58%|█████▊    | 2.87GB / 4.92GB,  155MB/s  \n",
            "Processing Files (0 / 1)                :  58%|█████▊    | 2.87GB / 4.92GB,  153MB/s  \n",
            "Processing Files (0 / 1)                :  59%|█████▉    | 2.90GB / 4.92GB,  153MB/s  \n",
            "Processing Files (0 / 1)                :  59%|█████▉    | 2.92GB / 4.92GB,  152MB/s  \n",
            "Processing Files (0 / 1)                :  60%|██████    | 2.98GB / 4.92GB,  158MB/s  \n",
            "Processing Files (0 / 1)                :  61%|██████    | 2.99GB / 4.92GB,  158MB/s  \n",
            "Processing Files (0 / 1)                :  63%|██████▎   | 3.08GB / 4.92GB,  165MB/s  \n",
            "Processing Files (0 / 1)                :  63%|██████▎   | 3.11GB / 4.92GB,  168MB/s  \n",
            "Processing Files (0 / 1)                :  64%|██████▍   | 3.15GB / 4.92GB,  171MB/s  \n",
            "Processing Files (0 / 1)                :  65%|██████▌   | 3.20GB / 4.92GB,  168MB/s  \n",
            "Processing Files (0 / 1)                :  65%|██████▌   | 3.20GB / 4.92GB,  165MB/s  \n",
            "Processing Files (0 / 1)                :  67%|██████▋   | 3.28GB / 4.92GB,  169MB/s  \n",
            "Processing Files (0 / 1)                :  67%|██████▋   | 3.30GB / 4.92GB,  166MB/s  \n",
            "Processing Files (0 / 1)                :  68%|██████▊   | 3.32GB / 4.92GB,  164MB/s  \n",
            "Processing Files (0 / 1)                :  68%|██████▊   | 3.37GB / 4.92GB,  167MB/s  \n",
            "\u001b[A\n",
            "Processing Files (0 / 1)                :  70%|██████▉   | 3.43GB / 4.92GB,  169MB/s  \n",
            "Processing Files (0 / 1)                :  70%|███████   | 3.45GB / 4.92GB,  164MB/s  \n",
            "Processing Files (0 / 1)                :  71%|███████   | 3.50GB / 4.92GB,  169MB/s  \n",
            "\u001b[A\n",
            "Processing Files (0 / 1)                :  72%|███████▏  | 3.55GB / 4.92GB,  167MB/s  \n",
            "Processing Files (0 / 1)                :  73%|███████▎  | 3.58GB / 4.92GB,  164MB/s  \n",
            "Processing Files (0 / 1)                :  73%|███████▎  | 3.59GB / 4.92GB,  164MB/s  \n",
            "\u001b[A\n",
            "Processing Files (0 / 1)                :  74%|███████▍  | 3.64GB / 4.92GB,  157MB/s  \n",
            "Processing Files (0 / 1)                :  74%|███████▍  | 3.66GB / 4.92GB,  157MB/s  \n",
            "Processing Files (0 / 1)                :  74%|███████▍  | 3.66GB / 4.92GB,  154MB/s  \n",
            "Processing Files (0 / 1)                :  75%|███████▌  | 3.71GB / 4.92GB,  156MB/s  \n",
            "Processing Files (0 / 1)                :  76%|███████▌  | 3.75GB / 4.92GB,  158MB/s  \n",
            "Processing Files (0 / 1)                :  77%|███████▋  | 3.78GB / 4.92GB,  158MB/s  \n",
            "Processing Files (0 / 1)                :  77%|███████▋  | 3.79GB / 4.92GB,  155MB/s  \n",
            "Processing Files (0 / 1)                :  78%|███████▊  | 3.84GB / 4.92GB,  155MB/s  \n",
            "Processing Files (0 / 1)                :  78%|███████▊  | 3.86GB / 4.92GB,  155MB/s  \n",
            "Processing Files (0 / 1)                :  79%|███████▉  | 3.91GB / 4.92GB,  160MB/s  \n",
            "Processing Files (0 / 1)                :  80%|███████▉  | 3.92GB / 4.92GB,  161MB/s  \n",
            "Processing Files (0 / 1)                :  81%|████████  | 3.99GB / 4.92GB,  161MB/s  \n",
            "Processing Files (0 / 1)                :  82%|████████▏ | 4.03GB / 4.92GB,  164MB/s  \n",
            "Processing Files (0 / 1)                :  83%|████████▎ | 4.06GB / 4.92GB,  163MB/s  \n",
            "Processing Files (0 / 1)                :  83%|████████▎ | 4.06GB / 4.92GB,  163MB/s  \n",
            "Processing Files (0 / 1)                :  83%|████████▎ | 4.06GB / 4.92GB,  157MB/s  \n",
            "Processing Files (0 / 1)                :  84%|████████▍ | 4.13GB / 4.92GB,  160MB/s  \n",
            "Processing Files (0 / 1)                :  85%|████████▍ | 4.17GB / 4.92GB,  160MB/s  \n",
            "Processing Files (0 / 1)                :  85%|████████▌ | 4.20GB / 4.92GB,  157MB/s  \n",
            "Processing Files (0 / 1)                :  87%|████████▋ | 4.27GB / 4.92GB,  163MB/s  \n",
            "Processing Files (0 / 1)                :  87%|████████▋ | 4.27GB / 4.92GB,  158MB/s  \n",
            "Processing Files (0 / 1)                :  87%|████████▋ | 4.27GB / 4.92GB,  155MB/s  \n",
            "Processing Files (0 / 1)                :  89%|████████▊ | 4.36GB / 4.92GB,  158MB/s  \n",
            "Processing Files (0 / 1)                :  89%|████████▊ | 4.36GB / 4.92GB,  159MB/s  \n",
            "Processing Files (0 / 1)                :  90%|████████▉ | 4.41GB / 4.92GB,  157MB/s  \n",
            "Processing Files (0 / 1)                :  90%|████████▉ | 4.41GB / 4.92GB,  157MB/s  \n",
            "Processing Files (0 / 1)                :  91%|█████████ | 4.46GB / 4.92GB,  160MB/s  \n",
            "Processing Files (0 / 1)                :  91%|█████████ | 4.47GB / 4.92GB,  157MB/s  \n",
            "Processing Files (0 / 1)                :  92%|█████████▏| 4.54GB / 4.92GB,  163MB/s  \n",
            "Processing Files (0 / 1)                :  92%|█████████▏| 4.54GB / 4.92GB,  160MB/s  \n",
            "Processing Files (0 / 1)                :  94%|█████████▎| 4.60GB / 4.92GB,  165MB/s  \n",
            "Processing Files (0 / 1)                :  94%|█████████▍| 4.64GB / 4.92GB,  163MB/s  \n",
            "Processing Files (0 / 1)                :  95%|█████████▌| 4.69GB / 4.92GB,  167MB/s  \n",
            "Processing Files (0 / 1)                :  95%|█████████▌| 4.69GB / 4.92GB,  158MB/s  \n",
            "Processing Files (0 / 1)                :  97%|█████████▋| 4.76GB / 4.92GB,  161MB/s  \n",
            "Processing Files (0 / 1)                :  97%|█████████▋| 4.78GB / 4.92GB,  159MB/s  \n",
            "Processing Files (0 / 1)                :  97%|█████████▋| 4.80GB / 4.92GB,  156MB/s  \n",
            "Processing Files (0 / 1)                :  98%|█████████▊| 4.84GB / 4.92GB,  161MB/s  \n",
            "Processing Files (0 / 1)                :  99%|█████████▉| 4.89GB / 4.92GB,  158MB/s  \n",
            "Processing Files (0 / 1)                :  99%|█████████▉| 4.89GB / 4.92GB,  156MB/s  \n",
            "Processing Files (0 / 1)                :  99%|█████████▉| 4.89GB / 4.92GB,  154MB/s  \n",
            "Processing Files (0 / 1)                : 100%|█████████▉| 4.91GB / 4.92GB,  151MB/s  \n",
            "Processing Files (0 / 1)                : 100%|█████████▉| 4.91GB / 4.92GB,  151MB/s  \n",
            "Processing Files (0 / 1)                : 100%|█████████▉| 4.91GB / 4.92GB,  145MB/s  \n",
            "Processing Files (0 / 1)                : 100%|█████████▉| 4.92GB / 4.92GB,  144MB/s  \n",
            "\u001b[A\n",
            "Processing Files (0 / 1)                : 100%|█████████▉| 4.92GB / 4.92GB,  139MB/s  \n",
            "\u001b[A\n",
            "Processing Files (0 / 1)                : 100%|█████████▉| 4.92GB / 4.92GB,  131MB/s  \n",
            "Processing Files (0 / 1)                : 100%|█████████▉| 4.92GB / 4.92GB,  131MB/s  \n",
            "\u001b[A\n",
            "\u001b[A\n",
            "Processing Files (0 / 1)                : 100%|█████████▉| 4.92GB / 4.92GB,  124MB/s  \n",
            "Processing Files (0 / 1)                : 100%|█████████▉| 4.92GB / 4.92GB,  123MB/s  \n",
            "\u001b[A\n",
            "\u001b[A\n",
            "Processing Files (0 / 1)                : 100%|█████████▉| 4.92GB / 4.92GB,  111MB/s  \n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "Processing Files (0 / 1)                : 100%|█████████▉| 4.92GB / 4.92GB, 99.6MB/s  \n",
            "New Data Upload                         : 100%|██████████| 4.27GB / 4.27GB, 99.6MB/s  \n",
            "  ...ma3.1-b_risks-2/unsloth.Q4_K_M.gguf: 100%|█████████▉| 4.92GB / 4.92GB            \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved GGUF to https://huggingface.co/jeanmcm/llama3.1-b_risks-2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "Unsloth: ##### The current model auto adds a BOS token.\n",
            "Unsloth: ##### We removed it in GGUF's chat template for you.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved Ollama Modelfile to https://huggingface.co/jeanmcm/llama3.1-b_risks-2\n"
          ]
        }
      ],
      "source": [
        "model.push_to_hub_gguf(model_name, tokenizer, quantization_method = \"q4_k_m\", token = hf_token)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "008dc8b8ea4f4cee91b93fbdea194eb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01353edbf0534514a8762146b84f55e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "021a36dd1b7f49729060be7457e40bf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "02fd407b25914c98993a4ac3d46eff8e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "060dbb641b324c7092f1aa46268ae354": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "082c7bcffd274215a1a3b03a56c0a303": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ac386dc6a9041dd9d3b6b9d54d5c4d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b3a2d641ac549e7b1b64af719abc05e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3d739a53fd34090920d1a5c9893f863",
            "placeholder": "​",
            "style": "IPY_MODEL_0ac386dc6a9041dd9d3b6b9d54d5c4d9",
            "value": "  ...p2pggtwf6/adapter_model.safetensors: 100%"
          }
        },
        "0b5dc50ae3dd42ad808db9a1b96c9e07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e5b241ec2e01445aa5b3ecb8430c2329",
              "IPY_MODEL_7f87b910edde4b89b89a71542771a06f",
              "IPY_MODEL_e0e224471a24406dbc6ca8e837c91fd1"
            ],
            "layout": "IPY_MODEL_673b23bc307a4c2baeca7f3d989207d8"
          }
        },
        "0cb7c78341d94916b03a73a18a90d124": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e017dfdc4ad41afa93e234eaad42dfb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e13d385ea134c2ea25eb5b0d4a4578b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "113ba718a2c146e4970546a3312e6230": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "175c9d72cc0249cc9e2ab1bc0d86405a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1fd61cc64eb453394d31ecfbeedf9f4",
            "max": 3392,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c66d2d831774e749ccc6263a5339e7d",
            "value": 3392
          }
        },
        "17bbd1d8e24c4c2494d3b1e5efc9a7c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68acfb4bf95547848a0fced9e6c0aa29",
            "placeholder": "​",
            "style": "IPY_MODEL_62c9849bf9fe41e3afa4f528bfa321f0",
            "value": " 380/380 [00:00&lt;00:00, 16.2kB/s]"
          }
        },
        "186daa6e79e9464aa04107b4776702d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "18cf67c84e4e4e9d9d1b4b47ec4125a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e794823e257646199142d5a2d34322c7",
            "max": 3392,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e27449c8fc740eba93cb175b6c103fc",
            "value": 3392
          }
        },
        "1c988da8790f484db0a36f21dc34f371": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1d4995e0f8684ee7acefebd6fd1caca3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d4ad81e71c94ce5a54b2aeeb59a0f46": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfece21a2b704a4e8f4e8c3159fd0a50",
            "placeholder": "​",
            "style": "IPY_MODEL_ddfdd8e47c454b17b2fd3a7ae751a821",
            "value": " 3392/3392 [00:03&lt;00:00, 1504.80 examples/s]"
          }
        },
        "1d6b0fe6e7454620b9380e1632b90e3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb30f78f3bc94e40a46574c546c9fd93",
            "placeholder": "​",
            "style": "IPY_MODEL_587488a53a0c4d10a93836ad36bfe60a",
            "value": "Map: 100%"
          }
        },
        "1e6127c6367340f1b4954499dd163b41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ff388a8c8444254b1b218225af1e506": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2121b6cbeb08488db8f1c4ddd068593c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ff22b6aec10444d852dc071374ecd8e",
              "IPY_MODEL_ef180749a010442e8c54a9b68e2c47cc",
              "IPY_MODEL_fafc4021a849469fb59b5df0987fd0b3"
            ],
            "layout": "IPY_MODEL_f001e1bd936b4c93bb5328f09ca208c1"
          }
        },
        "21f11fe64628423b86cfd1072510e8f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9daabb2fced47849eaeffdab8b586b1",
              "IPY_MODEL_18cf67c84e4e4e9d9d1b4b47ec4125a9",
              "IPY_MODEL_1d4ad81e71c94ce5a54b2aeeb59a0f46"
            ],
            "layout": "IPY_MODEL_4968ee9ca3564b77af8e3aeceb5abcab"
          }
        },
        "21f9d25a06154a89be7763626bf428be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23ecab9a709a4ac786a4e4fc99c2a1a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "241be6f77a2247999a6adb196047b6fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77cfc917e3f4439d978f9290ad2d1ffd",
            "placeholder": "​",
            "style": "IPY_MODEL_57a5483bf4fd4726a4d0d434e7aca338",
            "value": " 5.70G/5.70G [00:47&lt;00:00, 219MB/s]"
          }
        },
        "257afb34467c464daf3df45a42f697c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d90e48cf4bd46239267f2515133d3d1",
              "IPY_MODEL_483c113f25a845d392ccc456d3046a40",
              "IPY_MODEL_493abb845f1344f28675d00bdb6e02f5"
            ],
            "layout": "IPY_MODEL_8047fc11d6dc4635b42dd0ea6fb69bde"
          }
        },
        "28dee16d89d248a1ac7606faba9de6cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59056bccf795490d98664aadabc619c0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_49ff4a08713f49f49b687a3dd6ca1f0d",
            "value": 1
          }
        },
        "2ea103fd703c4917ace8df3ce4dc4bcd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f109f1cc0f04a7ca7d19ea508be1cb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3092d6eb5a2d4bfc93704bf02e46afb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d852f519cb449e993d54c9d3ac47d48",
            "max": 17209920,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_85daa7a105c64380a682620a9cb2ee96",
            "value": 17209920
          }
        },
        "314fe0becd0248648b6c39cc3eb839d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_113ba718a2c146e4970546a3312e6230",
            "max": 380,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c9364c8b32fd40168e6735f39b80a210",
            "value": 380
          }
        },
        "32183ededf184d2da61eb6529c36f4ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "328d5557de3c43ceb18a2cc3ca451025": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33d0ff7b3dad4a719d5414cc77dd7250": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_739dd44420774a4e89d3f4162c448186",
            "placeholder": "​",
            "style": "IPY_MODEL_d4d21f5053ab44e5beaf6dda775b7e63",
            "value": "Map: 100%"
          }
        },
        "34bac034017a43dba58c28c7e520f8b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "364d047cf1044594a5336d4659f24074": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9bda7063e364fdc8f371fe6f60ab425",
            "max": 235,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f8f3ca71aba6495fa9ebb32f3bd60ab3",
            "value": 235
          }
        },
        "39ee9f8ba8e74d7b8b1c41c97427c72f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3dd0f9ac6bae447f94637fbb7a5362d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42412db9b76f4bbfa67056c8acb9bab5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4531109e7a27424b87ba16fe473a7e22": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46bedf08f65644b58237ee1f6e18cc84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d7eb01c0e5d44b0a1c06a6a2a59a2bf",
            "max": 3392,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_510c2f7c0f8f44fdab1eb7a1fe0d6799",
            "value": 3392
          }
        },
        "483c113f25a845d392ccc456d3046a40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_964d4515de7945b5ac82c9372a08031a",
            "max": 459,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_186daa6e79e9464aa04107b4776702d8",
            "value": 459
          }
        },
        "493abb845f1344f28675d00bdb6e02f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4dfa74df37a34185ac5bdf1a2fc881e5",
            "placeholder": "​",
            "style": "IPY_MODEL_fdb9ff25e5054095ad4a784a5b0099ee",
            "value": " 459/459 [00:00&lt;00:00, 54.3kB/s]"
          }
        },
        "4968ee9ca3564b77af8e3aeceb5abcab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4981fbe58c31449dad4864b8ba2c151c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49df1af933914d1aab887f876b78012a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cb7c78341d94916b03a73a18a90d124",
            "placeholder": "​",
            "style": "IPY_MODEL_34bac034017a43dba58c28c7e520f8b1",
            "value": "Generating train split: 100%"
          }
        },
        "49ff4a08713f49f49b687a3dd6ca1f0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a0e30b9837640b7afc17fd47075a4f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8d5d268520e41e1a2d9799282b26e9d",
            "placeholder": "​",
            "style": "IPY_MODEL_768258bc409b4e9d808a97198cd173e0",
            "value": " 17.2M/17.2M [00:01&lt;00:00, 12.4MB/s]"
          }
        },
        "4b067e2fbddb4d36824e44914b686898": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_328d5557de3c43ceb18a2cc3ca451025",
            "placeholder": "​",
            "style": "IPY_MODEL_3dd0f9ac6bae447f94637fbb7a5362d2",
            "value": " 3392/3392 [00:00&lt;00:00, 10728.08 examples/s]"
          }
        },
        "4bf4dee1ea944b2ea71ba30925639cf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d27ed6e473ee4f8f97ada5a6a181dd16",
              "IPY_MODEL_175c9d72cc0249cc9e2ab1bc0d86405a",
              "IPY_MODEL_4b067e2fbddb4d36824e44914b686898"
            ],
            "layout": "IPY_MODEL_d419c950a6b94e8c9141e8e690148c83"
          }
        },
        "4d48b315656443ccb7e5656ea940f017": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7548e757996416e99109a3b6c55fd16",
            "placeholder": "​",
            "style": "IPY_MODEL_d18f9537929e4b5fb5009ce426791da5",
            "value": " 235/235 [00:00&lt;00:00, 19.4kB/s]"
          }
        },
        "4dfa74df37a34185ac5bdf1a2fc881e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e27449c8fc740eba93cb175b6c103fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e92b7c3e8ff4347971267aee205490a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "510c2f7c0f8f44fdab1eb7a1fe0d6799": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "556bbe564f4543b7a63ef1cf058623b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57a5483bf4fd4726a4d0d434e7aca338": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5801c065fa6e415d97ef3891d5c3d89a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "583702b26d274214a9b5f2e75b383888": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23ecab9a709a4ac786a4e4fc99c2a1a9",
            "max": 167832240,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f109f1cc0f04a7ca7d19ea508be1cb0",
            "value": 167832240
          }
        },
        "587488a53a0c4d10a93836ad36bfe60a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59056bccf795490d98664aadabc619c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "59bf80d05f654c44b87d3bfa1e2364a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cdc6d3a8d274913ada4ee4cb2b0715e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_060dbb641b324c7092f1aa46268ae354",
            "max": 1870,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f5b42bc85c374d5bb49cdfd77a636d73",
            "value": 1870
          }
        },
        "5f07a4774b0d4863b333dc5b4ea3555a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "617169c3b0af4b00a66d621469e87b94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4531109e7a27424b87ba16fe473a7e22",
            "placeholder": "​",
            "style": "IPY_MODEL_1ff388a8c8444254b1b218225af1e506",
            "value": "generation_config.json: 100%"
          }
        },
        "622204f814074b249422134e8d144f1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "62c9849bf9fe41e3afa4f528bfa321f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6337963a2bd24138892255335ac44128": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02fd407b25914c98993a4ac3d46eff8e",
            "placeholder": "​",
            "style": "IPY_MODEL_bb27d759f4c54c30b5bc4cdb03902622",
            "value": " 51760/51760 [00:01&lt;00:00, 48981.46 examples/s]"
          }
        },
        "6598808733c9480eaf9623b1e4ce0182": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a81cb0643de7456890f9e272c59fb318",
              "IPY_MODEL_3092d6eb5a2d4bfc93704bf02e46afb2",
              "IPY_MODEL_4a0e30b9837640b7afc17fd47075a4f8"
            ],
            "layout": "IPY_MODEL_709aebbe21a54336a907b3dab1872996"
          }
        },
        "673b23bc307a4c2baeca7f3d989207d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68acfb4bf95547848a0fced9e6c0aa29": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b096db4c2454b9eafaa35c6e4b05566": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c66d2d831774e749ccc6263a5339e7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6d90e48cf4bd46239267f2515133d3d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d40768f42f004960adb76a6aea583da6",
            "placeholder": "​",
            "style": "IPY_MODEL_1e6127c6367340f1b4954499dd163b41",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "6e6944997247422ca532ebc63171e804": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb6ca1deeaee47af9c1065a90d1f70ba",
            "placeholder": "​",
            "style": "IPY_MODEL_39ee9f8ba8e74d7b8b1c41c97427c72f",
            "value": "  168MB /  168MB            "
          }
        },
        "709aebbe21a54336a907b3dab1872996": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "713e05a8285c4b338921a63e82a30694": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "739dd44420774a4e89d3f4162c448186": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "768258bc409b4e9d808a97198cd173e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77cfc917e3f4439d978f9290ad2d1ffd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78d83c6676a44873af88b871b4be7a92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c3f9d4c18ba41b980ea9c7e0f7bb486": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e0ce4ea8d6243318f72cbfe1727c37b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f87b910edde4b89b89a71542771a06f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86f60a2df7c74583856dee0e69d4684d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c988da8790f484db0a36f21dc34f371",
            "value": 1
          }
        },
        "8047fc11d6dc4635b42dd0ea6fb69bde": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "805610edc25e4de1a48dbb6f67f27a11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e13d385ea134c2ea25eb5b0d4a4578b",
            "placeholder": "​",
            "style": "IPY_MODEL_854f5c279f8b4984bd036df1df3d9c92",
            "value": " 1870/1870 [00:00&lt;00:00, 3763.11 examples/s]"
          }
        },
        "8154794457f74509b2a5c6ea18c8027e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_33d0ff7b3dad4a719d5414cc77dd7250",
              "IPY_MODEL_97525916b6464a19998a1ff62e810c1c",
              "IPY_MODEL_6337963a2bd24138892255335ac44128"
            ],
            "layout": "IPY_MODEL_888c066045fc44c19ca8c7c26b9b6ea4"
          }
        },
        "818881c0a28e4ca6a325dcc3c2caa14f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_617169c3b0af4b00a66d621469e87b94",
              "IPY_MODEL_364d047cf1044594a5336d4659f24074",
              "IPY_MODEL_4d48b315656443ccb7e5656ea940f017"
            ],
            "layout": "IPY_MODEL_1d4995e0f8684ee7acefebd6fd1caca3"
          }
        },
        "81bcbbc9d7574220bf8f9b3ed40c7467": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b86bb63b80fb41c0bcdbd4fcfca1b6fe",
              "IPY_MODEL_314fe0becd0248648b6c39cc3eb839d8",
              "IPY_MODEL_17bbd1d8e24c4c2494d3b1e5efc9a7c3"
            ],
            "layout": "IPY_MODEL_6b096db4c2454b9eafaa35c6e4b05566"
          }
        },
        "8486203cb1454c5996944f60ce56e0ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "854f5c279f8b4984bd036df1df3d9c92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85daa7a105c64380a682620a9cb2ee96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86d5585d1cf047578dd9e9b4c6b2c50a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e86d45ab807c4053bc8bd041d1677c22",
              "IPY_MODEL_b26a8ca5860f4c23ac7a13726b08f78d",
              "IPY_MODEL_f7f23f23751c4ce9af26ce8126d03343"
            ],
            "layout": "IPY_MODEL_21f9d25a06154a89be7763626bf428be"
          }
        },
        "86f60a2df7c74583856dee0e69d4684d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "87748d6d8c6c4fd7abb62ffe71bb126a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "888c066045fc44c19ca8c7c26b9b6ea4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a00f259cf9a48059b31bfaa2386dc17": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bcf6c8b0701413696ac7af3a6febeed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c82af98ef85481da01f9f937c37ced2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ff22b6aec10444d852dc071374ecd8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01353edbf0534514a8762146b84f55e5",
            "placeholder": "​",
            "style": "IPY_MODEL_32183ededf184d2da61eb6529c36f4ec",
            "value": "data/train-00000-of-00001.parquet: 100%"
          }
        },
        "90b604e36b2c4a639bead4d06e8a34e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9560fbbb2020494284cfa52f3aba0ced": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87748d6d8c6c4fd7abb62ffe71bb126a",
            "max": 5702746390,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_acef14ef3ea74d389265c95cec7dd5f7",
            "value": 5702746390
          }
        },
        "964d4515de7945b5ac82c9372a08031a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97525916b6464a19998a1ff62e810c1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7f0b3dbc128479389f38d8b34dce034",
            "max": 51760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_622204f814074b249422134e8d144f1f",
            "value": 51760
          }
        },
        "9d7eb01c0e5d44b0a1c06a6a2a59a2bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d852f519cb449e993d54c9d3ac47d48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2f55a0b48894a46af1ee75cf8ae55dc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a457c94501c24c479bd677d8e5f71317": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a81cb0643de7456890f9e272c59fb318": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ea103fd703c4917ace8df3ce4dc4bcd",
            "placeholder": "​",
            "style": "IPY_MODEL_8bcf6c8b0701413696ac7af3a6febeed",
            "value": "tokenizer.json: 100%"
          }
        },
        "ab204469a3754ed2b9f220b0d74e0d63": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acef14ef3ea74d389265c95cec7dd5f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "afa1b973e435498e9e4d4cad1b5ebfc2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1e35f91bc8e438f9fb839ab582387de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1d6b0fe6e7454620b9380e1632b90e3d",
              "IPY_MODEL_46bedf08f65644b58237ee1f6e18cc84",
              "IPY_MODEL_fd2ebdc3e11844d99bbe1c22b022fc62"
            ],
            "layout": "IPY_MODEL_90b604e36b2c4a639bead4d06e8a34e5"
          }
        },
        "b21d6f7ebf6a4273b1d31645ca13ebf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e65fdd6951334a2a8f42d431254da207",
              "IPY_MODEL_28dee16d89d248a1ac7606faba9de6cd",
              "IPY_MODEL_d5359f910f6f497494fe09d54d3c80eb"
            ],
            "layout": "IPY_MODEL_082c7bcffd274215a1a3b03a56c0a303"
          }
        },
        "b26a8ca5860f4c23ac7a13726b08f78d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a457c94501c24c479bd677d8e5f71317",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c3f9d4c18ba41b980ea9c7e0f7bb486",
            "value": 1
          }
        },
        "b7f0b3dbc128479389f38d8b34dce034": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b86bb63b80fb41c0bcdbd4fcfca1b6fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f59d99fd1d0541d08b2d75528c6dcf0c",
            "placeholder": "​",
            "style": "IPY_MODEL_f7bc9724ea4e4592a3374724078b13aa",
            "value": "README.md: 100%"
          }
        },
        "b9bda7063e364fdc8f371fe6f60ab425": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb27d759f4c54c30b5bc4cdb03902622": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd014ce911cc4905bffa72939dca8cbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_49df1af933914d1aab887f876b78012a",
              "IPY_MODEL_5cdc6d3a8d274913ada4ee4cb2b0715e",
              "IPY_MODEL_805610edc25e4de1a48dbb6f67f27a11"
            ],
            "layout": "IPY_MODEL_a2f55a0b48894a46af1ee75cf8ae55dc"
          }
        },
        "bebc043d89a74a3bb9f9f812807b8c84": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfece21a2b704a4e8f4e8c3159fd0a50": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9364c8b32fd40168e6735f39b80a210": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb3293c9c37147d1997ab3e6cdc41cbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b3a2d641ac549e7b1b64af719abc05e",
              "IPY_MODEL_583702b26d274214a9b5f2e75b383888",
              "IPY_MODEL_6e6944997247422ca532ebc63171e804"
            ],
            "layout": "IPY_MODEL_fcc4ddc74fd64b7ab4332baf915a47df"
          }
        },
        "d18f9537929e4b5fb5009ce426791da5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d27ed6e473ee4f8f97ada5a6a181dd16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bebc043d89a74a3bb9f9f812807b8c84",
            "placeholder": "​",
            "style": "IPY_MODEL_7e0ce4ea8d6243318f72cbfe1727c37b",
            "value": "Map: 100%"
          }
        },
        "d32fbf3c48cf487088bb8fc612ff633d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d40768f42f004960adb76a6aea583da6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d419c950a6b94e8c9141e8e690148c83": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4d21f5053ab44e5beaf6dda775b7e63": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5359f910f6f497494fe09d54d3c80eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4981fbe58c31449dad4864b8ba2c151c",
            "placeholder": "​",
            "style": "IPY_MODEL_f29c365803104ffbaa3b50f167fd08ec",
            "value": " 50.6k/? [00:00&lt;00:00, 4.99MB/s]"
          }
        },
        "da505427887442579ccdad3fd452584e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddfdd8e47c454b17b2fd3a7ae751a821": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df8cb7c4b09d4b1ab1d49591f89b23fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f1969ec790764fb7878ec570464cf844",
              "IPY_MODEL_9560fbbb2020494284cfa52f3aba0ced",
              "IPY_MODEL_241be6f77a2247999a6adb196047b6fd"
            ],
            "layout": "IPY_MODEL_556bbe564f4543b7a63ef1cf058623b7"
          }
        },
        "e0e224471a24406dbc6ca8e837c91fd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d32fbf3c48cf487088bb8fc612ff633d",
            "placeholder": "​",
            "style": "IPY_MODEL_f8cc9eaec2b54c859e74b0401493d111",
            "value": "  168MB /  168MB, 16.8MB/s  "
          }
        },
        "e1fd61cc64eb453394d31ecfbeedf9f4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5b241ec2e01445aa5b3ecb8430c2329": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da505427887442579ccdad3fd452584e",
            "placeholder": "​",
            "style": "IPY_MODEL_8486203cb1454c5996944f60ce56e0ef",
            "value": "Processing Files (1 / 1)                : 100%"
          }
        },
        "e65fdd6951334a2a8f42d431254da207": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_713e05a8285c4b338921a63e82a30694",
            "placeholder": "​",
            "style": "IPY_MODEL_42412db9b76f4bbfa67056c8acb9bab5",
            "value": "tokenizer_config.json: "
          }
        },
        "e7548e757996416e99109a3b6c55fd16": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e794823e257646199142d5a2d34322c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e86d45ab807c4053bc8bd041d1677c22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c82af98ef85481da01f9f937c37ced2",
            "placeholder": "​",
            "style": "IPY_MODEL_008dc8b8ea4f4cee91b93fbdea194eb8",
            "value": "New Data Upload                         : 100%"
          }
        },
        "eb30f78f3bc94e40a46574c546c9fd93": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef180749a010442e8c54a9b68e2c47cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afa1b973e435498e9e4d4cad1b5ebfc2",
            "max": 834762,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_021a36dd1b7f49729060be7457e40bf2",
            "value": 834762
          }
        },
        "efd013183fd64920b0295e5a479e0868": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f001e1bd936b4c93bb5328f09ca208c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f09dff6f74fb4445817d155435b3e7c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1969ec790764fb7878ec570464cf844": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab204469a3754ed2b9f220b0d74e0d63",
            "placeholder": "​",
            "style": "IPY_MODEL_5801c065fa6e415d97ef3891d5c3d89a",
            "value": "model.safetensors: 100%"
          }
        },
        "f29c365803104ffbaa3b50f167fd08ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3d739a53fd34090920d1a5c9893f863": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f59d99fd1d0541d08b2d75528c6dcf0c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5b42bc85c374d5bb49cdfd77a636d73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f7bc9724ea4e4592a3374724078b13aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7f23f23751c4ce9af26ce8126d03343": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f09dff6f74fb4445817d155435b3e7c2",
            "placeholder": "​",
            "style": "IPY_MODEL_78d83c6676a44873af88b871b4be7a92",
            "value": "  168MB /  168MB, 16.8MB/s  "
          }
        },
        "f8cc9eaec2b54c859e74b0401493d111": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8d5d268520e41e1a2d9799282b26e9d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8f3ca71aba6495fa9ebb32f3bd60ab3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f9daabb2fced47849eaeffdab8b586b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e017dfdc4ad41afa93e234eaad42dfb",
            "placeholder": "​",
            "style": "IPY_MODEL_4e92b7c3e8ff4347971267aee205490a",
            "value": "Unsloth: Tokenizing [&quot;text&quot;] (num_proc=2): 100%"
          }
        },
        "fafc4021a849469fb59b5df0987fd0b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efd013183fd64920b0295e5a479e0868",
            "placeholder": "​",
            "style": "IPY_MODEL_5f07a4774b0d4863b333dc5b4ea3555a",
            "value": " 835k/835k [00:02&lt;00:00, 296kB/s]"
          }
        },
        "fb6ca1deeaee47af9c1065a90d1f70ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcc4ddc74fd64b7ab4332baf915a47df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd2ebdc3e11844d99bbe1c22b022fc62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a00f259cf9a48059b31bfaa2386dc17",
            "placeholder": "​",
            "style": "IPY_MODEL_59bf80d05f654c44b87d3bfa1e2364a9",
            "value": " 3392/3392 [00:00&lt;00:00, 14533.80 examples/s]"
          }
        },
        "fdb9ff25e5054095ad4a784a5b0099ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
