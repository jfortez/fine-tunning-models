{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "067529bb",
   "metadata": {},
   "source": [
    "# Generacion de Datasets utilizando RLHF para fine tunning\n",
    "\n",
    "## Procedimiento\n",
    "\n",
    "- Recoleccion de Datos: Obtener Informacion mediante documentos de textos seleccionables (no escaneados), mediante web, redes sociales, scrapping,etc\n",
    "- Extraer Texto: Usa PyMuPDF para extraer texto de PDFs no escaneados, procesando en paralelo con ProcessPoolExecutor para manejar grandes volúmenes (>100, >10,000).\n",
    "- Limpiar Datos: Elimina espacios múltiples y caracteres no deseados con expresiones regulares, asegurando texto coherente.\n",
    "- Validar: Verifica que los fragmentos extraídos no estén vacíos y tengan una longitud mínima (por ejemplo, 50 caracteres).\n",
    "- Generar Conversaciones: Divide el texto en párrafos, usa un modelo como meta-llama/Llama-3.2-1B-Instruct para generar diálogos conversacionales específicos al contenido, con al menos 4 intercambios por conversación, en formato {\"messages\": [{\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}], \"topic\": \"...\"}.\n",
    "- Estructurar Dataset: Guarda las conversaciones en archivos JSONL con dos columnas: \"messages\" y \"topic\".\n",
    "Cargar y Subir: Carga el dataset con load_dataset(\"json\", data_files=\"path/*.jsonl\") y súbelo a un repositorio privado en Hugging Face Hub con push_to_hub(\"username/dataset_name\", private=True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45295ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in /workspace/venv/lib/python3.11/site-packages (1.26.3)\n",
      "Requirement already satisfied: nltk in /workspace/venv/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: ollama in /workspace/venv/lib/python3.11/site-packages (0.5.1)\n",
      "Collecting ollama\n",
      "  Downloading ollama-0.5.3-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: openai in /workspace/venv/lib/python3.11/site-packages (1.99.1)\n",
      "Collecting openai\n",
      "  Downloading openai-1.99.3-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: click in /workspace/venv/lib/python3.11/site-packages (from nltk) (8.2.2)\n",
      "Requirement already satisfied: joblib in /workspace/venv/lib/python3.11/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /workspace/venv/lib/python3.11/site-packages (from nltk) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in /workspace/venv/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: httpx>=0.27 in /workspace/venv/lib/python3.11/site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in /workspace/venv/lib/python3.11/site-packages (from ollama) (2.11.7)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /workspace/venv/lib/python3.11/site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /workspace/venv/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /workspace/venv/lib/python3.11/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /workspace/venv/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /workspace/venv/lib/python3.11/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /workspace/venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /workspace/venv/lib/python3.11/site-packages (from httpx>=0.27->ollama) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /workspace/venv/lib/python3.11/site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /workspace/venv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /workspace/venv/lib/python3.11/site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /workspace/venv/lib/python3.11/site-packages (from pydantic>=2.9->ollama) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /workspace/venv/lib/python3.11/site-packages (from pydantic>=2.9->ollama) (0.4.1)\n",
      "Downloading ollama-0.5.3-py3-none-any.whl (13 kB)\n",
      "Downloading openai-1.99.3-py3-none-any.whl (785 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.8/785.8 kB\u001b[0m \u001b[31m187.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: openai, ollama\n",
      "\u001b[2K  Attempting uninstall: openai\n",
      "\u001b[2K    Found existing installation: openai 1.99.1\n",
      "\u001b[2K    Uninstalling openai-1.99.1:━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [openai]\n",
      "\u001b[2K      Successfully uninstalled openai-1.99.1━━━━\u001b[0m \u001b[32m0/2\u001b[0m [openai]\n",
      "\u001b[2K  Attempting uninstall: ollama━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [openai]\n",
      "\u001b[2K    Found existing installation: ollama 0.5.10m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [ollama]\n",
      "\u001b[2K    Uninstalling ollama-0.5.1:90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [ollama]\n",
      "\u001b[2K      Successfully uninstalled ollama-0.5.1━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [ollama]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [ollama]2m1/2\u001b[0m [ollama]\n",
      "\u001b[1A\u001b[2KSuccessfully installed ollama-0.5.1 openai-1.99.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pymupdf nltk ollama openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fa351d6-c527-423f-9d2e-417d0ef2b8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4760f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OLLAMA_MODEL = \"llama3.1:8b\"\n",
    "TEMPERATURE = 0.7  # Equilibrio entre creatividad y coherencia\n",
    "TOP_P = 0.9  # Filtrado de núcleo para diversidad\n",
    "MIN_CONVERSATION_LENGTH = 3 \n",
    "\n",
    "# Constantes\n",
    "MIN_FRAGMENT_LENGTH = 500\n",
    "MAX_FRAGMENT_LENGTH = 2000\n",
    "REPEAT_THRESHOLD = 0.3\n",
    "MIN_BLOCK_LENGTH = 30  # Reducido de 50 a 30 para incluir bloques más cortos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62be033f",
   "metadata": {},
   "source": [
    "### Recoleccion de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b45b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Verificar si la carpeta existe\n",
    "\n",
    "def get_files():\n",
    "    folder_url = \"./docs\"\n",
    "    folder = Path(folder_url)\n",
    "\n",
    "\n",
    "    if not folder.exists() or not folder.is_dir():\n",
    "        print(\"Invalid Folder\")\n",
    "        \n",
    "    # Obtener todos los archivos de la carpeta\n",
    "    files = [f for f in folder.rglob(\"*\") if f.is_file()]\n",
    "\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce4f060",
   "metadata": {},
   "source": [
    "### Extraccion de Texto y Limpieza de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ed080bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import re\n",
    "import os\n",
    "import hashlib\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text, repeated_blocks=None):\n",
    "    \"\"\"Limpia el texto, eliminando caracteres repetitivos, texto redundante y caracteres no deseados.\"\"\"\n",
    "    # Eliminar caracteres de control no deseados (excepto \\n)\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]', '', text)\n",
    "    # Eliminar patrones repetitivos como ----, ...., ****\n",
    "    text = re.sub(r'([^\\w\\s])\\1{2,}|\\s*[.]{3,}\\s*', '', text)\n",
    "    # Normalizar espacios múltiples\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    # Eliminar espacios al inicio y final de cada línea\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    # Filtrar líneas duplicadas, números, correos y contenido administrativo\n",
    "    seen_lines = set()\n",
    "    unique_lines = []\n",
    "    for line in lines:\n",
    "        if line not in seen_lines and \\\n",
    "           not re.match(r'^\\d+$', line) and \\\n",
    "           not re.match(r'.*@(.*\\.)+.*', line) and \\\n",
    "           not re.match(r'^(Tel|Fax|E-mail|www\\.).*', line, re.IGNORECASE) and \\\n",
    "           (repeated_blocks is None or line not in repeated_blocks):\n",
    "            unique_lines.append(line)\n",
    "            seen_lines.add(line)\n",
    "    return '\\n'.join(unique_lines)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extrae texto de un PDF en una sola pasada, preservando el texto del inicio de la página,\n",
    "    eliminando encabezados, pies de página y contenido irrelevante,\n",
    "    dividiendo en fragmentos de 500 a 2000 caracteres con metadata corregida.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = pymupdf.open(pdf_path)\n",
    "        total_pages = len(doc)\n",
    "        chunks = []\n",
    "        current_chunk = []  # Lista de (párrafo, página)\n",
    "        current_chunk_length = 0\n",
    "        filename = os.path.basename(pdf_path)\n",
    "        block_counter = Counter()\n",
    "\n",
    "        for page_number in range(1, total_pages + 1):\n",
    "            page = doc[page_number - 1]\n",
    "            page_height = page.rect.height\n",
    "            blocks = page.get_text(\"blocks\")\n",
    "            page_text = []\n",
    "\n",
    "            # Procesar bloques y filtrar encabezados/pies de página\n",
    "            for block in blocks:\n",
    "                text = block[4]\n",
    "                y0, y1 = block[1], block[3]\n",
    "                # Excluir pies de página (parte inferior de la página)\n",
    "                if y1 > 0.95 * page_height:  # Relajado de 0.95 a 0.9\n",
    "                    continue\n",
    "                # Excluir encabezados solo si son repetitivos\n",
    "                if y0 < 0.05 * page_height:  # Relajado de 0.05 a 0.1\n",
    "                    block_counter[text] += 1\n",
    "                    if block_counter[text] > total_pages * REPEAT_THRESHOLD:\n",
    "                        continue\n",
    "                if text and len(text) >= MIN_BLOCK_LENGTH:\n",
    "                    block_counter[text] += 1\n",
    "                    page_text.append((text, page_number))\n",
    "\n",
    "            # Si no hay texto válido en la página, continuar\n",
    "            if not page_text:\n",
    "                continue\n",
    "\n",
    "            # Acumular párrafos con su número de página\n",
    "            for paragraph, page in page_text:\n",
    "                current_chunk.append((paragraph, page))\n",
    "                current_chunk_length += len(paragraph) + 2  # +2 por \"\\n\\n\"\n",
    "\n",
    "                # Si el fragmento alcanza la longitud mínima, procesarlo\n",
    "                if current_chunk_length >= MIN_FRAGMENT_LENGTH:\n",
    "                    chunk_text = \"\\n\\n\".join(p for p, _ in current_chunk)\n",
    "                    cleaned_chunk = clean_text(chunk_text, None)\n",
    "                    cleaned_length = len(cleaned_chunk)\n",
    "\n",
    "                    # Obtener el rango de páginas del fragmento\n",
    "                    page_numbers = sorted(set(page for _, page in current_chunk))\n",
    "                    start_page = page_numbers[0] if page_numbers else page_number\n",
    "                    end_page = page_numbers[-1] if page_numbers else page_number\n",
    "\n",
    "                    # Dividir fragmentos largos\n",
    "                    while cleaned_length > MAX_FRAGMENT_LENGTH:\n",
    "                        sub_chunk = cleaned_chunk[:MAX_FRAGMENT_LENGTH]\n",
    "                        last_paragraph_end = sub_chunk.rfind(\"\\n\\n\")\n",
    "                        if last_paragraph_end == -1:\n",
    "                            last_paragraph_end = MAX_FRAGMENT_LENGTH\n",
    "                        chunk_to_add = cleaned_chunk[:last_paragraph_end].strip()\n",
    "\n",
    "                        # Calcular el número de páginas para el subfragmento\n",
    "                        chars_so_far = 0\n",
    "                        sub_chunk_pages = []\n",
    "                        for paragraph, page in current_chunk:\n",
    "                            chars_so_far += len(paragraph) + 2\n",
    "                            if chars_so_far <= last_paragraph_end:\n",
    "                                sub_chunk_pages.append(page)\n",
    "                            else:\n",
    "                                break\n",
    "                        sub_start_page = min(sub_chunk_pages) if sub_chunk_pages else page_number\n",
    "                        sub_end_page = max(sub_chunk_pages) if sub_chunk_pages else page_number\n",
    "\n",
    "                        metadata = (\n",
    "                            f\"# FILENAME: {filename} | CHARACTERS: {len(chunk_to_add)} | \"\n",
    "                            f\"PAGES: {sub_start_page}-{sub_end_page}/{total_pages}\\n\\n\"\n",
    "                        )\n",
    "                        chunks.append((metadata + chunk_to_add, hashlib.md5(chunk_to_add.encode()).hexdigest()))\n",
    "                        cleaned_chunk = cleaned_chunk[last_paragraph_end:].strip()\n",
    "                        cleaned_length = len(cleaned_chunk)\n",
    "\n",
    "                        # Actualizar current_chunk para los párrafos restantes\n",
    "                        remaining_chunk = []\n",
    "                        chars_so_far = 0\n",
    "                        for paragraph, page in current_chunk:\n",
    "                            chars_so_far += len(paragraph) + 2\n",
    "                            if chars_so_far > last_paragraph_end:\n",
    "                                remaining_chunk.append((paragraph, page))\n",
    "                        current_chunk = remaining_chunk\n",
    "                        current_chunk_length = cleaned_length\n",
    "                        page_numbers = sorted(set(page for _, page in current_chunk))\n",
    "                        start_page = page_numbers[0] if page_numbers else page_number\n",
    "\n",
    "                    # Añadir el fragmento completo\n",
    "                    if cleaned_length >= MIN_FRAGMENT_LENGTH:\n",
    "                        metadata = (\n",
    "                            f\"# FILENAME: {filename} | CHARACTERS: {cleaned_length} | \"\n",
    "                            f\"PAGES: {start_page}-{end_page}/{total_pages}\\n\\n\"\n",
    "                        )\n",
    "                        chunks.append((metadata + cleaned_chunk, hashlib.md5(cleaned_chunk.encode()).hexdigest()))\n",
    "                        current_chunk = []\n",
    "                        current_chunk_length = 0\n",
    "\n",
    "                    else:\n",
    "                        current_chunk = [(cleaned_chunk, page_numbers[-1])] if page_numbers else []\n",
    "                        current_chunk_length = cleaned_length\n",
    "\n",
    "        # Añadir el fragmento final si cumple con la longitud mínima\n",
    "        if current_chunk and current_chunk_length >= MIN_FRAGMENT_LENGTH:\n",
    "            chunk_text = \"\\n\\n\".join(p for p, _ in current_chunk)\n",
    "            cleaned_chunk = clean_text(chunk_text, None)\n",
    "            cleaned_length = len(cleaned_chunk)\n",
    "            if cleaned_length >= MIN_FRAGMENT_LENGTH:\n",
    "                page_numbers = sorted(set(page for _, page in current_chunk))\n",
    "                start_page = page_numbers[0] if page_numbers else total_pages\n",
    "                end_page = page_numbers[-1] if page_numbers else total_pages\n",
    "                metadata = (\n",
    "                    f\"# FILENAME: {filename} | CHARACTERS: {cleaned_length} | \"\n",
    "                    f\"PAGES: {start_page}-{end_page}/{total_pages}\\n\\n\"\n",
    "                )\n",
    "                chunks.append((metadata + cleaned_chunk, hashlib.md5(cleaned_chunk.encode()).hexdigest()))\n",
    "\n",
    "        doc.close()\n",
    "\n",
    "        # Filtrar bloques repetitivos y duplicados\n",
    "        repeated_blocks = {text for text, count in block_counter.items() if count > total_pages * REPEAT_THRESHOLD}\n",
    "        final_chunks = []\n",
    "        seen_hashes = set()\n",
    "\n",
    "        for chunk, chunk_hash in chunks:\n",
    "            chunk_text = '\\n'.join(line for line in chunk.splitlines() if not line.startswith('#'))\n",
    "            cleaned_chunk = clean_text(chunk_text, repeated_blocks)\n",
    "            if len(cleaned_chunk) >= MIN_FRAGMENT_LENGTH and chunk_hash not in seen_hashes:\n",
    "                # Actualizar la longitud en la metadata después de la limpieza final\n",
    "                metadata_lines = chunk.splitlines()[0]\n",
    "                metadata = re.sub(r'CHARACTERS: \\d+', f'CHARACTERS: {len(cleaned_chunk)}', metadata_lines)\n",
    "                final_chunks.append(f\"{metadata}\\n\\n{cleaned_chunk}\")\n",
    "                seen_hashes.add(chunk_hash)\n",
    "\n",
    "        return final_chunks if final_chunks else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {pdf_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a912cd27",
   "metadata": {},
   "source": [
    "### Generar Conversaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c7a947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "class Message(BaseModel):\n",
    "    role: str = Field(..., pattern=\"^(user|assistant)$\")\n",
    "    content: str\n",
    "\n",
    "class Conversation(BaseModel):\n",
    "    messages: List[Message] = Field(..., min_items=MIN_CONVERSATION_LENGTH)\n",
    "    topic: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82198409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompts(fragment: str):\n",
    "  INSTRUCTIONS = \"\"\"\n",
    "  Eres un generador de conversaciones optimizadas para entrenamiento supervisado (SFT) con técnicas avanzadas de prompt engineering. \n",
    "Tu objetivo es producir diálogos profesionales, coherentes y progresivos basados en fragmentos de texto, orientados al ámbito laboral. \n",
    "Cada conversación debe tener continuidad entre turnos y mantener un estilo claro, formal pero natural.\n",
    "\n",
    "<instrucciones>\n",
    "\n",
    "1. **Formato de salida**:\n",
    "- Devuelve un JSON válido con:\n",
    "  - `messages`: Lista con { \"role\": \"user\" | \"assistant\", \"content\": \"...\" }\n",
    "  - `topic`: cadena breve (máx. 50 caracteres)\n",
    "- **No incluir rol \"system\"** en la salida final.\n",
    "\n",
    "2. **Estructura de la conversación**:\n",
    "- 3 a 4 intercambios (mínimo 4 y máximo 8 mensajes).\n",
    "- Siempre empieza con \"user\" y alterna roles.\n",
    "- Cada mensaje del usuario debe partir del contexto previo y avanzar el tema.\n",
    "- Respuestas del asistente con razonamiento paso a paso (chain-of-thought) y ejemplos prácticos.\n",
    "- Usa Markdown para estructurar la respuesta: títulos, listas, negritas, etc.\n",
    "\n",
    "3. **Tono y estilo**:\n",
    "- Profesional y aplicable en contextos de trabajo (consultoría, gestión de proyectos, análisis técnico, comunicación corporativa).\n",
    "- Lenguaje formal pero natural, evitando jerga innecesaria.\n",
    "- Respuestas concisas pero completas, con enfoque en resultados y acciones concretas.\n",
    "\n",
    "4. **Patrones de pregunta** (adaptados a entornos laborales):\n",
    "- \"Me gustaría implementar {tema} en la empresa. ¿Qué pasos sugieres?\"\n",
    "- \"Basado en {contexto}, propón un plan detallado para {objetivo}.\"\n",
    "- \"Genera un cronograma para {proyecto}, asignando responsables.\"\n",
    "- \"Identifica riesgos y métricas para {iniciativa}.\"\n",
    "- \"Crea un documento de recomendaciones sobre {tema}.\"\n",
    "- \"Evalúa ventajas y desventajas de {opción1} frente a {opción2}.\"\n",
    "- \"Desglosa en fases el proceso para {tarea}.\"\n",
    "- \"Propón mejoras para optimizar {área o proceso}.\"\n",
    "- \"Dame indicadores clave para medir {resultado}.\"\n",
    "- \"Elabora un resumen ejecutivo sobre {tema} para la gerencia.\"\n",
    "\n",
    "5. **Criterios de calidad**:\n",
    "- Coherencia en todos los turnos.\n",
    "- Uso de información previa en las respuestas siguientes.\n",
    "- Evitar repeticiones innecesarias.\n",
    "- Claridad en la explicación y aplicabilidad práctica.\n",
    "\n",
    "\n",
    "<PROMPT_PATTERNS>\n",
    "- \"Me intriga {tema}. ¿Puedes explicármelo detalladamente?\"\n",
    "- \"En pocas palabras, resúmeme cómo funciona {tema}.\"\n",
    "- \"Crea una carta formal sobre {tema} dirigida a {destinatario}.\"\n",
    "- \"Basado en {contexto}, explícame qué significa {tema}.\"\n",
    "- \"Quiero mejorar en {tema}. ¿Qué consejos prácticos me das?\"\n",
    "- \"Agrega una sección sobre {tema} a este informe: {contexto}.\"\n",
    "- \"No entiendo {tema}. ¿Puedes desglosarlo en pasos simples?\"\n",
    "- \"Imagina que soy nuevo en {tema}. ¿Cómo me lo explicarías?\"\n",
    "- \"Podrías darme un resumen sobre {tema} y sus implicaciones?\"\n",
    "- \"Estoy preocupado por {tema}. ¿Qué debería saber al respecto?\"\n",
    "- \"Explícame {tema} como si fuera un niño de 10 años.\"\n",
    "- \"Redacta un correo informal sobre {tema} para un colega.\"\n",
    "</PROMPT_PATTERNS>\n",
    "\"\"\"\n",
    "\n",
    "  BASE_EXAMPLE = \"\"\"\"\n",
    "\n",
    "**Ejemplo n — Gestión Documental**\n",
    "<fragment>\n",
    "Implementar un sistema de gestión documental para mejorar la búsqueda y control de versiones.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Necesito un plan para implementar un sistema de gestión documental en la empresa. Tenemos problemas con la búsqueda de archivos y duplicación de versiones.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Claro. Vamos a estructurarlo paso a paso:\\\\n\\\\n**1. Análisis inicial**\\\\n- Auditar los flujos actuales de manejo documental.\\\\n- Identificar carpetas duplicadas y problemas de control de versiones.\\\\n\\\\n**2. Selección de plataforma**\\\\n- Comparar opciones como SharePoint, Alfresco o Google Workspace.\\\\n- Priorizar integración, seguridad y facilidad de búsqueda.\\\\n\\\\n**3. Migración y normalización**\\\\n- Definir nomenclatura estándar.\\\\n- Centralizar repositorios.\\\\n\\\\n**4. Capacitación y seguimiento**\\\\n- Formación en la plataforma elegida.\\\\n- Revisión mensual para detectar desviaciones.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Sí, dame el cronograma y asigna responsables para un equipo de 5 personas en administración.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"**Cronograma — Implementación Gestión Documental**\\\\n\\\\n**Semana 1-2 — Auditoría**\\\\n- Responsable: Laura (Jefa de administración)\\\\n\\\\n**Semana 3 — Selección de plataforma**\\\\n- Responsable: Jorge (Analista de sistemas)\\\\n\\\\n**Semana 4-6 — Migración inicial**\\\\n- Responsables: Ana y Ricardo\\\\n\\\\n**Semana 7 — Capacitación**\\\\n- Responsable: Mariana\\\\n\\\\n**Semana 8-9 — Seguimiento**\\\\n- Responsable: Laura\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Perfecto. ¿Puedes incluir métricas para evaluar si la implementación fue exitosa?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"**Métricas de éxito**:\\\\n- Tiempo de búsqueda reducido en 50%.\\\\n- Menos del 5% de duplicados.\\\\n- 90% de cumplimiento en nomenclatura.\\\\n- 95% de adopción entre usuarios.\\\\n- Satisfacción ≥ 8/10 en encuestas internas.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Implementación de gestión documental\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  EXAMPLES = \"\"\"\n",
    "<ejemplos>\n",
    "\n",
    "Ejemplo Base:\n",
    "%s\n",
    "\n",
    "Otros Ejemplos:\n",
    "\n",
    "**Ejemplo 1 — Técnico (dispersión de luz, patrón: explicación detallada, fragmento largo)**:\n",
    "<fragment>\n",
    "La dispersión de Rayleigh explica por qué el cielo aparece azul durante el día. Las partículas en la atmósfera dispersan preferentemente las longitudes de onda cortas, como el azul, debido a su interacción con la luz solar. Durante el atardecer, el cielo adquiere tonos rojos porque la luz atraviesa más atmósfera, dispersando el azul y dejando predominar las longitudes de onda largas. Este fenómeno depende de la densidad atmosférica y el ángulo de incidencia de la luz solar.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Me intriga por qué el cielo es azul de día y rojo al atardecer. ¿Puedes explicármelo detalladamente?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"# Dispersión de Rayleigh y el Color del Cielo\\n\\nPara entender por qué el cielo cambia de color, desglosemos el fenómeno paso a paso:\\n\\n1. **Composición de la luz solar**: La luz del sol parece blanca, pero está formada por todas las longitudes de onda visibles, desde el rojo (largo) hasta el violeta (corto).\\n\\n2. **Dispersión de Rayleigh**: En la atmósfera, las partículas (como moléculas de nitrógeno y oxígeno) dispersan la luz. La dispersión es más intensa para longitudes de onda cortas, como el azul, porque es inversamente proporcional a la **cuarta potencia** de la longitud de onda. Esto significa que el azul se esparce mucho más que el rojo.\\n\\n3. **Cielo diurno**: Durante el día, la luz solar atraviesa una capa relativamente delgada de atmósfera. El azul se dispersa en todas direcciones, haciendo que el cielo se vea azul. Aunque el violeta se dispersa más, nuestros ojos son menos sensibles a él, y parte es absorbida, por lo que el azul domina.\\n\\n4. **Atardecer**: Al atardecer, el sol está más bajo, y la luz atraviesa una capa más gruesa de atmósfera. Esto dispersa casi todo el azul, permitiendo que las longitudes de onda largas (rojo, naranja) lleguen a nuestros ojos, dando tonos cálidos.\\n\\n5. **Factores adicionales**: La densidad atmosférica y el ángulo de incidencia afectan la intensidad del fenómeno. Por ejemplo, más partículas (como polvo) pueden intensificar los colores del atardecer.\\n\\nEn resumen, la dispersión de Rayleigh explica el azul diurno y los tonos rojos del atardecer, influenciada por la física de la luz y la geometría de la Tierra.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"¿Por qué a veces el cielo se ve más violeta en ciertos atardeceres?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"A veces, el cielo puede parecer violeta en el atardecer por la presencia de partículas adicionales, como polvo o contaminación, que dispersan más longitudes de onda cortas, incluyendo el violeta. Además, si la atmósfera está muy limpia y el sol está en un ángulo específico, el violeta puede ser más visible porque menos se absorbe. Sin embargo, nuestros ojos suelen percibir más el azul, así que el violeta es menos común.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Dispersión de luz y color del cielo\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 2 — Técnico (dispersión de luz, patrón: resumen breve, fragmento corto)**:\n",
    "<fragment>\n",
    "La dispersión de Rayleigh hace que el cielo sea azul al dispersar más las longitudes de onda cortas.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"En pocas palabras, resúmeme cómo funciona la dispersión de luz en el cielo.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"La dispersión de Rayleigh ocurre cuando las partículas atmosféricas dispersan más las longitudes de onda cortas, como el azul, haciendo que el cielo se vea azul durante el día.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Dispersión de luz en el cielo\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 3 — Contextual (política monetaria, patrón: carta formal, fragmento largo)**:\n",
    "<fragment>\n",
    "Subir las tasas de interés es una herramienta del banco central para controlar la inflación. Al encarecer los préstamos, se reduce el consumo y la inversión, lo que desacelera la economía y disminuye la presión sobre los precios. Sin embargo, un aumento excesivo puede llevar a una recesión, afectando el empleo y el crecimiento económico. Los bancos centrales deben monitorear indicadores como el consumo y el desempleo para ajustar las tasas cuidadosamente.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Crea una carta formal sobre el impacto de las tasas de interés, dirigida al director de mi banco.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"**Asunto**: Preocupación por el Impacto de las Tasas de Interés\\n\\nEstimado Director,\\n\\nMe dirijo a usted para expresar mi preocupación sobre el reciente aumento de las tasas de interés y su impacto en los clientes. Entiendo que el banco central sube las tasas para controlar la inflación, encareciendo los préstamos, lo que reduce el consumo y la presión sobre los precios. Sin embargo, esto afecta directamente a los consumidores, elevando el costo de hipotecas y créditos, y podría desencadenar una recesión si las tasas suben demasiado.\\n\\nSolicito información sobre cómo el banco planea apoyar a los clientes en este contexto, como opciones de refinanciamiento o asesoramiento financiero. Espero su respuesta en un plazo de 7 días hábiles.\\n\\nAtentamente,  \\n[Nombre del Cliente]\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Agrega una mención sobre el impacto en las pequeñas empresas.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"**Asunto**: Preocupación por el Impacto de las Tasas de Interés\\n\\nEstimado Director,\\n\\nMe dirijo a usted para expresar mi preocupación sobre el aumento de las tasas de interés. Estas medidas, destinadas a controlar la inflación al encarecer los préstamos, reducen el consumo y la inversión, afectando tanto a los consumidores como a las pequeñas empresas, que enfrentan mayores costos de financiamiento. Esto podría desencadenar una recesión, impactando el empleo y el crecimiento económico.\\n\\nSolicito información sobre cómo el banco apoyará a los clientes, incluyendo a pequeñas empresas, con opciones como refinanciamiento o asesoramiento financiero. Espero su respuesta en 7 días hábiles.\\n\\nAtentamente,  \\n[Nombre del Cliente]\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Impacto de tasas de interés\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 4 — Incoherente (fragmento ambiguo, patrón: contexto adicional, fragmento corto)**:\n",
    "<fragment>\n",
    "...fluctuaciones de energía... inestabilidad...\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Basado en problemas con el parpadeo de luces en mi casa, explícame qué significan fluctuaciones de energía.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Las fluctuaciones de energía son cambios en el voltaje eléctrico, causando inestabilidad como el parpadeo de luces. Pueden deberse a cableado defectuoso o sobrecarga en la red. Usa un regulador de voltaje o consulta a un electricista.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Fluctuaciones de energía\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 5 — Casual (salud visual, patrón: consejos prácticos, fragmento largo)**:\n",
    "<fragment>\n",
    "La exposición prolongada a la luz azul de pantallas puede interrumpir la producción de melatonina, afectando el ciclo de sueño. Esto ocurre porque la luz azul engaña al cerebro, haciéndole creer que es de día, lo que retrasa el inicio del sueño. También puede causar fatiga visual y molestias como sequedad ocular.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Quiero mejorar mi sueño porque paso mucho tiempo frente a pantallas. ¿Qué consejos prácticos me das?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"# Mejorar el Sueño y Proteger la Vista frente a Pantallas\\n\\nLa luz azul de las pantallas puede afectar tu sueño y ojos, pero hay formas prácticas de mitigar esto. Aquí van algunos consejos:\\n\\n- **Usa el modo nocturno**: Activa el filtro de luz azul en tus dispositivos para reducir su emisión, especialmente por la noche.\\n- **Evita pantallas antes de dormir**: Intenta no usar dispositivos al menos 1 hora antes de acostarte para que la melatonina, la hormona del sueño, se produzca normalmente.\\n- **Aplica la regla 20-20-20**: Cada 20 minutos, mira algo a 20 pies de distancia durante 20 segundos para descansar tus ojos y evitar fatiga visual.\\n- **Considera gafas con filtro**: Si trabajas mucho con pantallas, unas gafas con filtro de luz azul pueden reducir molestias como sequedad ocular.\\n- **Mantén un ambiente adecuado**: Usa iluminación suave en tu espacio de trabajo para reducir el contraste con la pantalla.\\n\\n*Por qué funciona*: La luz azul engaña al cerebro, haciéndole creer que es de día, lo que inhibe la melatonina y retrasa el sueño. Estos pasos ayudan a proteger tu ciclo de sueño y la salud de tus ojos.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"¿Qué hago si no puedo evitar las pantallas por trabajo?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Si no puedes evitar pantallas, refuerza las medidas: usa siempre el modo nocturno, haz pausas frecuentes con la regla 20-20-20, y considera gafas con filtro de luz azul. También ajusta el brillo de la pantalla para que sea similar al de tu entorno y evita trabajar en completa oscuridad.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Luz azul y sueño\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 6 — Contextual (criptografía, patrón: informe, fragmento largo)**:\n",
    "<fragment>\n",
    "La criptografía de clave pública utiliza un par de claves: una pública para cifrar y una privada para descifrar. Esto asegura que solo el destinatario con la clave privada pueda acceder al mensaje, garantizando confidencialidad y autenticidad. Es ampliamente usada en transacciones seguras en línea.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Agrega una sección sobre criptografía de clave pública a este informe: 'Seguridad en transacciones digitales'.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"## Criptografía de Clave Pública en Transacciones Digitales\\n\\nLa **criptografía de clave pública** es un pilar fundamental para la seguridad en transacciones digitales. Funciona mediante un par de claves:\\n\\n- **Clave pública**: Se comparte libremente y permite cifrar mensajes, convirtiéndolos en texto ilegible.\\n- **Clave privada**: Solo la posee el destinatario y permite descifrar el mensaje.\\n\\nEste mecanismo asegura:\\n\\n1. **Confidencialidad**: Solo el destinatario con la clave privada puede leer el mensaje.\\n2. **Autenticidad**: Si el remitente firma con su clave privada, el receptor verifica su identidad con la clave pública.\\n\\n*Ejemplo práctico*: En compras online, la criptografía de clave pública protege los datos de tu tarjeta, asegurando que solo el vendedor autorizado los descifre. Es esencial para protocolos como HTTPS y sistemas de pago seguro.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Criptografía en transacciones\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 7 — Técnico (cambio climático, patrón: pasos simples, fragmento corto)**:\n",
    "<fragment>\n",
    "El cambio climático aumenta las temperaturas globales debido a los gases de efecto invernadero.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"No entiendo el cambio climático. ¿Puedes desglosarlo en pasos simples?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"1. Quemamos combustibles como carbón y petróleo.\\n2. Esto libera gases de efecto invernadero, como el dióxido de carbono.\\n3. Los gases atrapan calor en la atmósfera.\\n4. El planeta se calienta, causando cambio climático.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Cambio climático explicado\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 8 — Casual (nutrición, patrón: explicación para principiantes, fragmento largo)**:\n",
    "<fragment>\n",
    "El consumo excesivo de azúcar puede aumentar el riesgo de diabetes tipo 2 y obesidad. Reducir la ingesta de bebidas azucaradas y alimentos procesados es clave para una dieta saludable. Los azúcares añadidos en productos como refrescos, dulces y salsas comerciales suelen ser los principales culpables.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Imagina que soy nuevo en nutrición. ¿Cómo me explicarías lo del azúcar?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Piénsalo como gasolina para tu cuerpo: el azúcar te da energía, pero demasiada puede hacerte daño. Hay azúcar natural, como en las frutas, que está bien en moderación. Pero el problema es el **azúcar añadido**, que encuentras en refrescos, dulces o salsas. Comer mucho de esto puede subir tu peso y aumentar el riesgo de diabetes tipo 2, porque tu cuerpo lucha para manejar tanto azúcar. Para comer mejor, cambia refrescos por agua, elige snacks como nueces y lee etiquetas para evitar productos con azúcares ocultos.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Azúcar y salud\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 9 — Incoherente (fragmento sin contexto, patrón: resumen e implicaciones, fragmento corto)**:\n",
    "<fragment>\n",
    "...error crítico en el núcleo...\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Podrías darme un resumen sobre un error crítico en el núcleo y sus implicaciones?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"# Error Crítico en el Núcleo\\n\\n**Resumen**: Un error crítico en el núcleo es un fallo grave en el sistema operativo, que actúa como el cerebro de tu computadora. Puede ocurrir por hardware defectuoso, software corrupto o malware.\\n\\n**Implicaciones**:\\n- **Pérdida de datos**: Los archivos pueden perderse si el sistema no guarda correctamente.\\n- **Interrupciones**: El sistema puede congelarse o reiniciarse.\\n- **Costos**: Reparaciones o recuperación de datos pueden ser necesarias.\\n\\n**Acción**: Usa software de recuperación como TestDisk y haz backups regulares para minimizar riesgos.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Error crítico en sistema\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 10 — Contextual (economía, patrón: preocupación, fragmento largo)**:\n",
    "<fragment>\n",
    "La inflación persistente puede erosionar el poder adquisitivo, haciendo que los bienes y servicios sean más caros. Los bancos centrales intentan controlarla con políticas monetarias, pero esto puede tener efectos secundarios en el empleo y el crecimiento económico.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Estoy preocupado por la inflación. ¿Qué debería saber al respecto?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"# Entendiendo la Inflación\\n\\nLa inflación ocurre cuando los precios de bienes y servicios suben, reduciendo tu **poder adquisitivo** (es decir, con el mismo dinero compras menos). Aquí está lo que debes saber:\\n\\n- **Causas**: Puede ser por alta demanda, costos de producción elevados o políticas monetarias expansivas.\\n- **Impacto**: Tus ahorros valen menos, y cosas como alimentos o gasolina se encarecen.\\n- **Control**: Los bancos centrales suben tasas de interés para reducir el gasto, pero esto puede ralentizar la economía, afectando empleos.\\n\\n**Qué hacer**:\\n- **Ahorra inteligentemente**: Invierte en activos que superen la inflación, como bonos ajustados.\\n- **Presupuesto**: Prioriza gastos esenciales y reduce los no necesarios.\\n- **Infórmate**: Sigue las políticas del banco central para anticipar cambios.\\n\\nHabla con un asesor financiero si quieres estrategias específicas.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Inflación y poder adquisitivo\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 11 — Técnico (energía renovable, patrón: explicación para niño, fragmento corto)**:\n",
    "<fragment>\n",
    "La energía solar usa paneles para convertir la luz del sol en electricidad.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Explícame la energía solar como si fuera un niño de 10 años.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Imagina que el sol es como una lámpara gigante que brilla todo el día. La **energía solar** usa placas especiales, como espejos mágicos, que atrapan la luz del sol y la convierten en electricidad para encender luces, televisions o cargar tu tablet. ¡Es como usar el poder del sol para hacer cosas cool!\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Energía solar simplificada\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 12 — Casual (trabajo remoto, patrón: correo informal, fragmento largo)**:\n",
    "<fragment>\n",
    "El trabajo remoto puede aumentar la productividad, pero también presenta desafíos como la falta de interacción social y dificultades para desconectarse del trabajo. Herramientas como Zoom y Slack facilitan la colaboración, pero requieren una buena gestión del tiempo.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Redacta un correo informal sobre trabajo remoto para un colega.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"**Asunto**: Tips para el Trabajo Remoto\\n\\nHola [Nombre],\\n\\nEstuve pensando en cómo sacarle provecho al trabajo desde casa. Puede ser súper productivo porque evitas el tráfico, pero a veces se siente raro sin las charlas en la oficina, ¿no? Herramientas como Zoom o Slack ayudan a mantenernos conectados, pero la clave está en organizarse bien. Intenta poner horarios fijos para trabajar y desconectarte, así no terminas respondiendo correos a medianoche. ¿Tú cómo lo llevas? ¡Cuéntame si tienes algún truco!\\n\\nSaludos,  \\n[Tu Nombre]\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Trabajo remoto y productividad\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "</ejemplos>\n",
    "\"\"\"\n",
    "  system_examples = EXAMPLES % BASE_EXAMPLE\n",
    "  system_content = f\"{INSTRUCTIONS}\\n{system_examples}\"\n",
    "  return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_content\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": fragment\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e408c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_genterated_conv(conversation:Conversation):\n",
    "  # Validar que la conversación tenga al menos un intercambio completo\n",
    "  if len(conversation.messages) < 2:\n",
    "      print(f\"Error: Conversación inválida, número de mensajes insuficiente: {len(conversation.messages)}\")\n",
    "      return None\n",
    "  # Si el número de mensajes es impar, eliminar el último (asumiendo que es del usuario)\n",
    "  if len(conversation.messages) % 2 != 0 and len(conversation.messages) > 2:\n",
    "      conversation.messages = conversation.messages[:-1]\n",
    "      \n",
    "  return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e353ea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import AsyncClient\n",
    "\n",
    "ollama_client = AsyncClient()\n",
    "\n",
    "async def generate_ollama_conversations(messages):\n",
    "    try:\n",
    "        # Llama a Ollama con roles system y user\n",
    "        response = await ollama_client.chat(\n",
    "            model=OLLAMA_MODEL,\n",
    "            messages=messages,\n",
    "            options={\n",
    "                \"temperature\": TEMPERATURE,\n",
    "                \"top_p\": TOP_P,\n",
    "                \n",
    "                \n",
    "            },\n",
    "            format=Conversation.model_json_schema()  # Especifica el esquema JSON\n",
    "            \n",
    "        )\n",
    "        conversation = Conversation.model_validate_json(response.message.content)\n",
    "\n",
    "        return validate_genterated_conv(conversation)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generando conversación con Ollama: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16928054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncClient, RateLimitError\n",
    "\n",
    "openai_client = AsyncClient(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "async def generate_openai_conversations(messages, model=\"gpt-4o-mini\"):\n",
    "    try:\n",
    "        response = await openai_client.responses.parse(\n",
    "            model=model,\n",
    "            input=messages,\n",
    "            text_format=Conversation\n",
    "        )\n",
    "        conversation = response.output_parsed\n",
    "        return validate_genterated_conv(conversation) if conversation else None\n",
    "    except RateLimitError as e:\n",
    "        print(f\"Rate limit hit for {model}: {e}\")\n",
    "        next_model = \"gpt-5-nano\"\n",
    "        if next_model != model:\n",
    "            return await generate_openai_conversations(messages, next_model)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating conversation with OpenAI {model}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab2c9167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "async def generate_conversation(fragment:str, provider: Literal[\"ollama\", \"openai\"]):\n",
    "    \"\"\"\n",
    "    Genera una conversación estructurada en formato JSON usando Ollama, basada en un fragmento de texto.\n",
    "    Optimizado para múltiples iteraciones en la generación de datasets para fine-tuning.\n",
    "    \"\"\"\n",
    "    if not fragment or len(fragment) < MIN_FRAGMENT_LENGTH:\n",
    "        print(f\"Error: Fragmento demasiado corto ({len(fragment)} caracteres).\")\n",
    "        return None\n",
    "\n",
    "    # Extraer el texto sin la metadata (líneas que comienzan con '#')\n",
    "    fragment_content = '\\n'.join(line for line in fragment.splitlines() if not line.startswith('#')).strip()\n",
    "    if len(fragment_content) < MIN_FRAGMENT_LENGTH:\n",
    "        print(f\"Error: Contenido útil del fragmento demasiado corto ({len(fragment_content)} caracteres).\")\n",
    "        return None\n",
    "\n",
    "    # Prompt optimizado para múltiples iteraciones\n",
    "    messages = get_prompts(fragment_content)\n",
    "\n",
    "    try:\n",
    "        if provider=='ollama':\n",
    "            return await generate_ollama_conversations(messages)\n",
    "        elif provider=='openai':\n",
    "            return await generate_openai_conversations(messages)\n",
    "        else: return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error generando conversación con Ollama: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20c7088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configuración del logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('pdf_processing.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43e9efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from filelock import FileLock\n",
    "import asyncio\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "OUTPUT_FOLDER = \"data\"\n",
    "\n",
    "class MetadataManager:\n",
    "    \"\"\"Clase para manejar archivos de metadatos JSON.\"\"\"\n",
    "    def __init__(self, index: int, output_dir: str):\n",
    "        self.folder = os.path.join(output_dir, \"metadata\")\n",
    "        self.metadata_file = os.path.join(self.folder, f\"metadata_{index:04d}.json\")\n",
    "        self.lock_file = f\"{self.metadata_file}.lock\"\n",
    "        self.index = index\n",
    "        os.makedirs(self.folder, exist_ok=True)\n",
    "\n",
    "    def exists(self) -> bool:\n",
    "        \"\"\"Verifica si el archivo de metadatos existe.\"\"\"\n",
    "        return os.path.exists(self.metadata_file)\n",
    "\n",
    "    def get(self, param: str):\n",
    "        \"\"\"Obtiene chunks, conversations, fileName, num_chunks, num_messages o num_exchanges desde el archivo de metadatos.\"\"\"\n",
    "        if not self.exists():\n",
    "            return [] if param in [\"chunks\", \"conversations\"] else 0 if param in [\"num_chunks\", \"num_messages\", \"num_exchanges\"] else \"\"\n",
    "        try:\n",
    "            with FileLock(self.lock_file):\n",
    "                with open(self.metadata_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    metadata = json.load(f)\n",
    "                \n",
    "                updated = False\n",
    "                if param == \"conversations\":\n",
    "                    conversations = metadata.get(\"conversations\", [])\n",
    "                    for i, conv in enumerate(conversations):\n",
    "                        if isinstance(conv, dict) and \"chunk_index\" not in conv:\n",
    "                            conv[\"chunk_index\"] = i\n",
    "                            updated = True\n",
    "                    conversations.sort(key=lambda x: x.get(\"chunk_index\", 0))\n",
    "                    metadata[\"conversations\"] = conversations\n",
    "                \n",
    "                if \"num_chunks\" not in metadata and \"chunks\" in metadata:\n",
    "                    metadata[\"num_chunks\"] = len(metadata.get(\"chunks\", []))\n",
    "                    updated = True\n",
    "                \n",
    "                if \"num_exchanges\" not in metadata and \"conversations\" in metadata:\n",
    "                    metadata[\"num_exchanges\"] = sum(len(conv[\"messages\"]) // 2 for conv in metadata.get(\"conversations\", []))\n",
    "                    updated = True\n",
    "                \n",
    "                if \"num_messages\" not in metadata and \"conversations\" in metadata:\n",
    "                    metadata[\"num_messages\"] = sum(len(conv[\"messages\"]) for conv in metadata.get(\"conversations\", []))\n",
    "                    updated = True\n",
    "                    \n",
    "                if \"total_conversations\" not in metadata and \"conversations\" in metadata:\n",
    "                    metadata[\"total_conversations\"] = len(conversations)\n",
    "                    updated = True\n",
    "                \n",
    "                if updated:\n",
    "                    with open(self.metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                        json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "                \n",
    "                return metadata.get(param, []) if param in [\"chunks\", \"conversations\"] else metadata.get(param, 0 if param in [\"num_chunks\", \"num_messages\", \"num_exchanges\"] else metadata.get(param,\"\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Error al leer {param} desde {self.metadata_file}: {e}\")\n",
    "            return [] if param in [\"chunks\", \"conversations\"] else 0 if param in [\"num_chunks\", \"num_messages\", \"num_exchanges\"] else \"\"\n",
    "\n",
    "    def set(self, param: str, value):\n",
    "        \"\"\"Establece chunks, conversations, fileName, num_chunks, num_messages o num_exchanges en el archivo de metadatos.\"\"\"\n",
    "        try:\n",
    "            with FileLock(self.lock_file):\n",
    "                metadata = {\"chunks\": [], \"fileName\": \"\", \"conversations\": [], \"num_chunks\": 0, \"num_messages\": 0, \"num_exchanges\": 0}\n",
    "                if self.exists():\n",
    "                    with open(self.metadata_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                        metadata = json.load(f)\n",
    "                \n",
    "                metadata[param] = value\n",
    "                if param == \"conversations\":\n",
    "                    metadata[\"conversations\"].sort(key=lambda x: x.get(\"chunk_index\", 0))\n",
    "                    metadata[\"num_messages\"] = sum(len(conv[\"messages\"]) for conv in metadata[\"conversations\"])\n",
    "                    metadata[\"num_exchanges\"] = sum(len(conv[\"messages\"]) // 2 for conv in metadata[\"conversations\"])\n",
    "                    metadata[\"total_conversations\"] = len(metadata[\"conversations\"])\n",
    "                if param == \"chunks\":\n",
    "                    metadata[\"num_chunks\"] = len(value)\n",
    "                with open(self.metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al escribir {param} en {self.metadata_file}: {e}\")\n",
    "\n",
    "    def append_conversation(self, conversation: dict):\n",
    "        \"\"\"Añade una conversación al array de conversaciones en el archivo de metadatos.\"\"\"\n",
    "        try:\n",
    "            with FileLock(self.lock_file):\n",
    "                metadata = {\"chunks\": [], \"fileName\": \"\", \"conversations\": [], \"num_chunks\": 0, \"num_messages\": 0, \"num_exchanges\": 0}\n",
    "                if self.exists():\n",
    "                    with open(self.metadata_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                        metadata = json.load(f)\n",
    "                \n",
    "                for i, conv in enumerate(metadata[\"conversations\"]):\n",
    "                    if isinstance(conv, dict) and \"chunk_index\" not in conv:\n",
    "                        conv[\"chunk_index\"] = i\n",
    "                \n",
    "                if \"num_chunks\" not in metadata:\n",
    "                    metadata[\"num_chunks\"] = len(metadata.get(\"chunks\", []))\n",
    "                \n",
    "                if \"num_exchanges\" not in metadata:\n",
    "                    metadata[\"num_exchanges\"] = sum(len(conv[\"messages\"]) // 2 for conv in metadata.get(\"conversations\", []))\n",
    "                \n",
    "                if \"num_messages\" not in metadata:\n",
    "                    metadata[\"num_messages\"] = sum(len(conv[\"messages\"]) for conv in metadata.get(\"conversations\", []))\n",
    "                \n",
    "                metadata[\"conversations\"].append(conversation)\n",
    "                metadata[\"conversations\"].sort(key=lambda x: x.get(\"chunk_index\", 0))\n",
    "                metadata[\"num_messages\"] = sum(len(conv[\"messages\"]) for conv in metadata[\"conversations\"])\n",
    "                metadata[\"num_exchanges\"] = sum(len(conv[\"messages\"]) // 2 for conv in metadata[\"conversations\"])\n",
    "                metadata[\"total_conversations\"] = len(metadata[\"conversations\"])\n",
    "                \n",
    "                with open(self.metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al añadir conversación a {self.metadata_file}: {e}\")\n",
    "\n",
    "def get_text_from_pdf(index: int, pdf_path: str, output_dir: str) -> list:\n",
    "    \"\"\"Obtiene el texto de un PDF desde la caché (JSON) o lo extrae si no existe.\"\"\"\n",
    "    metadata_manager = MetadataManager(index, output_dir)\n",
    "    \n",
    "    if metadata_manager.exists():\n",
    "        chunks = metadata_manager.get(\"chunks\")\n",
    "        file_name = metadata_manager.get(\"fileName\")\n",
    "        if chunks and file_name == os.path.basename(pdf_path):\n",
    "            return chunks\n",
    "    \n",
    "    try:\n",
    "        pages_text = extract_text_from_pdf(pdf_path)\n",
    "        if pages_text:\n",
    "            metadata_manager.set(\"chunks\", pages_text)\n",
    "            metadata_manager.set(\"fileName\", os.path.basename(pdf_path))\n",
    "            metadata_manager.set(\"conversations\", [])\n",
    "        return pages_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error al extraer texto del PDF {pdf_path}: {e}\")\n",
    "        return []\n",
    "    \n",
    "def get_conv_from_jsonl(index, output_dir):\n",
    "    metadata_manager = MetadataManager(index, output_dir)\n",
    "    jsonl_file = os.path.join(output_dir, OUTPUT_FOLDER, f\"pdf_{index:04d}.jsonl\")\n",
    "    if os.path.exists(jsonl_file):\n",
    "        try:\n",
    "            chunks = metadata_manager.get(\"chunks\")\n",
    "            with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                conversations = []\n",
    "                for i, line in enumerate(f):\n",
    "                    if line.strip():\n",
    "                        conv = json.loads(line)\n",
    "                        conv[\"source_chunk\"] = chunks[i]\n",
    "                        conv[\"chunk_index\"] = i\n",
    "                        conversations.append(conv)\n",
    "                conversations.sort(key=lambda x: x.get(\"chunk_index\", 0))\n",
    "                metadata_manager.set(\"conversations\", conversations)\n",
    "                metadata_manager.set(\"num_messages\", sum(len(conv[\"messages\"]) for conv in conversations))\n",
    "                metadata_manager.set(\"num_exchanges\", sum(len(conv[\"messages\"]) // 2 for conv in conversations))\n",
    "                metadata_manager.set(\"total_conversations\", len(conversations))\n",
    "                return conversations\n",
    "        except Exception as e:\n",
    "            print(f\"Error al leer conversaciones desde {jsonl_file}: {e}\")\n",
    "    return []\n",
    "\n",
    "async def get_conversation_from_chunk(index: int, output_dir: str, chunk: str, chunk_index: int, provider: Literal[\"ollama\", \"openai\"]):\n",
    "    \"\"\"Obtiene o genera una conversación para un fragmento de texto.\"\"\"\n",
    "    metadata_manager = MetadataManager(index, output_dir)\n",
    "    existing_conversations = metadata_manager.get(\"conversations\")\n",
    "    \n",
    "    if not existing_conversations:\n",
    "        existing_conversations = get_conv_from_jsonl(index, output_dir)\n",
    "    \n",
    "    for conv in existing_conversations:\n",
    "        if isinstance(conv, dict) and conv.get(\"source_chunk\") == chunk and conv.get(\"chunk_index\") == chunk_index:\n",
    "            return conv\n",
    "    try:\n",
    "        conversation = await generate_conversation(chunk, provider)\n",
    "        if conversation:\n",
    "            conv_dict = conversation.model_dump()\n",
    "            conv_dict[\"source_chunk\"] = chunk\n",
    "            conv_dict[\"chunk_index\"] = chunk_index\n",
    "            conv_dict[\"provider\"]= provider\n",
    "            from datetime import datetime\n",
    "            conv_dict[\"created_at\"] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            metadata_manager.append_conversation(conv_dict)\n",
    "            return conv_dict\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error al generar conversación para chunk_index {chunk_index} en PDF #{index} con {provider}: {e}\")\n",
    "        return None\n",
    "\n",
    "async def get_conv(index: int, output_dir: str, chunk: str, chunk_index: int, provider: Literal[\"ollama\", \"openai\"]):\n",
    "    result = await get_conversation_from_chunk(index, output_dir, chunk, chunk_index, provider)\n",
    "    return result if result is not None else []\n",
    "\n",
    "async def process_pdf(index: int, pdf_path: str, output_dir: str):\n",
    "    \"\"\"Procesa un PDF, genera conversaciones y las guarda en JSONL.\"\"\"\n",
    "    logger = logging.getLogger()\n",
    "    logger.info(f\"[PDF {index}] Procesando PDF: {Path(pdf_path).name}\")\n",
    "    try:\n",
    "        pages_text = get_text_from_pdf(index, pdf_path, output_dir)\n",
    "        \n",
    "        if not pages_text:\n",
    "            logger.warning(f\"[PDF {index}] No se encontraron fragmentos de texto\")\n",
    "            return\n",
    "        \n",
    "        data_dir = os.path.join(output_dir, OUTPUT_FOLDER)\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "        jsonl_file = os.path.join(data_dir, f\"pdf_{index:04d}.jsonl\")\n",
    "        \n",
    "        tasks = [(i, fragment) for i, fragment in enumerate(pages_text) if len(fragment) > 20]\n",
    "        if not tasks:\n",
    "            logger.warning(f\"[PDF {index}] No hay fragmentos válidos para procesar\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"[PDF {index}] Procesando {len(tasks)} fragmentos\")\n",
    "        \n",
    "        # Dividir tareas entre providers (50% openai, 50% ollama)\n",
    "        total_tasks = len(tasks)\n",
    "        half_tasks = total_tasks // 2\n",
    "        openai_tasks = [get_conv(index, output_dir, fragment, i, \"openai\") for i, fragment in tasks[:half_tasks]]\n",
    "        ollama_tasks = [get_conv(index, output_dir, fragment, i, \"ollama\") for i, fragment in tasks[half_tasks:]]\n",
    "        \n",
    "        # Ejecutar tareas en paralelo con límite de concurrencia\n",
    "        from asyncio import Semaphore\n",
    "        async def limited_gather(tasks, limit=10):\n",
    "            semaphore = Semaphore(limit)\n",
    "            async def sem_task(task):\n",
    "                async with semaphore:\n",
    "                    return await task\n",
    "            return await asyncio.gather(*[sem_task(task) for task in tasks], return_exceptions=True)\n",
    "        \n",
    "        task_results = await limited_gather(openai_tasks + ollama_tasks)\n",
    "        \n",
    "        if task_results:\n",
    "            valid_conversations = [conv for conv in task_results if conv is not None and conv is not TypeError]\n",
    "            if valid_conversations:\n",
    "                valid_conversations.sort(key=lambda x: x[\"chunk_index\"])\n",
    "                \n",
    "                actual_indices = [conv[\"chunk_index\"] for conv in valid_conversations]\n",
    "                if actual_indices != sorted(actual_indices):\n",
    "                    logger.warning(f\"[PDF {index}] El orden de las conversaciones no coincide con el esperado\")\n",
    "                \n",
    "                with open(jsonl_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    output_str = \"\\n\".join(json.dumps({\"messages\": conv[\"messages\"], \"topic\": conv[\"topic\"]}, ensure_ascii=False) \n",
    "                                        for conv in valid_conversations)\n",
    "                    f.write(output_str + \"\\n\")\n",
    "                    logger.info(f\"[PDF {index}] Dataset conversacional generado y guardado en JSONL\")\n",
    "            else:\n",
    "                logger.warning(f\"[PDF {index}] No se generaron conversaciones\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"[PDF {index}] Error al procesar el PDF: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fdbb014-2ca2-4f7f-bdfe-1430423e7d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import asyncio\n",
    "import os\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "\n",
    "def process_pdf_wrapper(args):\n",
    "    index, pdf_path, output_dir = args\n",
    "    logger = logging.getLogger()\n",
    "    logger.info(f\"[PDF {index}] Inicio del procesamiento del PDF: {Path(pdf_path).name}\")\n",
    "    try:\n",
    "        asyncio.run(process_pdf(index, pdf_path, output_dir))\n",
    "        logger.info(f\"[PDF {index}] Procesamiento del PDF completado\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"[PDF {index}] Error durante el procesamiento: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def generate(pdf_files,max_workers=12):\n",
    "    output_dir = \"outputs_2\"\n",
    "    output_folder_path = Path(output_dir)\n",
    "    os.makedirs(output_folder_path, exist_ok=True)\n",
    "    \n",
    "    logger = logging.getLogger()\n",
    "    logger.info(f\"Generando Datasets - max_workers={max_workers}, # Files: {len(pdf_files)}\")\n",
    "    \n",
    "    # Dividir archivos en lotes para optimizar el uso de memoria\n",
    "    batch_size = max(1, math.ceil(len(pdf_files) / max_workers))\n",
    "    batches = [pdf_files[i:i + batch_size] for i in range(0, len(pdf_files), batch_size)]\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for batch_idx, batch in enumerate(batches):\n",
    "            logger.info(f\"Procesando lote {batch_idx + 1}/{len(batches)} con {len(batch)} archivos\")\n",
    "            tasks = [(i + batch_idx * batch_size, p, output_dir) for i, p in enumerate(batch)]\n",
    "            for _ in tqdm(\n",
    "                executor.map(process_pdf_wrapper, tasks),\n",
    "                total=len(batch),\n",
    "                desc=f\"Batch {batch_idx + 1}/{len(batches)}\"\n",
    "            ):\n",
    "                pass\n",
    "            logger.info(f\"Lote {batch_idx + 1}/{len(batches)} completado\")\n",
    "    logger.info(\"Generación de datasets finalizada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a3dadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:55:54,873 - INFO - Generando Datasets - max_workers=15, # Files: 89\n",
      "2025-08-12 16:55:54,879 - INFO - Procesando lote 1/15 con 6 archivos\n",
      "Batch 1/15:   0%|          | 0/6 [00:00<?, ?it/s]2025-08-12 16:55:55,009 - INFO - [PDF 0] Inicio del procesamiento del PDF: L1_XVII_cap_IV.pdf\n",
      "2025-08-12 16:55:55,010 - INFO - [PDF 1] Inicio del procesamiento del PDF: L1_IX_cap_III.pdf\n",
      "2025-08-12 16:55:55,010 - INFO - [PDF 2] Inicio del procesamiento del PDF: manual-para-la-gestion-de-riesgos-lavados-de-activos-y-financiacion-del-terrorismo (1).pdf\n",
      "2025-08-12 16:55:55,011 - INFO - [PDF 4] Inicio del procesamiento del PDF: Modelo_de_Administracion_del_Riesgo_de_LAFT_y_Contrabando_web.pdf\n",
      "2025-08-12 16:55:55,011 - INFO - [PDF 5] Inicio del procesamiento del PDF: manual-para-la-gestion-de-riesgos-lavados-de-activos-y-financiacion-del-terrorismo.pdf\n",
      "2025-08-12 16:55:55,010 - INFO - [PDF 3] Inicio del procesamiento del PDF: L1_XVII_cap_III.pdf\n",
      "2025-08-12 16:55:55,017 - INFO - [PDF 1] Procesando PDF: L1_IX_cap_III.pdf\n",
      "2025-08-12 16:55:55,018 - INFO - [PDF 0] Procesando PDF: L1_XVII_cap_IV.pdf\n",
      "2025-08-12 16:55:55,019 - INFO - [PDF 2] Procesando PDF: manual-para-la-gestion-de-riesgos-lavados-de-activos-y-financiacion-del-terrorismo (1).pdf\n",
      "2025-08-12 16:55:55,020 - INFO - [PDF 4] Procesando PDF: Modelo_de_Administracion_del_Riesgo_de_LAFT_y_Contrabando_web.pdf\n",
      "2025-08-12 16:55:55,021 - INFO - [PDF 3] Procesando PDF: L1_XVII_cap_III.pdf\n",
      "2025-08-12 16:55:55,021 - INFO - [PDF 5] Procesando PDF: manual-para-la-gestion-de-riesgos-lavados-de-activos-y-financiacion-del-terrorismo.pdf\n",
      "2025-08-12 16:55:55,046 - INFO - [PDF 1] Procesando 104 fragmentos\n",
      "2025-08-12 16:55:55,052 - INFO - [PDF 2] Procesando 53 fragmentos\n",
      "2025-08-12 16:55:55,053 - INFO - [PDF 5] Procesando 53 fragmentos\n",
      "2025-08-12 16:55:55,061 - INFO - [PDF 0] Procesando 145 fragmentos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:55:55,072 - INFO - [PDF 3] Procesando 90 fragmentos\n",
      "2025-08-12 16:55:55,157 - INFO - [PDF 4] Procesando 592 fragmentos\n",
      "2025-08-12 16:56:03,586 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:03,590 - INFO - Retrying request to /responses in 0.485141 seconds\n",
      "2025-08-12 16:56:03,701 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:03,711 - INFO - Retrying request to /responses in 0.486123 seconds\n",
      "2025-08-12 16:56:04,938 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:04,943 - INFO - Retrying request to /responses in 0.452966 seconds\n",
      "2025-08-12 16:56:09,076 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:09,082 - INFO - Retrying request to /responses in 0.859895 seconds\n",
      "2025-08-12 16:56:09,305 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:09,315 - INFO - Retrying request to /responses in 0.841404 seconds\n",
      "2025-08-12 16:56:10,168 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:10,187 - INFO - Retrying request to /responses in 0.467575 seconds\n",
      "2025-08-12 16:56:10,401 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:10,412 - INFO - Retrying request to /responses in 0.429871 seconds\n",
      "2025-08-12 16:56:10,530 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:10,535 - INFO - Retrying request to /responses in 0.485410 seconds\n",
      "2025-08-12 16:56:10,584 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:10,588 - INFO - Retrying request to /responses in 0.471802 seconds\n",
      "2025-08-12 16:56:10,662 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:10,667 - INFO - Retrying request to /responses in 0.423941 seconds\n",
      "2025-08-12 16:56:10,932 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:10,937 - INFO - Retrying request to /responses in 0.467769 seconds\n",
      "2025-08-12 16:56:11,037 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:11,042 - INFO - Retrying request to /responses in 0.438879 seconds\n",
      "2025-08-12 16:56:11,120 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:11,125 - INFO - Retrying request to /responses in 0.402066 seconds\n",
      "2025-08-12 16:56:11,356 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:11,361 - INFO - Retrying request to /responses in 0.898775 seconds\n",
      "2025-08-12 16:56:11,881 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:11,887 - INFO - Retrying request to /responses in 0.428070 seconds\n",
      "2025-08-12 16:56:12,228 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:12,233 - INFO - Retrying request to /responses in 0.489247 seconds\n",
      "2025-08-12 16:56:12,893 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:14,628 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 199041, Requested 5723. Please try again in 1.429s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:56:14,964 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 198648, Requested 5741. Please try again in 1.316s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:56:15,569 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:15,575 - INFO - Retrying request to /responses in 0.921792 seconds\n",
      "2025-08-12 16:56:16,008 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:16,184 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:16,190 - INFO - Retrying request to /responses in 0.991614 seconds\n",
      "2025-08-12 16:56:16,234 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:16,240 - INFO - Retrying request to /responses in 0.963053 seconds\n",
      "2025-08-12 16:56:16,313 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:16,326 - INFO - Retrying request to /responses in 0.847851 seconds\n",
      "2025-08-12 16:56:16,722 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:16,728 - INFO - Retrying request to /responses in 0.976824 seconds\n",
      "2025-08-12 16:56:16,975 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:16,980 - INFO - Retrying request to /responses in 0.933788 seconds\n",
      "2025-08-12 16:56:17,004 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:17,011 - INFO - Retrying request to /responses in 0.997547 seconds\n",
      "2025-08-12 16:56:17,113 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:17,222 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:17,639 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:17,644 - INFO - Retrying request to /responses in 0.998049 seconds\n",
      "2025-08-12 16:56:17,670 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:18,065 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:18,104 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:18,631 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:18,748 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:18,753 - INFO - Retrying request to /responses in 0.379771 seconds\n",
      "2025-08-12 16:56:19,353 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:20,189 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:21,696 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:21,999 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 195616, Requested 5795. Please try again in 423ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:56:22,280 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 194542, Requested 5703. Please try again in 73ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:56:23,016 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:23,022 - INFO - Retrying request to /responses in 0.452677 seconds\n",
      "2025-08-12 16:56:23,126 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 197449, Requested 5731. Please try again in 954ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:56:23,153 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 197379, Requested 5722. Please try again in 930ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:56:23,297 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 196826, Requested 5723. Please try again in 764ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:56:23,533 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:23,540 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:23,593 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:23,599 - INFO - Retrying request to /responses in 0.395657 seconds\n",
      "2025-08-12 16:56:23,616 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 197079, Requested 5758. Please try again in 851ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:56:23,958 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 198286, Requested 5746. Please try again in 1.209s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:56:24,030 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:24,037 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:24,186 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:24,209 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:24,214 - INFO - Retrying request to /responses in 0.458854 seconds\n",
      "2025-08-12 16:56:24,521 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:24,529 - INFO - Retrying request to /responses in 0.451153 seconds\n",
      "2025-08-12 16:56:24,676 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:24,733 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:24,781 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:25,025 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:25,549 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:25,553 - INFO - Retrying request to /responses in 0.953666 seconds\n",
      "2025-08-12 16:56:26,296 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:26,301 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:26,976 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:27,516 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:27,665 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:27,728 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:27,754 - INFO - Retrying request to /responses in 0.439166 seconds\n",
      "2025-08-12 16:56:28,139 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:28,611 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:28,643 - INFO - Retrying request to /responses in 0.783247 seconds\n",
      "2025-08-12 16:56:28,655 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:28,812 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:28,818 - INFO - Retrying request to /responses in 0.910759 seconds\n",
      "2025-08-12 16:56:29,343 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:29,363 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:29,368 - INFO - Retrying request to /responses in 0.416373 seconds\n",
      "2025-08-12 16:56:29,444 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:29,448 - INFO - Retrying request to /responses in 0.874865 seconds\n",
      "2025-08-12 16:56:29,658 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:29,664 - INFO - Retrying request to /responses in 0.443816 seconds\n",
      "2025-08-12 16:56:29,754 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:29,758 - INFO - Retrying request to /responses in 0.956541 seconds\n",
      "2025-08-12 16:56:30,322 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:30,786 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:30,913 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:31,305 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:32,032 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:32,225 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 196129, Requested 5725. Please try again in 556ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:56:32,386 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:32,711 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:33,215 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:33,220 - INFO - Retrying request to /responses in 0.813419 seconds\n",
      "2025-08-12 16:56:34,588 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5747. Please try again in 1.724s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:56:34,597 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:34,704 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:56:34,714 - INFO - Retrying request to /responses in 0.801961 seconds\n",
      "2025-08-12 16:56:35,332 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:35,402 - INFO - [PDF 2] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-12 16:56:35,420 - INFO - [PDF 2] Procesamiento del PDF completado\n",
      "2025-08-12 16:56:35,615 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:35,616 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5786. Please try again in 1.735s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:56:35,782 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5724. Please try again in 1.717s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:56:36,316 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:37,127 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:38,649 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:38,655 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:38,761 - INFO - [PDF 3] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-12 16:56:38,778 - INFO - [PDF 3] Procesamiento del PDF completado\n",
      "2025-08-12 16:56:39,747 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 194946, Requested 5813. Please try again in 227ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:56:39,861 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:40,673 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:43,250 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:43,910 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:44,698 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:44,759 - INFO - [PDF 5] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-12 16:56:44,771 - INFO - [PDF 5] Procesamiento del PDF completado\n",
      "2025-08-12 16:56:47,269 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:52,959 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:53,875 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:55,749 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:57,107 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:57,340 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:57,532 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:58,037 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:58,043 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:56:59,012 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:01,988 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:02,874 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:03,186 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:05,408 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:07,447 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:07,534 - INFO - [PDF 0] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-12 16:57:07,547 - INFO - [PDF 0] Procesamiento del PDF completado\n",
      "Batch 1/15:  17%|█▋        | 1/6 [01:12<06:02, 72.54s/it]2025-08-12 16:57:10,618 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:11,266 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:11,681 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:12,241 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:12,457 - INFO - [PDF 1] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-12 16:57:12,475 - INFO - [PDF 1] Procesamiento del PDF completado\n",
      "Batch 1/15:  33%|███▎      | 2/6 [01:17<02:11, 32.77s/it]2025-08-12 16:57:21,235 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:28,294 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:31,888 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:32,240 - INFO - [PDF 4] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-12 16:57:32,263 - INFO - [PDF 4] Procesamiento del PDF completado\n",
      "Batch 1/15: 100%|██████████| 6/6 [01:37<00:00, 16.21s/it]\n",
      "2025-08-12 16:57:32,272 - INFO - Lote 1/15 completado\n",
      "2025-08-12 16:57:32,274 - INFO - Procesando lote 2/15 con 6 archivos\n",
      "Batch 2/15:   0%|          | 0/6 [00:00<?, ?it/s]2025-08-12 16:57:32,282 - INFO - [PDF 6] Inicio del procesamiento del PDF: L1_VIII_cap_I.pdf\n",
      "2025-08-12 16:57:32,283 - INFO - [PDF 8] Inicio del procesamiento del PDF: Tipologias_de_lavado_de_activos_2020_al_12_Noviembre.pdf\n",
      "2025-08-12 16:57:32,284 - INFO - [PDF 9] Inicio del procesamiento del PDF: Guía-para-la-elaboración-del-Manual-de-Prevención-de-Lavado-de-Activos-y-del-Financiamiento-de-Delitos-ESFPS.pdf\n",
      "2025-08-12 16:57:32,283 - INFO - [PDF 7] Inicio del procesamiento del PDF: RML BANCO CENTRAL_2.pdf\n",
      "2025-08-12 16:57:32,284 - INFO - [PDF 10] Inicio del procesamiento del PDF: manual_riesgos_mercado_liquidez_9_jul_10.pdf\n",
      "2025-08-12 16:57:32,284 - INFO - [PDF 11] Inicio del procesamiento del PDF: manual_estructuras_datos_generalidades_1_abr_14.pdf\n",
      "2025-08-12 16:57:32,292 - INFO - [PDF 6] Procesando PDF: L1_VIII_cap_I.pdf\n",
      "2025-08-12 16:57:32,292 - INFO - [PDF 9] Procesando PDF: Guía-para-la-elaboración-del-Manual-de-Prevención-de-Lavado-de-Activos-y-del-Financiamiento-de-Delitos-ESFPS.pdf\n",
      "2025-08-12 16:57:32,292 - INFO - [PDF 8] Procesando PDF: Tipologias_de_lavado_de_activos_2020_al_12_Noviembre.pdf\n",
      "2025-08-12 16:57:32,293 - INFO - [PDF 7] Procesando PDF: RML BANCO CENTRAL_2.pdf\n",
      "2025-08-12 16:57:32,293 - INFO - [PDF 10] Procesando PDF: manual_riesgos_mercado_liquidez_9_jul_10.pdf\n",
      "2025-08-12 16:57:32,296 - INFO - [PDF 11] Procesando PDF: manual_estructuras_datos_generalidades_1_abr_14.pdf\n",
      "2025-08-12 16:57:32,329 - INFO - [PDF 7] Procesando 18 fragmentos\n",
      "2025-08-12 16:57:32,330 - INFO - [PDF 11] Procesando 20 fragmentos\n",
      "2025-08-12 16:57:32,332 - INFO - [PDF 6] Procesando 9 fragmentos\n",
      "2025-08-12 16:57:32,332 - INFO - [PDF 10] Procesando 56 fragmentos\n",
      "2025-08-12 16:57:32,335 - INFO - [PDF 9] Procesando 125 fragmentos\n",
      "2025-08-12 16:57:32,353 - INFO - [PDF 8] Procesando 108 fragmentos\n",
      "2025-08-12 16:57:32,397 - INFO - [PDF 6] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-12 16:57:32,408 - INFO - [PDF 6] Procesamiento del PDF completado\n",
      "Batch 2/15:  17%|█▋        | 1/6 [00:00<00:00,  7.68it/s]2025-08-12 16:57:32,454 - INFO - [PDF 7] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-12 16:57:32,463 - INFO - [PDF 7] Procesamiento del PDF completado\n",
      "2025-08-12 16:57:32,525 - INFO - [PDF 11] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-12 16:57:32,538 - INFO - [PDF 11] Procesamiento del PDF completado\n",
      "2025-08-12 16:57:33,072 - INFO - [PDF 10] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-12 16:57:33,084 - INFO - [PDF 10] Procesamiento del PDF completado\n",
      "2025-08-12 16:57:34,258 - INFO - [PDF 9] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-12 16:57:34,269 - INFO - [PDF 9] Procesamiento del PDF completado\n",
      "2025-08-12 16:57:34,667 - INFO - [PDF 8] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-12 16:57:34,681 - INFO - [PDF 8] Procesamiento del PDF completado\n",
      "Batch 2/15: 100%|██████████| 6/6 [00:02<00:00,  2.49it/s]\n",
      "2025-08-12 16:57:34,689 - INFO - Lote 2/15 completado\n",
      "2025-08-12 16:57:34,691 - INFO - Procesando lote 3/15 con 6 archivos\n",
      "Batch 3/15:   0%|          | 0/6 [00:00<?, ?it/s]2025-08-12 16:57:34,694 - INFO - [PDF 12] Inicio del procesamiento del PDF: RECOMENDACIONES GAFI 49.pdf\n",
      "2025-08-12 16:57:34,696 - INFO - [PDF 17] Inicio del procesamiento del PDF: RML BANCO CENTRAL.pdf\n",
      "2025-08-12 16:57:34,696 - INFO - [PDF 15] Inicio del procesamiento del PDF: L1_IX_cap_IV.pdf\n",
      "2025-08-12 16:57:34,695 - INFO - [PDF 13] Inicio del procesamiento del PDF: RESUMEN-EJECUTIVO.pdf\n",
      "2025-08-12 16:57:34,697 - INFO - [PDF 16] Inicio del procesamiento del PDF: Spanish-Money-Laundering-National-Risk-Assessment-Guidance.pdf.coredownload.inline.pdf\n",
      "2025-08-12 16:57:34,696 - INFO - [PDF 14] Inicio del procesamiento del PDF: RESOLUCION UIF.pdf\n",
      "2025-08-12 16:57:34,701 - INFO - [PDF 17] Procesando PDF: RML BANCO CENTRAL.pdf\n",
      "2025-08-12 16:57:34,700 - INFO - [PDF 12] Procesando PDF: RECOMENDACIONES GAFI 49.pdf\n",
      "2025-08-12 16:57:34,702 - INFO - [PDF 15] Procesando PDF: L1_IX_cap_IV.pdf\n",
      "2025-08-12 16:57:34,702 - INFO - [PDF 16] Procesando PDF: Spanish-Money-Laundering-National-Risk-Assessment-Guidance.pdf.coredownload.inline.pdf\n",
      "2025-08-12 16:57:34,703 - INFO - [PDF 14] Procesando PDF: RESOLUCION UIF.pdf\n",
      "2025-08-12 16:57:34,703 - INFO - [PDF 13] Procesando PDF: RESUMEN-EJECUTIVO.pdf\n",
      "2025-08-12 16:57:34,724 - INFO - [PDF 17] Procesando 2 fragmentos\n",
      "2025-08-12 16:57:34,728 - INFO - [PDF 15] Procesando 35 fragmentos\n",
      "2025-08-12 16:57:34,729 - INFO - [PDF 12] Procesando 56 fragmentos\n",
      "2025-08-12 16:57:34,729 - INFO - [PDF 13] Procesando 30 fragmentos\n",
      "2025-08-12 16:57:34,742 - INFO - [PDF 16] Procesando 221 fragmentos\n",
      "2025-08-12 16:57:34,743 - INFO - Retrying request to /responses in 0.428865 seconds\n",
      "2025-08-12 16:57:34,753 - INFO - Retrying request to /responses in 0.376742 seconds\n",
      "2025-08-12 16:57:34,779 - WARNING - [PDF 14] No se encontraron fragmentos de texto\n",
      "2025-08-12 16:57:34,782 - INFO - [PDF 14] Procesamiento del PDF completado\n",
      "2025-08-12 16:57:34,786 - INFO - Retrying request to /responses in 0.440499 seconds\n",
      "2025-08-12 16:57:38,429 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:41,565 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:57:41,570 - INFO - Retrying request to /responses in 0.378171 seconds\n",
      "2025-08-12 16:57:42,018 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:57:42,024 - INFO - Retrying request to /responses in 0.378096 seconds\n",
      "2025-08-12 16:57:42,263 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:57:42,269 - INFO - Retrying request to /responses in 0.387635 seconds\n",
      "2025-08-12 16:57:42,612 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:57:42,616 - INFO - Retrying request to /responses in 0.483823 seconds\n",
      "2025-08-12 16:57:42,696 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:57:42,702 - INFO - Retrying request to /responses in 0.467786 seconds\n",
      "2025-08-12 16:57:44,299 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:57:44,304 - INFO - Retrying request to /responses in 0.493019 seconds\n",
      "2025-08-12 16:57:46,229 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:46,259 - INFO - [PDF 17] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-12 16:57:46,268 - INFO - [PDF 17] Procesamiento del PDF completado\n",
      "2025-08-12 16:57:48,302 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:57:48,311 - INFO - Retrying request to /responses in 0.763140 seconds\n",
      "2025-08-12 16:57:48,962 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:57:48,968 - INFO - Retrying request to /responses in 0.971991 seconds\n",
      "2025-08-12 16:57:49,060 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:50,067 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:57:50,075 - INFO - Retrying request to /responses in 0.758459 seconds\n",
      "2025-08-12 16:57:50,921 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:57:50,927 - INFO - Retrying request to /responses in 0.986688 seconds\n",
      "2025-08-12 16:57:52,751 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:52,933 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:53,112 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:53,486 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:54,154 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 196252, Requested 5748. Please try again in 600ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:57:54,640 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:54,803 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:56,028 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:56,620 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:57,040 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:57,182 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:57,938 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:57:57,944 - INFO - Retrying request to /responses in 0.472860 seconds\n",
      "2025-08-12 16:57:58,124 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 195282, Requested 5851. Please try again in 339ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:57:58,407 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:58,640 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:57:58,646 - INFO - Retrying request to /responses in 0.409081 seconds\n",
      "2025-08-12 16:57:59,404 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:57:59,412 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:57:59,418 - INFO - Retrying request to /responses in 0.454397 seconds\n",
      "2025-08-12 16:57:59,847 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:01,051 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:01,313 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:01,348 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:01,354 - INFO - Retrying request to /responses in 0.434922 seconds\n",
      "2025-08-12 16:58:02,255 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:02,260 - INFO - Retrying request to /responses in 0.486775 seconds\n",
      "2025-08-12 16:58:02,511 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:02,516 - INFO - Retrying request to /responses in 0.426843 seconds\n",
      "2025-08-12 16:58:03,287 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:03,292 - INFO - Retrying request to /responses in 0.453575 seconds\n",
      "2025-08-12 16:58:03,476 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:03,482 - INFO - Retrying request to /responses in 0.427178 seconds\n",
      "2025-08-12 16:58:03,559 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:03,877 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:03,953 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:04,197 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:04,355 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:04,377 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:04,383 - INFO - Retrying request to /responses in 0.772871 seconds\n",
      "2025-08-12 16:58:04,913 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:05,173 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:05,179 - INFO - Retrying request to /responses in 0.816855 seconds\n",
      "2025-08-12 16:58:05,399 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:05,505 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:05,512 - INFO - Retrying request to /responses in 0.401281 seconds\n",
      "2025-08-12 16:58:05,539 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:05,545 - INFO - Retrying request to /responses in 0.437035 seconds\n",
      "2025-08-12 16:58:05,878 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:05,883 - INFO - Retrying request to /responses in 0.915522 seconds\n",
      "2025-08-12 16:58:05,906 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:05,964 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:06,132 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:06,137 - INFO - Retrying request to /responses in 0.433342 seconds\n",
      "2025-08-12 16:58:06,688 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:06,955 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:06,959 - INFO - Retrying request to /responses in 0.781750 seconds\n",
      "2025-08-12 16:58:07,193 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:07,199 - INFO - Retrying request to /responses in 0.462386 seconds\n",
      "2025-08-12 16:58:07,274 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:07,279 - INFO - Retrying request to /responses in 0.826041 seconds\n",
      "2025-08-12 16:58:07,968 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:08,166 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:08,182 - INFO - Retrying request to /responses in 0.799962 seconds\n",
      "2025-08-12 16:58:08,629 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:08,636 - INFO - Retrying request to /responses in 0.406555 seconds\n",
      "2025-08-12 16:58:08,680 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:08,685 - INFO - Retrying request to /responses in 0.972122 seconds\n",
      "2025-08-12 16:58:08,778 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:08,783 - INFO - Retrying request to /responses in 0.980146 seconds\n",
      "2025-08-12 16:58:08,953 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:09,389 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:09,531 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:09,536 - INFO - Retrying request to /responses in 0.431777 seconds\n",
      "2025-08-12 16:58:09,702 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:09,734 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:09,739 - INFO - Retrying request to /responses in 0.453777 seconds\n",
      "2025-08-12 16:58:09,766 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:09,766 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:09,772 - INFO - Retrying request to /responses in 0.412401 seconds\n",
      "2025-08-12 16:58:10,601 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 198649, Requested 5808. Please try again in 1.337s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:58:10,733 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:10,738 - INFO - Retrying request to /responses in 0.472274 seconds\n",
      "2025-08-12 16:58:10,749 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:10,755 - INFO - Retrying request to /responses in 0.777986 seconds\n",
      "2025-08-12 16:58:11,097 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:11,102 - INFO - Retrying request to /responses in 0.478651 seconds\n",
      "2025-08-12 16:58:11,437 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:11,488 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:11,493 - INFO - Retrying request to /responses in 0.420535 seconds\n",
      "2025-08-12 16:58:11,560 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:11,565 - INFO - Retrying request to /responses in 0.481647 seconds\n",
      "2025-08-12 16:58:11,634 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:11,634 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:11,639 - INFO - Retrying request to /responses in 0.449194 seconds\n",
      "2025-08-12 16:58:11,842 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:11,847 - INFO - Retrying request to /responses in 0.880073 seconds\n",
      "2025-08-12 16:58:12,352 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 195423, Requested 5731. Please try again in 346ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:58:12,462 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:12,467 - INFO - Retrying request to /responses in 0.918916 seconds\n",
      "2025-08-12 16:58:12,555 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 194658, Requested 5749. Please try again in 122ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:58:12,603 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 194503, Requested 5722. Please try again in 67ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:58:12,952 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:13,439 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:13,506 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:13,527 - INFO - Retrying request to /responses in 0.985667 seconds\n",
      "2025-08-12 16:58:13,861 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:13,865 - INFO - Retrying request to /responses in 0.493563 seconds\n",
      "2025-08-12 16:58:14,400 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5726. Please try again in 1.717s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:58:14,582 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:14,587 - INFO - Retrying request to /responses in 0.425876 seconds\n",
      "2025-08-12 16:58:14,923 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 199913, Requested 5829. Please try again in 1.722s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:58:15,529 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:15,533 - INFO - Retrying request to /responses in 0.812869 seconds\n",
      "2025-08-12 16:58:15,558 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:15,564 - INFO - Retrying request to /responses in 0.494165 seconds\n",
      "2025-08-12 16:58:15,907 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 196625, Requested 5998. Please try again in 786ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:58:16,380 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:16,383 - INFO - Retrying request to /responses in 0.933142 seconds\n",
      "2025-08-12 16:58:16,458 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:16,462 - INFO - Retrying request to /responses in 0.926381 seconds\n",
      "2025-08-12 16:58:16,850 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:16,962 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:16,966 - INFO - Retrying request to /responses in 0.838781 seconds\n",
      "2025-08-12 16:58:17,405 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:17,411 - INFO - Retrying request to /responses in 0.856894 seconds\n",
      "2025-08-12 16:58:17,431 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:17,435 - INFO - Retrying request to /responses in 0.929695 seconds\n",
      "2025-08-12 16:58:17,440 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:17,774 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:17,902 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5772. Please try again in 1.731s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:58:18,333 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:18,338 - INFO - Retrying request to /responses in 0.471990 seconds\n",
      "2025-08-12 16:58:19,014 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 6056. Please try again in 1.816s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:58:19,029 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5791. Please try again in 1.737s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:58:19,759 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:20,266 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:20,272 - INFO - Retrying request to /responses in 0.946404 seconds\n",
      "2025-08-12 16:58:20,476 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 197959, Requested 5750. Please try again in 1.112s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:58:20,792 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:20,798 - INFO - Retrying request to /responses in 0.768847 seconds\n",
      "2025-08-12 16:58:20,829 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:21,210 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:21,216 - INFO - Retrying request to /responses in 0.841086 seconds\n",
      "2025-08-12 16:58:21,553 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:21,669 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 195113, Requested 5793. Please try again in 271ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:58:21,860 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 194380, Requested 6008. Please try again in 116ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:58:21,942 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:21,948 - INFO - Retrying request to /responses in 0.390970 seconds\n",
      "2025-08-12 16:58:22,759 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:23,062 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 198613, Requested 5796. Please try again in 1.322s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:58:23,066 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:58:23,200 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 198240, Requested 6048. Please try again in 1.286s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:58:23,585 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:23,590 - INFO - Retrying request to /responses in 0.973505 seconds\n",
      "2025-08-12 16:58:24,212 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 196616, Requested 5732. Please try again in 704ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:58:24,288 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:24,490 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 196327, Requested 5751. Please try again in 623ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:58:24,706 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:25,473 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:26,347 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 195214, Requested 5775. Please try again in 296ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:58:26,473 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:26,479 - INFO - Retrying request to /responses in 0.472683 seconds\n",
      "2025-08-12 16:58:26,584 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:26,590 - INFO - Retrying request to /responses in 0.394022 seconds\n",
      "2025-08-12 16:58:27,664 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 196636, Requested 5716. Please try again in 705ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:58:28,440 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:29,208 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:34,164 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:34,465 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:35,802 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:37,055 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:38,759 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:42,398 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:44,525 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:44,750 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:44,804 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:45,093 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:45,339 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:45,510 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:45,964 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:46,355 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:46,462 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:48,202 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:50,119 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:50,940 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:51,378 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:51,383 - INFO - Retrying request to /responses in 0.450943 seconds\n",
      "2025-08-12 16:58:52,121 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:52,127 - INFO - Retrying request to /responses in 0.493422 seconds\n",
      "2025-08-12 16:58:52,296 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:53,054 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:54,322 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:55,968 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:57,406 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 16:58:57,411 - INFO - Retrying request to /responses in 0.941123 seconds\n",
      "2025-08-12 16:58:57,478 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:57,612 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:58,284 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:59,170 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:58:59,890 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:00,138 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:01,113 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:02,655 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:03,175 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:03,307 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:03,418 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:03,429 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:03,852 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 196724, Requested 5974. Please try again in 809ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:59:05,874 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:09,321 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:11,232 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:12,392 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:12,969 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:14,417 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:14,980 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:16,990 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:19,308 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:20,221 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:21,848 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:23,324 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:25,253 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:28,030 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:28,068 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:28,880 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:29,535 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:33,770 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:34,215 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:34,985 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:35,240 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:35,800 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:37,925 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:38,921 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:39,362 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:41,651 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:43,156 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:46,371 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:47,141 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:52,309 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:52,711 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:55,607 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:55,897 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 16:59:59,541 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:00:06,583 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:00:07,144 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:00:07,369 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:00:08,096 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:00:08,598 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:00:10,900 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:00:11,904 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:00:13,569 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:00:19,704 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:00:22,566 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:00:25,544 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:00:29,173 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:00:32,582 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:00:34,296 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:00:35,820 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:00:38,790 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:00:39,070 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:00:40,148 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:00:40,366 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:00:41,445 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:00:44,350 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:00:44,675 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:00:50,569 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:00:50,758 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:00:51,171 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:00:57,550 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:00:57,593 - INFO - [PDF 13] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-12 17:00:57,611 - INFO - [PDF 13] Procesamiento del PDF completado\n",
      "2025-08-12 17:01:00,238 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:01:00,941 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:01:04,930 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:01:06,518 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:01:07,949 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:01:10,585 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:01:13,208 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:01:15,636 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:01:21,527 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:01:21,919 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:01:26,242 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:01:34,943 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:01:42,534 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:01:43,859 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:01:46,051 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:01:47,432 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:01:47,791 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:01:52,971 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:01:53,678 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:01:54,091 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:02:07,939 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:02:08,434 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:02:11,812 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:02:13,490 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:02:21,141 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:02:22,046 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:02:22,770 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:02:24,522 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:02:25,256 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:02:25,541 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:02:29,095 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:02:35,947 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:02:42,704 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:02:43,959 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:02:44,292 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:02:51,151 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:02:51,837 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:02:52,454 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:02:53,337 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:02:56,824 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:02:57,296 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:03:03,395 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:03:11,828 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:03:12,291 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:03:22,209 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:03:22,552 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:03:26,106 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:03:26,255 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:03:32,777 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:03:37,409 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:03:44,070 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:03:50,293 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:03:52,368 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:03:53,617 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:03:57,209 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:04:03,334 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:04:04,080 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:04:20,648 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:04:25,464 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:10:44,554 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generando conversación con Ollama: 1 validation error for Conversation\n",
      "  Invalid JSON: EOF while parsing a value at line 2419 column 17 [type=json_invalid, input_value='{ \"messages\": [\\n{ \"role... ,\\n\\n{ \"role\": \"user\",', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 17:10:57,729 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:11:08,712 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:11:12,749 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:11:24,717 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:11:29,257 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:11:34,598 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:11:41,681 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:11:45,439 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:11:51,782 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:11:57,895 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:12:04,010 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:12:04,065 - INFO - [PDF 15] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-12 17:12:04,079 - INFO - [PDF 15] Procesamiento del PDF completado\n",
      "2025-08-12 17:12:10,439 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:12:12,813 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:12:22,678 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:12:27,144 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:12:32,225 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:12:35,552 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:12:47,052 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:12:56,532 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:13:08,103 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:13:15,731 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:13:30,390 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:13:39,102 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:13:50,571 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:14:04,363 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:15:23,432 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:15:36,305 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:15:40,500 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:15:45,810 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:15:51,604 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:15:58,074 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:16:04,552 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:16:13,239 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:16:20,273 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:16:22,911 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:16:32,857 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:16:36,925 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:16:41,639 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:16:45,294 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:16:53,504 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:17:01,550 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:17:06,888 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:17:11,155 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:29:51,232 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generando conversación con Ollama: 1 validation error for Conversation\n",
      "  Invalid JSON: EOF while parsing a string at line 5418 column 1 [type=json_invalid, input_value='{\"messages\": [{\\n\\n\"role...árrafo?\"\\n\\n} , {\\n\\n\"', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 17:30:03,584 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:30:28,307 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:30:57,182 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:31:03,359 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:31:19,713 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:31:19,778 - ERROR - [PDF 12] Error al procesar el PDF: list indices must be integers or slices, not str\n",
      "2025-08-12 17:31:19,786 - INFO - [PDF 12] Procesamiento del PDF completado\n",
      "Batch 3/15:  17%|█▋        | 1/6 [33:45<2:48:45, 2025.09s/it]2025-08-12 17:31:42,157 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:31:57,221 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:32:09,690 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:32:17,717 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:32:30,476 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:32:39,075 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:32:59,135 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:33:11,252 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:33:27,616 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:33:36,250 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:34:08,398 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:34:19,184 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:34:39,398 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:34:55,117 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:35:04,603 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:35:17,926 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:53:58,207 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generando conversación con Ollama: 1 validation error for Conversation\n",
      "  Invalid JSON: EOF while parsing a string at line 238 column 1464 [type=json_invalid, input_value='{\"messages\": [ {\\n\"role\"...to y monitoreo.</li>\\\\n', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 17:54:17,676 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:54:30,216 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:54:39,853 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:54:51,781 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:55:02,713 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:55:30,018 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:55:35,943 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:55:50,252 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:55:57,366 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:56:22,546 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:56:53,884 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:57:01,678 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:57:10,517 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:57:24,144 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:57:44,750 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:57:55,358 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:58:10,467 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:58:21,647 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:58:33,900 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:58:57,963 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:59:10,082 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:59:15,512 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:59:30,725 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:59:40,718 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 17:59:53,843 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:00:07,786 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:00:23,284 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:00:32,443 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:00:42,987 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:00:57,062 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:01:04,551 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:01:15,478 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:01:25,072 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:01:33,372 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:01:47,922 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:02:05,863 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:02:12,635 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:02:20,097 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:02:28,155 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:02:36,512 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:02:41,353 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:02:55,124 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:03:03,866 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:03:17,849 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:03:39,988 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:03:49,795 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:03:54,182 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:03:59,426 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:04:08,096 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:04:14,797 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:04:23,809 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:04:40,850 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:04:47,516 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:05:04,511 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:05:19,999 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:05:24,029 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:05:34,740 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 18:05:43,118 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "files = get_files()\n",
    "generate(files,max_workers=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a4ebf-86b7-4ce0-a89a-975ee4a86092",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79e48dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.11/site-packages (4.0.0)\n",
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.11/site-packages (0.34.3)\n",
      "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.11/site-packages (8.1.7)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.11/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.11/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in ./.venv/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (1.1.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (9.4.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./.venv/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade datasets huggingface_hub ipywidgets pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1685376-51e6-4287-a74e-26a1e3deffb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/fine-tunning-models/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading data: 100%|██████████| 64/64 [00:00<00:00, 374909.85files/s]\n",
      "Generating train split: 4682 examples [00:00, 17601.63 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages', 'topic'],\n",
       "        num_rows: 4682\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"./outputs/data/*.jsonl\")\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "151e67bb-0111-4991-b87d-e724b183ea9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 55.02ba/s]\n",
      "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            \n",
      "\u001b[A\n",
      "Processing Files (0 / 1)                :  14%|█▎        |  524kB / 3.86MB, 1.31MB/s  \n",
      "Processing Files (0 / 1)                :  27%|██▋       | 1.05MB / 3.86MB, 1.75MB/s  \n",
      "Processing Files (0 / 1)                :  95%|█████████▌| 3.67MB / 3.86MB, 4.59MB/s  \n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████| 3.86MB / 3.86MB, 3.21MB/s  \n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████| 3.86MB / 3.86MB, 2.76MB/s  \n",
      "New Data Upload                         : 100%|██████████| 3.86MB / 3.86MB, 2.76MB/s  \n",
      "                                        : 100%|██████████| 3.86MB / 3.86MB            \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.87s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/jeanmcm/b_risks/commit/6f625bb43e6b588893a70d8769613ed11130179a', commit_message='Upload dataset', commit_description='', oid='6f625bb43e6b588893a70d8769613ed11130179a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/jeanmcm/b_risks', endpoint='https://huggingface.co', repo_type='dataset', repo_id='jeanmcm/b_risks'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "hf_token = os.environ.get(\"HUGGING_FACE_KEY\")\n",
    "dataset.push_to_hub(\"jeanmcm/b_risks\",token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f0891",
   "metadata": {},
   "source": [
    "# Testing RAG with Open WebUI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ce0bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Respuesta en streaming:</b> El riesgo financiero se refiere a las posibles pérdidas o daños económicos que pueden afectar a una empresa, individuo u organización debido a sus actividades financieras. Esto puede incluir la incertidumbre sobre los rendimientos de los activos financieros, el valor de los bienes y servicios, o la capacidad de un individuo o organización para generar ingresos.\n",
       "\n",
       "En el contexto del sistema financiero, las instituciones deben conocer a fondo las características particulares de las actividades económicas de sus clientes y las operaciones que realizan para identificar y gestionar los riesgos financieros asociados. Esto es fundamental para prevenir la ocurrencia de incumplimientos en las regulaciones financieras y minimizar el impacto de cualquier pérdida o daño.\n",
       "\n",
       "Según el artículo 43 del \"Bsoft Documents\", las instituciones del sistema financiero deben contar con una unidad de cumplimiento dirigida por un oficial de cumplimiento, que debe tener formación profesional en áreas como administración, contaduría, derecho o economía. Este equipo es responsable de garantizar que las políticas y procedimientos financieros sean adecuados y cumplan con los requisitos legales.\n",
       "\n",
       "Por lo tanto, el riesgo financiero se considera un aspecto crítico para las instituciones del sistema financiero, ya que puede tener graves consecuencias en su estabilidad y solvencia. Es importante que estas entidades tomen medidas proactivas para identificar, evaluar y gestionar los riesgos financieros asociados con sus actividades y operaciones.\n",
       "\n",
       "No se incluye ninguna cita porque no hay un id atributo en el source tag correspondiente a esta información, pero la respuesta se basa en el contexto proporcionado.."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "from IPython.display import display, clear_output, HTML\n",
    "\n",
    "url = \"https://vwlppjjfa98c9x-8080.proxy.runpod.net\"\n",
    "api_key =\"sk-05568562f28844fe997cadf960a346cd\"\n",
    "\n",
    "messages =  [{\"role\": \"user\", \"content\": \"Que es el Riesgo Financiero?\"}]\n",
    "\n",
    "try:\n",
    "    # Realizar la solicitud con stream=True\n",
    "    with requests.post(f\"{url}/api/chat/completions\", stream=True,headers={\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"Authorization\": f\"Bearer {api_key}\"\n",
    "    },json={\n",
    "      \"model\":\"bosft-riesgos-rag-model\",\n",
    "      \"messages\":messages,\n",
    "      \"stream\":True\n",
    "      }) as response:\n",
    "        response.raise_for_status()\n",
    "        # Variable para almacenar la salida acumulada\n",
    "        accumulated_output = \"\"\n",
    "\n",
    "        # Iterar sobre las líneas de la respuesta\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                # Decodificar la línea\n",
    "                decoded_line = line.decode('utf-8').strip()\n",
    "                # Si la línea comienza con \"data:\", extraer el contenido\n",
    "                if decoded_line.startswith(\"data:\"):\n",
    "                    decoded_line = decoded_line[5:].strip()  # Quitar \"data: \"\n",
    "\n",
    "                # Ignorar líneas vacías o marcadores de fin como \"[DONE]\"\n",
    "                if not decoded_line or decoded_line == \"[DONE]\" :\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    # Parsear si es JSON\n",
    "                    data = json.loads(decoded_line)\n",
    "                    if \"choices\" not in data: continue\n",
    "                    \n",
    "                    delta = data['choices'][0]['delta']\n",
    "                    if \"content\" in delta: new_data = delta['content']\n",
    "                except json.JSONDecodeError:\n",
    "                    # Si no es JSON, usar la línea como texto\n",
    "                    new_data = decoded_line\n",
    "\n",
    "                # Acumular y mostrar la salida dinámicamente\n",
    "                if new_data:\n",
    "                    accumulated_output += new_data\n",
    "                    # Limpiar la salida anterior y mostrar la nueva\n",
    "                    clear_output(wait=True)\n",
    "                    display(HTML(f\"<b>Respuesta en streaming:</b> {accumulated_output}\"))\n",
    "                    time.sleep(0.1)  # Pequeña pausa para visibilidad\n",
    "                    \n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"\\nError en la solicitud: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e64806a-ac56-417a-8dd9-175c3a8447a1",
   "metadata": {},
   "source": [
    "# Testing with Flowise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797cb1c0-bb15-4c97-9e0c-0156e8b90df1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
