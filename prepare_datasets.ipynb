{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "067529bb",
   "metadata": {},
   "source": [
    "# Generacion de Datasets utilizando RLHF para fine tunning\n",
    "\n",
    "## Procedimiento\n",
    "\n",
    "- Recoleccion de Datos: Obtener Informacion mediante documentos de textos seleccionables (no escaneados), mediante web, redes sociales, scrapping,etc\n",
    "- Extraer Texto: Usa PyMuPDF para extraer texto de PDFs no escaneados, procesando en paralelo con ProcessPoolExecutor para manejar grandes volúmenes (>100, >10,000).\n",
    "- Limpiar Datos: Elimina espacios múltiples y caracteres no deseados con expresiones regulares, asegurando texto coherente.\n",
    "- Validar: Verifica que los fragmentos extraídos no estén vacíos y tengan una longitud mínima (por ejemplo, 50 caracteres).\n",
    "- Generar Conversaciones: Divide el texto en párrafos, usa un modelo como meta-llama/Llama-3.2-1B-Instruct para generar diálogos conversacionales específicos al contenido, con al menos 4 intercambios por conversación, en formato {\"messages\": [{\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}], \"topic\": \"...\"}.\n",
    "- Estructurar Dataset: Guarda las conversaciones en archivos JSONL con dos columnas: \"messages\" y \"topic\".\n",
    "Cargar y Subir: Carga el dataset con load_dataset(\"json\", data_files=\"path/*.jsonl\") y súbelo a un repositorio privado en Hugging Face Hub con push_to_hub(\"username/dataset_name\", private=True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45295ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in /workspace/venv/lib/python3.11/site-packages (1.26.3)\n",
      "Requirement already satisfied: nltk in /workspace/venv/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: ollama in /workspace/venv/lib/python3.11/site-packages (0.5.1)\n",
      "Collecting ollama\n",
      "  Downloading ollama-0.5.3-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: openai in /workspace/venv/lib/python3.11/site-packages (1.99.1)\n",
      "Collecting openai\n",
      "  Downloading openai-1.99.3-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: click in /workspace/venv/lib/python3.11/site-packages (from nltk) (8.2.2)\n",
      "Requirement already satisfied: joblib in /workspace/venv/lib/python3.11/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /workspace/venv/lib/python3.11/site-packages (from nltk) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in /workspace/venv/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: httpx>=0.27 in /workspace/venv/lib/python3.11/site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in /workspace/venv/lib/python3.11/site-packages (from ollama) (2.11.7)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /workspace/venv/lib/python3.11/site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /workspace/venv/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /workspace/venv/lib/python3.11/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /workspace/venv/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /workspace/venv/lib/python3.11/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /workspace/venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /workspace/venv/lib/python3.11/site-packages (from httpx>=0.27->ollama) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /workspace/venv/lib/python3.11/site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /workspace/venv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /workspace/venv/lib/python3.11/site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /workspace/venv/lib/python3.11/site-packages (from pydantic>=2.9->ollama) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /workspace/venv/lib/python3.11/site-packages (from pydantic>=2.9->ollama) (0.4.1)\n",
      "Downloading ollama-0.5.3-py3-none-any.whl (13 kB)\n",
      "Downloading openai-1.99.3-py3-none-any.whl (785 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.8/785.8 kB\u001b[0m \u001b[31m187.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: openai, ollama\n",
      "\u001b[2K  Attempting uninstall: openai\n",
      "\u001b[2K    Found existing installation: openai 1.99.1\n",
      "\u001b[2K    Uninstalling openai-1.99.1:━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [openai]\n",
      "\u001b[2K      Successfully uninstalled openai-1.99.1━━━━\u001b[0m \u001b[32m0/2\u001b[0m [openai]\n",
      "\u001b[2K  Attempting uninstall: ollama━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [openai]\n",
      "\u001b[2K    Found existing installation: ollama 0.5.10m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [ollama]\n",
      "\u001b[2K    Uninstalling ollama-0.5.1:90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [ollama]\n",
      "\u001b[2K      Successfully uninstalled ollama-0.5.1━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [ollama]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [ollama]2m1/2\u001b[0m [ollama]\n",
      "\u001b[1A\u001b[2KSuccessfully installed ollama-0.5.1 openai-1.99.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pymupdf nltk ollama openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fa351d6-c527-423f-9d2e-417d0ef2b8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4760f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OLLAMA_MODEL = \"llama3.1:8b\"\n",
    "TEMPERATURE = 0.7  # Equilibrio entre creatividad y coherencia\n",
    "TOP_P = 0.9  # Filtrado de núcleo para diversidad\n",
    "MIN_CONVERSATION_LENGTH = 3 \n",
    "\n",
    "# Constantes\n",
    "MIN_FRAGMENT_LENGTH = 500\n",
    "MAX_FRAGMENT_LENGTH = 2000\n",
    "REPEAT_THRESHOLD = 0.3\n",
    "MIN_BLOCK_LENGTH = 30  # Reducido de 50 a 30 para incluir bloques más cortos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62be033f",
   "metadata": {},
   "source": [
    "### Recoleccion de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b45b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Verificar si la carpeta existe\n",
    "\n",
    "def get_files():\n",
    "    folder_url = \"./docs\"\n",
    "    folder = Path(folder_url)\n",
    "\n",
    "\n",
    "    if not folder.exists() or not folder.is_dir():\n",
    "        print(\"Invalid Folder\")\n",
    "        \n",
    "    # Obtener todos los archivos de la carpeta\n",
    "    files = [f for f in folder.rglob(\"*\") if f.is_file()]\n",
    "\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce4f060",
   "metadata": {},
   "source": [
    "### Extraccion de Texto y Limpieza de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ed080bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import re\n",
    "import os\n",
    "import hashlib\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text, repeated_blocks=None):\n",
    "    \"\"\"Limpia el texto, eliminando caracteres repetitivos, texto redundante y caracteres no deseados.\"\"\"\n",
    "    # Eliminar caracteres de control no deseados (excepto \\n)\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]', '', text)\n",
    "    # Eliminar patrones repetitivos como ----, ...., ****\n",
    "    text = re.sub(r'([^\\w\\s])\\1{2,}|\\s*[.]{3,}\\s*', '', text)\n",
    "    # Normalizar espacios múltiples\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    # Eliminar espacios al inicio y final de cada línea\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    # Filtrar líneas duplicadas, números, correos y contenido administrativo\n",
    "    seen_lines = set()\n",
    "    unique_lines = []\n",
    "    for line in lines:\n",
    "        if line not in seen_lines and \\\n",
    "           not re.match(r'^\\d+$', line) and \\\n",
    "           not re.match(r'.*@(.*\\.)+.*', line) and \\\n",
    "           not re.match(r'^(Tel|Fax|E-mail|www\\.).*', line, re.IGNORECASE) and \\\n",
    "           (repeated_blocks is None or line not in repeated_blocks):\n",
    "            unique_lines.append(line)\n",
    "            seen_lines.add(line)\n",
    "    return '\\n'.join(unique_lines)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extrae texto de un PDF en una sola pasada, preservando el texto del inicio de la página,\n",
    "    eliminando encabezados, pies de página y contenido irrelevante,\n",
    "    dividiendo en fragmentos de 500 a 2000 caracteres con metadata corregida.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = pymupdf.open(pdf_path)\n",
    "        total_pages = len(doc)\n",
    "        chunks = []\n",
    "        current_chunk = []  # Lista de (párrafo, página)\n",
    "        current_chunk_length = 0\n",
    "        filename = os.path.basename(pdf_path)\n",
    "        block_counter = Counter()\n",
    "\n",
    "        for page_number in range(1, total_pages + 1):\n",
    "            page = doc[page_number - 1]\n",
    "            page_height = page.rect.height\n",
    "            blocks = page.get_text(\"blocks\")\n",
    "            page_text = []\n",
    "\n",
    "            # Procesar bloques y filtrar encabezados/pies de página\n",
    "            for block in blocks:\n",
    "                text = block[4]\n",
    "                y0, y1 = block[1], block[3]\n",
    "                # Excluir pies de página (parte inferior de la página)\n",
    "                if y1 > 0.95 * page_height:  # Relajado de 0.95 a 0.9\n",
    "                    continue\n",
    "                # Excluir encabezados solo si son repetitivos\n",
    "                if y0 < 0.05 * page_height:  # Relajado de 0.05 a 0.1\n",
    "                    block_counter[text] += 1\n",
    "                    if block_counter[text] > total_pages * REPEAT_THRESHOLD:\n",
    "                        continue\n",
    "                if text and len(text) >= MIN_BLOCK_LENGTH:\n",
    "                    block_counter[text] += 1\n",
    "                    page_text.append((text, page_number))\n",
    "\n",
    "            # Si no hay texto válido en la página, continuar\n",
    "            if not page_text:\n",
    "                continue\n",
    "\n",
    "            # Acumular párrafos con su número de página\n",
    "            for paragraph, page in page_text:\n",
    "                current_chunk.append((paragraph, page))\n",
    "                current_chunk_length += len(paragraph) + 2  # +2 por \"\\n\\n\"\n",
    "\n",
    "                # Si el fragmento alcanza la longitud mínima, procesarlo\n",
    "                if current_chunk_length >= MIN_FRAGMENT_LENGTH:\n",
    "                    chunk_text = \"\\n\\n\".join(p for p, _ in current_chunk)\n",
    "                    cleaned_chunk = clean_text(chunk_text, None)\n",
    "                    cleaned_length = len(cleaned_chunk)\n",
    "\n",
    "                    # Obtener el rango de páginas del fragmento\n",
    "                    page_numbers = sorted(set(page for _, page in current_chunk))\n",
    "                    start_page = page_numbers[0] if page_numbers else page_number\n",
    "                    end_page = page_numbers[-1] if page_numbers else page_number\n",
    "\n",
    "                    # Dividir fragmentos largos\n",
    "                    while cleaned_length > MAX_FRAGMENT_LENGTH:\n",
    "                        sub_chunk = cleaned_chunk[:MAX_FRAGMENT_LENGTH]\n",
    "                        last_paragraph_end = sub_chunk.rfind(\"\\n\\n\")\n",
    "                        if last_paragraph_end == -1:\n",
    "                            last_paragraph_end = MAX_FRAGMENT_LENGTH\n",
    "                        chunk_to_add = cleaned_chunk[:last_paragraph_end].strip()\n",
    "\n",
    "                        # Calcular el número de páginas para el subfragmento\n",
    "                        chars_so_far = 0\n",
    "                        sub_chunk_pages = []\n",
    "                        for paragraph, page in current_chunk:\n",
    "                            chars_so_far += len(paragraph) + 2\n",
    "                            if chars_so_far <= last_paragraph_end:\n",
    "                                sub_chunk_pages.append(page)\n",
    "                            else:\n",
    "                                break\n",
    "                        sub_start_page = min(sub_chunk_pages) if sub_chunk_pages else page_number\n",
    "                        sub_end_page = max(sub_chunk_pages) if sub_chunk_pages else page_number\n",
    "\n",
    "                        metadata = (\n",
    "                            f\"# FILENAME: {filename} | CHARACTERS: {len(chunk_to_add)} | \"\n",
    "                            f\"PAGES: {sub_start_page}-{sub_end_page}/{total_pages}\\n\\n\"\n",
    "                        )\n",
    "                        chunks.append((metadata + chunk_to_add, hashlib.md5(chunk_to_add.encode()).hexdigest()))\n",
    "                        cleaned_chunk = cleaned_chunk[last_paragraph_end:].strip()\n",
    "                        cleaned_length = len(cleaned_chunk)\n",
    "\n",
    "                        # Actualizar current_chunk para los párrafos restantes\n",
    "                        remaining_chunk = []\n",
    "                        chars_so_far = 0\n",
    "                        for paragraph, page in current_chunk:\n",
    "                            chars_so_far += len(paragraph) + 2\n",
    "                            if chars_so_far > last_paragraph_end:\n",
    "                                remaining_chunk.append((paragraph, page))\n",
    "                        current_chunk = remaining_chunk\n",
    "                        current_chunk_length = cleaned_length\n",
    "                        page_numbers = sorted(set(page for _, page in current_chunk))\n",
    "                        start_page = page_numbers[0] if page_numbers else page_number\n",
    "\n",
    "                    # Añadir el fragmento completo\n",
    "                    if cleaned_length >= MIN_FRAGMENT_LENGTH:\n",
    "                        metadata = (\n",
    "                            f\"# FILENAME: {filename} | CHARACTERS: {cleaned_length} | \"\n",
    "                            f\"PAGES: {start_page}-{end_page}/{total_pages}\\n\\n\"\n",
    "                        )\n",
    "                        chunks.append((metadata + cleaned_chunk, hashlib.md5(cleaned_chunk.encode()).hexdigest()))\n",
    "                        current_chunk = []\n",
    "                        current_chunk_length = 0\n",
    "\n",
    "                    else:\n",
    "                        current_chunk = [(cleaned_chunk, page_numbers[-1])] if page_numbers else []\n",
    "                        current_chunk_length = cleaned_length\n",
    "\n",
    "        # Añadir el fragmento final si cumple con la longitud mínima\n",
    "        if current_chunk and current_chunk_length >= MIN_FRAGMENT_LENGTH:\n",
    "            chunk_text = \"\\n\\n\".join(p for p, _ in current_chunk)\n",
    "            cleaned_chunk = clean_text(chunk_text, None)\n",
    "            cleaned_length = len(cleaned_chunk)\n",
    "            if cleaned_length >= MIN_FRAGMENT_LENGTH:\n",
    "                page_numbers = sorted(set(page for _, page in current_chunk))\n",
    "                start_page = page_numbers[0] if page_numbers else total_pages\n",
    "                end_page = page_numbers[-1] if page_numbers else total_pages\n",
    "                metadata = (\n",
    "                    f\"# FILENAME: {filename} | CHARACTERS: {cleaned_length} | \"\n",
    "                    f\"PAGES: {start_page}-{end_page}/{total_pages}\\n\\n\"\n",
    "                )\n",
    "                chunks.append((metadata + cleaned_chunk, hashlib.md5(cleaned_chunk.encode()).hexdigest()))\n",
    "\n",
    "        doc.close()\n",
    "\n",
    "        # Filtrar bloques repetitivos y duplicados\n",
    "        repeated_blocks = {text for text, count in block_counter.items() if count > total_pages * REPEAT_THRESHOLD}\n",
    "        final_chunks = []\n",
    "        seen_hashes = set()\n",
    "\n",
    "        for chunk, chunk_hash in chunks:\n",
    "            chunk_text = '\\n'.join(line for line in chunk.splitlines() if not line.startswith('#'))\n",
    "            cleaned_chunk = clean_text(chunk_text, repeated_blocks)\n",
    "            if len(cleaned_chunk) >= MIN_FRAGMENT_LENGTH and chunk_hash not in seen_hashes:\n",
    "                # Actualizar la longitud en la metadata después de la limpieza final\n",
    "                metadata_lines = chunk.splitlines()[0]\n",
    "                metadata = re.sub(r'CHARACTERS: \\d+', f'CHARACTERS: {len(cleaned_chunk)}', metadata_lines)\n",
    "                final_chunks.append(f\"{metadata}\\n\\n{cleaned_chunk}\")\n",
    "                seen_hashes.add(chunk_hash)\n",
    "\n",
    "        return final_chunks if final_chunks else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {pdf_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a912cd27",
   "metadata": {},
   "source": [
    "### Generar Conversaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c7a947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "class Message(BaseModel):\n",
    "    role: str = Field(..., pattern=\"^(user|assistant)$\")\n",
    "    content: str\n",
    "\n",
    "class Conversation(BaseModel):\n",
    "    messages: List[Message] = Field(..., min_items=MIN_CONVERSATION_LENGTH)\n",
    "    topic: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "82198409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompts(fragment: str):\n",
    "  INSTRUCTIONS = \"\"\"\n",
    "  Eres un generador de conversaciones optimizadas para entrenamiento supervisado (SFT) con técnicas avanzadas de prompt engineering. \n",
    "Tu objetivo es producir diálogos profesionales, coherentes y progresivos basados en fragmentos de texto, orientados al ámbito laboral. \n",
    "Cada conversación debe tener continuidad entre turnos y mantener un estilo claro, formal pero natural.\n",
    "\n",
    "<instrucciones>\n",
    "\n",
    "1. **Formato de salida**:\n",
    "- Devuelve un JSON válido con:\n",
    "  - `messages`: Lista con { \"role\": \"user\" | \"assistant\", \"content\": \"...\" }\n",
    "  - `topic`: cadena breve (máx. 50 caracteres)\n",
    "- **No incluir rol \"system\"** en la salida final.\n",
    "\n",
    "2. **Estructura de la conversación**:\n",
    "- 3 a 4 intercambios (mínimo 4 y máximo 8 mensajes).\n",
    "- Siempre empieza con \"user\" y alterna roles.\n",
    "- Cada mensaje del usuario debe partir del contexto previo y avanzar el tema.\n",
    "- Respuestas del asistente con razonamiento paso a paso (chain-of-thought) y ejemplos prácticos.\n",
    "- Usa Markdown para estructurar la respuesta: títulos, listas, negritas, etc.\n",
    "\n",
    "3. **Tono y estilo**:\n",
    "- Profesional y aplicable en contextos de trabajo (consultoría, gestión de proyectos, análisis técnico, comunicación corporativa).\n",
    "- Lenguaje formal pero natural, evitando jerga innecesaria.\n",
    "- Respuestas concisas pero completas, con enfoque en resultados y acciones concretas.\n",
    "\n",
    "4. **Patrones de pregunta** (adaptados a entornos laborales):\n",
    "- \"Me gustaría implementar {tema} en la empresa. ¿Qué pasos sugieres?\"\n",
    "- \"Basado en {contexto}, propón un plan detallado para {objetivo}.\"\n",
    "- \"Genera un cronograma para {proyecto}, asignando responsables.\"\n",
    "- \"Identifica riesgos y métricas para {iniciativa}.\"\n",
    "- \"Crea un documento de recomendaciones sobre {tema}.\"\n",
    "- \"Evalúa ventajas y desventajas de {opción1} frente a {opción2}.\"\n",
    "- \"Desglosa en fases el proceso para {tarea}.\"\n",
    "- \"Propón mejoras para optimizar {área o proceso}.\"\n",
    "- \"Dame indicadores clave para medir {resultado}.\"\n",
    "- \"Elabora un resumen ejecutivo sobre {tema} para la gerencia.\"\n",
    "\n",
    "5. **Criterios de calidad**:\n",
    "- Coherencia en todos los turnos.\n",
    "- Uso de información previa en las respuestas siguientes.\n",
    "- Evitar repeticiones innecesarias.\n",
    "- Claridad en la explicación y aplicabilidad práctica.\n",
    "\n",
    "\n",
    "<PROMPT_PATTERNS>\n",
    "- \"Me intriga {tema}. ¿Puedes explicármelo detalladamente?\"\n",
    "- \"En pocas palabras, resúmeme cómo funciona {tema}.\"\n",
    "- \"Crea una carta formal sobre {tema} dirigida a {destinatario}.\"\n",
    "- \"Basado en {contexto}, explícame qué significa {tema}.\"\n",
    "- \"Quiero mejorar en {tema}. ¿Qué consejos prácticos me das?\"\n",
    "- \"Agrega una sección sobre {tema} a este informe: {contexto}.\"\n",
    "- \"No entiendo {tema}. ¿Puedes desglosarlo en pasos simples?\"\n",
    "- \"Imagina que soy nuevo en {tema}. ¿Cómo me lo explicarías?\"\n",
    "- \"Podrías darme un resumen sobre {tema} y sus implicaciones?\"\n",
    "- \"Estoy preocupado por {tema}. ¿Qué debería saber al respecto?\"\n",
    "- \"Explícame {tema} como si fuera un niño de 10 años.\"\n",
    "- \"Redacta un correo informal sobre {tema} para un colega.\"\n",
    "</PROMPT_PATTERNS>\n",
    "\"\"\"\n",
    "\n",
    "  BASE_EXAMPLE = \"\"\"\"\n",
    "\n",
    "**Ejemplo n — Gestión Documental**\n",
    "<fragment>\n",
    "Implementar un sistema de gestión documental para mejorar la búsqueda y control de versiones.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Necesito un plan para implementar un sistema de gestión documental en la empresa. Tenemos problemas con la búsqueda de archivos y duplicación de versiones.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Claro. Vamos a estructurarlo paso a paso:\\\\n\\\\n**1. Análisis inicial**\\\\n- Auditar los flujos actuales de manejo documental.\\\\n- Identificar carpetas duplicadas y problemas de control de versiones.\\\\n\\\\n**2. Selección de plataforma**\\\\n- Comparar opciones como SharePoint, Alfresco o Google Workspace.\\\\n- Priorizar integración, seguridad y facilidad de búsqueda.\\\\n\\\\n**3. Migración y normalización**\\\\n- Definir nomenclatura estándar.\\\\n- Centralizar repositorios.\\\\n\\\\n**4. Capacitación y seguimiento**\\\\n- Formación en la plataforma elegida.\\\\n- Revisión mensual para detectar desviaciones.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Sí, dame el cronograma y asigna responsables para un equipo de 5 personas en administración.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"**Cronograma — Implementación Gestión Documental**\\\\n\\\\n**Semana 1-2 — Auditoría**\\\\n- Responsable: Laura (Jefa de administración)\\\\n\\\\n**Semana 3 — Selección de plataforma**\\\\n- Responsable: Jorge (Analista de sistemas)\\\\n\\\\n**Semana 4-6 — Migración inicial**\\\\n- Responsables: Ana y Ricardo\\\\n\\\\n**Semana 7 — Capacitación**\\\\n- Responsable: Mariana\\\\n\\\\n**Semana 8-9 — Seguimiento**\\\\n- Responsable: Laura\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Perfecto. ¿Puedes incluir métricas para evaluar si la implementación fue exitosa?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"**Métricas de éxito**:\\\\n- Tiempo de búsqueda reducido en 50%.\\\\n- Menos del 5% de duplicados.\\\\n- 90% de cumplimiento en nomenclatura.\\\\n- 95% de adopción entre usuarios.\\\\n- Satisfacción ≥ 8/10 en encuestas internas.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Implementación de gestión documental\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  EXAMPLES = \"\"\"\n",
    "<ejemplos>\n",
    "\n",
    "Ejemplo Base:\n",
    "%s\n",
    "\n",
    "Otros Ejemplos:\n",
    "\n",
    "**Ejemplo 1 — Técnico (dispersión de luz, patrón: explicación detallada, fragmento largo)**:\n",
    "<fragment>\n",
    "La dispersión de Rayleigh explica por qué el cielo aparece azul durante el día. Las partículas en la atmósfera dispersan preferentemente las longitudes de onda cortas, como el azul, debido a su interacción con la luz solar. Durante el atardecer, el cielo adquiere tonos rojos porque la luz atraviesa más atmósfera, dispersando el azul y dejando predominar las longitudes de onda largas. Este fenómeno depende de la densidad atmosférica y el ángulo de incidencia de la luz solar.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Me intriga por qué el cielo es azul de día y rojo al atardecer. ¿Puedes explicármelo detalladamente?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"# Dispersión de Rayleigh y el Color del Cielo\\n\\nPara entender por qué el cielo cambia de color, desglosemos el fenómeno paso a paso:\\n\\n1. **Composición de la luz solar**: La luz del sol parece blanca, pero está formada por todas las longitudes de onda visibles, desde el rojo (largo) hasta el violeta (corto).\\n\\n2. **Dispersión de Rayleigh**: En la atmósfera, las partículas (como moléculas de nitrógeno y oxígeno) dispersan la luz. La dispersión es más intensa para longitudes de onda cortas, como el azul, porque es inversamente proporcional a la **cuarta potencia** de la longitud de onda. Esto significa que el azul se esparce mucho más que el rojo.\\n\\n3. **Cielo diurno**: Durante el día, la luz solar atraviesa una capa relativamente delgada de atmósfera. El azul se dispersa en todas direcciones, haciendo que el cielo se vea azul. Aunque el violeta se dispersa más, nuestros ojos son menos sensibles a él, y parte es absorbida, por lo que el azul domina.\\n\\n4. **Atardecer**: Al atardecer, el sol está más bajo, y la luz atraviesa una capa más gruesa de atmósfera. Esto dispersa casi todo el azul, permitiendo que las longitudes de onda largas (rojo, naranja) lleguen a nuestros ojos, dando tonos cálidos.\\n\\n5. **Factores adicionales**: La densidad atmosférica y el ángulo de incidencia afectan la intensidad del fenómeno. Por ejemplo, más partículas (como polvo) pueden intensificar los colores del atardecer.\\n\\nEn resumen, la dispersión de Rayleigh explica el azul diurno y los tonos rojos del atardecer, influenciada por la física de la luz y la geometría de la Tierra.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"¿Por qué a veces el cielo se ve más violeta en ciertos atardeceres?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"A veces, el cielo puede parecer violeta en el atardecer por la presencia de partículas adicionales, como polvo o contaminación, que dispersan más longitudes de onda cortas, incluyendo el violeta. Además, si la atmósfera está muy limpia y el sol está en un ángulo específico, el violeta puede ser más visible porque menos se absorbe. Sin embargo, nuestros ojos suelen percibir más el azul, así que el violeta es menos común.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Dispersión de luz y color del cielo\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 2 — Técnico (dispersión de luz, patrón: resumen breve, fragmento corto)**:\n",
    "<fragment>\n",
    "La dispersión de Rayleigh hace que el cielo sea azul al dispersar más las longitudes de onda cortas.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"En pocas palabras, resúmeme cómo funciona la dispersión de luz en el cielo.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"La dispersión de Rayleigh ocurre cuando las partículas atmosféricas dispersan más las longitudes de onda cortas, como el azul, haciendo que el cielo se vea azul durante el día.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Dispersión de luz en el cielo\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 3 — Contextual (política monetaria, patrón: carta formal, fragmento largo)**:\n",
    "<fragment>\n",
    "Subir las tasas de interés es una herramienta del banco central para controlar la inflación. Al encarecer los préstamos, se reduce el consumo y la inversión, lo que desacelera la economía y disminuye la presión sobre los precios. Sin embargo, un aumento excesivo puede llevar a una recesión, afectando el empleo y el crecimiento económico. Los bancos centrales deben monitorear indicadores como el consumo y el desempleo para ajustar las tasas cuidadosamente.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Crea una carta formal sobre el impacto de las tasas de interés, dirigida al director de mi banco.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"**Asunto**: Preocupación por el Impacto de las Tasas de Interés\\n\\nEstimado Director,\\n\\nMe dirijo a usted para expresar mi preocupación sobre el reciente aumento de las tasas de interés y su impacto en los clientes. Entiendo que el banco central sube las tasas para controlar la inflación, encareciendo los préstamos, lo que reduce el consumo y la presión sobre los precios. Sin embargo, esto afecta directamente a los consumidores, elevando el costo de hipotecas y créditos, y podría desencadenar una recesión si las tasas suben demasiado.\\n\\nSolicito información sobre cómo el banco planea apoyar a los clientes en este contexto, como opciones de refinanciamiento o asesoramiento financiero. Espero su respuesta en un plazo de 7 días hábiles.\\n\\nAtentamente,  \\n[Nombre del Cliente]\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Agrega una mención sobre el impacto en las pequeñas empresas.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"**Asunto**: Preocupación por el Impacto de las Tasas de Interés\\n\\nEstimado Director,\\n\\nMe dirijo a usted para expresar mi preocupación sobre el aumento de las tasas de interés. Estas medidas, destinadas a controlar la inflación al encarecer los préstamos, reducen el consumo y la inversión, afectando tanto a los consumidores como a las pequeñas empresas, que enfrentan mayores costos de financiamiento. Esto podría desencadenar una recesión, impactando el empleo y el crecimiento económico.\\n\\nSolicito información sobre cómo el banco apoyará a los clientes, incluyendo a pequeñas empresas, con opciones como refinanciamiento o asesoramiento financiero. Espero su respuesta en 7 días hábiles.\\n\\nAtentamente,  \\n[Nombre del Cliente]\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Impacto de tasas de interés\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 4 — Incoherente (fragmento ambiguo, patrón: contexto adicional, fragmento corto)**:\n",
    "<fragment>\n",
    "...fluctuaciones de energía... inestabilidad...\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Basado en problemas con el parpadeo de luces en mi casa, explícame qué significan fluctuaciones de energía.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Las fluctuaciones de energía son cambios en el voltaje eléctrico, causando inestabilidad como el parpadeo de luces. Pueden deberse a cableado defectuoso o sobrecarga en la red. Usa un regulador de voltaje o consulta a un electricista.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Fluctuaciones de energía\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 5 — Casual (salud visual, patrón: consejos prácticos, fragmento largo)**:\n",
    "<fragment>\n",
    "La exposición prolongada a la luz azul de pantallas puede interrumpir la producción de melatonina, afectando el ciclo de sueño. Esto ocurre porque la luz azul engaña al cerebro, haciéndole creer que es de día, lo que retrasa el inicio del sueño. También puede causar fatiga visual y molestias como sequedad ocular.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Quiero mejorar mi sueño porque paso mucho tiempo frente a pantallas. ¿Qué consejos prácticos me das?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"# Mejorar el Sueño y Proteger la Vista frente a Pantallas\\n\\nLa luz azul de las pantallas puede afectar tu sueño y ojos, pero hay formas prácticas de mitigar esto. Aquí van algunos consejos:\\n\\n- **Usa el modo nocturno**: Activa el filtro de luz azul en tus dispositivos para reducir su emisión, especialmente por la noche.\\n- **Evita pantallas antes de dormir**: Intenta no usar dispositivos al menos 1 hora antes de acostarte para que la melatonina, la hormona del sueño, se produzca normalmente.\\n- **Aplica la regla 20-20-20**: Cada 20 minutos, mira algo a 20 pies de distancia durante 20 segundos para descansar tus ojos y evitar fatiga visual.\\n- **Considera gafas con filtro**: Si trabajas mucho con pantallas, unas gafas con filtro de luz azul pueden reducir molestias como sequedad ocular.\\n- **Mantén un ambiente adecuado**: Usa iluminación suave en tu espacio de trabajo para reducir el contraste con la pantalla.\\n\\n*Por qué funciona*: La luz azul engaña al cerebro, haciéndole creer que es de día, lo que inhibe la melatonina y retrasa el sueño. Estos pasos ayudan a proteger tu ciclo de sueño y la salud de tus ojos.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"¿Qué hago si no puedo evitar las pantallas por trabajo?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Si no puedes evitar pantallas, refuerza las medidas: usa siempre el modo nocturno, haz pausas frecuentes con la regla 20-20-20, y considera gafas con filtro de luz azul. También ajusta el brillo de la pantalla para que sea similar al de tu entorno y evita trabajar en completa oscuridad.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Luz azul y sueño\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 6 — Contextual (criptografía, patrón: informe, fragmento largo)**:\n",
    "<fragment>\n",
    "La criptografía de clave pública utiliza un par de claves: una pública para cifrar y una privada para descifrar. Esto asegura que solo el destinatario con la clave privada pueda acceder al mensaje, garantizando confidencialidad y autenticidad. Es ampliamente usada en transacciones seguras en línea.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Agrega una sección sobre criptografía de clave pública a este informe: 'Seguridad en transacciones digitales'.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"## Criptografía de Clave Pública en Transacciones Digitales\\n\\nLa **criptografía de clave pública** es un pilar fundamental para la seguridad en transacciones digitales. Funciona mediante un par de claves:\\n\\n- **Clave pública**: Se comparte libremente y permite cifrar mensajes, convirtiéndolos en texto ilegible.\\n- **Clave privada**: Solo la posee el destinatario y permite descifrar el mensaje.\\n\\nEste mecanismo asegura:\\n\\n1. **Confidencialidad**: Solo el destinatario con la clave privada puede leer el mensaje.\\n2. **Autenticidad**: Si el remitente firma con su clave privada, el receptor verifica su identidad con la clave pública.\\n\\n*Ejemplo práctico*: En compras online, la criptografía de clave pública protege los datos de tu tarjeta, asegurando que solo el vendedor autorizado los descifre. Es esencial para protocolos como HTTPS y sistemas de pago seguro.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Criptografía en transacciones\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 7 — Técnico (cambio climático, patrón: pasos simples, fragmento corto)**:\n",
    "<fragment>\n",
    "El cambio climático aumenta las temperaturas globales debido a los gases de efecto invernadero.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"No entiendo el cambio climático. ¿Puedes desglosarlo en pasos simples?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"1. Quemamos combustibles como carbón y petróleo.\\n2. Esto libera gases de efecto invernadero, como el dióxido de carbono.\\n3. Los gases atrapan calor en la atmósfera.\\n4. El planeta se calienta, causando cambio climático.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Cambio climático explicado\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 8 — Casual (nutrición, patrón: explicación para principiantes, fragmento largo)**:\n",
    "<fragment>\n",
    "El consumo excesivo de azúcar puede aumentar el riesgo de diabetes tipo 2 y obesidad. Reducir la ingesta de bebidas azucaradas y alimentos procesados es clave para una dieta saludable. Los azúcares añadidos en productos como refrescos, dulces y salsas comerciales suelen ser los principales culpables.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Imagina que soy nuevo en nutrición. ¿Cómo me explicarías lo del azúcar?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Piénsalo como gasolina para tu cuerpo: el azúcar te da energía, pero demasiada puede hacerte daño. Hay azúcar natural, como en las frutas, que está bien en moderación. Pero el problema es el **azúcar añadido**, que encuentras en refrescos, dulces o salsas. Comer mucho de esto puede subir tu peso y aumentar el riesgo de diabetes tipo 2, porque tu cuerpo lucha para manejar tanto azúcar. Para comer mejor, cambia refrescos por agua, elige snacks como nueces y lee etiquetas para evitar productos con azúcares ocultos.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Azúcar y salud\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 9 — Incoherente (fragmento sin contexto, patrón: resumen e implicaciones, fragmento corto)**:\n",
    "<fragment>\n",
    "...error crítico en el núcleo...\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Podrías darme un resumen sobre un error crítico en el núcleo y sus implicaciones?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"# Error Crítico en el Núcleo\\n\\n**Resumen**: Un error crítico en el núcleo es un fallo grave en el sistema operativo, que actúa como el cerebro de tu computadora. Puede ocurrir por hardware defectuoso, software corrupto o malware.\\n\\n**Implicaciones**:\\n- **Pérdida de datos**: Los archivos pueden perderse si el sistema no guarda correctamente.\\n- **Interrupciones**: El sistema puede congelarse o reiniciarse.\\n- **Costos**: Reparaciones o recuperación de datos pueden ser necesarias.\\n\\n**Acción**: Usa software de recuperación como TestDisk y haz backups regulares para minimizar riesgos.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Error crítico en sistema\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 10 — Contextual (economía, patrón: preocupación, fragmento largo)**:\n",
    "<fragment>\n",
    "La inflación persistente puede erosionar el poder adquisitivo, haciendo que los bienes y servicios sean más caros. Los bancos centrales intentan controlarla con políticas monetarias, pero esto puede tener efectos secundarios en el empleo y el crecimiento económico.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Estoy preocupado por la inflación. ¿Qué debería saber al respecto?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"# Entendiendo la Inflación\\n\\nLa inflación ocurre cuando los precios de bienes y servicios suben, reduciendo tu **poder adquisitivo** (es decir, con el mismo dinero compras menos). Aquí está lo que debes saber:\\n\\n- **Causas**: Puede ser por alta demanda, costos de producción elevados o políticas monetarias expansivas.\\n- **Impacto**: Tus ahorros valen menos, y cosas como alimentos o gasolina se encarecen.\\n- **Control**: Los bancos centrales suben tasas de interés para reducir el gasto, pero esto puede ralentizar la economía, afectando empleos.\\n\\n**Qué hacer**:\\n- **Ahorra inteligentemente**: Invierte en activos que superen la inflación, como bonos ajustados.\\n- **Presupuesto**: Prioriza gastos esenciales y reduce los no necesarios.\\n- **Infórmate**: Sigue las políticas del banco central para anticipar cambios.\\n\\nHabla con un asesor financiero si quieres estrategias específicas.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Inflación y poder adquisitivo\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 11 — Técnico (energía renovable, patrón: explicación para niño, fragmento corto)**:\n",
    "<fragment>\n",
    "La energía solar usa paneles para convertir la luz del sol en electricidad.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Explícame la energía solar como si fuera un niño de 10 años.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Imagina que el sol es como una lámpara gigante que brilla todo el día. La **energía solar** usa placas especiales, como espejos mágicos, que atrapan la luz del sol y la convierten en electricidad para encender luces, televisions o cargar tu tablet. ¡Es como usar el poder del sol para hacer cosas cool!\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Energía solar simplificada\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 12 — Casual (trabajo remoto, patrón: correo informal, fragmento largo)**:\n",
    "<fragment>\n",
    "El trabajo remoto puede aumentar la productividad, pero también presenta desafíos como la falta de interacción social y dificultades para desconectarse del trabajo. Herramientas como Zoom y Slack facilitan la colaboración, pero requieren una buena gestión del tiempo.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Redacta un correo informal sobre trabajo remoto para un colega.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"**Asunto**: Tips para el Trabajo Remoto\\n\\nHola [Nombre],\\n\\nEstuve pensando en cómo sacarle provecho al trabajo desde casa. Puede ser súper productivo porque evitas el tráfico, pero a veces se siente raro sin las charlas en la oficina, ¿no? Herramientas como Zoom o Slack ayudan a mantenernos conectados, pero la clave está en organizarse bien. Intenta poner horarios fijos para trabajar y desconectarte, así no terminas respondiendo correos a medianoche. ¿Tú cómo lo llevas? ¡Cuéntame si tienes algún truco!\\n\\nSaludos,  \\n[Tu Nombre]\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Trabajo remoto y productividad\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "</ejemplos>\n",
    "\"\"\"\n",
    "  system_examples = EXAMPLES % BASE_EXAMPLE\n",
    "  system_content = f\"{INSTRUCTIONS}\\n{system_examples}\"\n",
    "  return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_content\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": fragment\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e408c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_genterated_conv(conversation:Conversation):\n",
    "  # Validar que la conversación tenga al menos un intercambio completo\n",
    "  if len(conversation.messages) < 2:\n",
    "      print(f\"Error: Conversación inválida, número de mensajes insuficiente: {len(conversation.messages)}\")\n",
    "      return None\n",
    "  # Si el número de mensajes es impar, eliminar el último (asumiendo que es del usuario)\n",
    "  if len(conversation.messages) % 2 != 0 and len(conversation.messages) > 2:\n",
    "      conversation.messages = conversation.messages[:-1]\n",
    "      \n",
    "  return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e353ea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import AsyncClient\n",
    "\n",
    "ollama_client = AsyncClient()\n",
    "\n",
    "async def generate_ollama_conversations(messages):\n",
    "    try:\n",
    "        # Llama a Ollama con roles system y user\n",
    "        response = await ollama_client.chat(\n",
    "            model=OLLAMA_MODEL,\n",
    "            messages=messages,\n",
    "            options={\n",
    "                \"temperature\": TEMPERATURE,\n",
    "                \"top_p\": TOP_P,\n",
    "                \n",
    "                \n",
    "            },\n",
    "            format=Conversation.model_json_schema()  # Especifica el esquema JSON\n",
    "            \n",
    "        )\n",
    "        conversation = Conversation.model_validate_json(response.message.content)\n",
    "\n",
    "        return validate_genterated_conv(conversation)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generando conversación con Ollama: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "16928054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncClient, RateLimitError\n",
    "\n",
    "openai_client = AsyncClient(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "async def generate_openai_conversations(messages, model=\"gpt-4o-mini\"):\n",
    "    try:\n",
    "        response = await openai_client.responses.parse(\n",
    "            model=model,\n",
    "            input=messages,\n",
    "            text_format=Conversation\n",
    "        )\n",
    "        conversation = response.output_parsed\n",
    "        return validate_genterated_conv(conversation) if conversation else None\n",
    "    except RateLimitError as e:\n",
    "        print(f\"Rate limit hit for {model}: {e}\")\n",
    "        next_model = \"gpt-5-nano\"\n",
    "        if next_model != model:\n",
    "            return await generate_openai_conversations(messages, next_model)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating conversation with OpenAI {model}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ab2c9167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "async def generate_conversation(fragment:str, provider: Literal[\"ollama\", \"openai\"]):\n",
    "    \"\"\"\n",
    "    Genera una conversación estructurada en formato JSON usando Ollama, basada en un fragmento de texto.\n",
    "    Optimizado para múltiples iteraciones en la generación de datasets para fine-tuning.\n",
    "    \"\"\"\n",
    "    if not fragment or len(fragment) < MIN_FRAGMENT_LENGTH:\n",
    "        print(f\"Error: Fragmento demasiado corto ({len(fragment)} caracteres).\")\n",
    "        return None\n",
    "\n",
    "    # Extraer el texto sin la metadata (líneas que comienzan con '#')\n",
    "    fragment_content = '\\n'.join(line for line in fragment.splitlines() if not line.startswith('#')).strip()\n",
    "    if len(fragment_content) < MIN_FRAGMENT_LENGTH:\n",
    "        print(f\"Error: Contenido útil del fragmento demasiado corto ({len(fragment_content)} caracteres).\")\n",
    "        return None\n",
    "\n",
    "    # Prompt optimizado para múltiples iteraciones\n",
    "    messages = get_prompts(fragment_content)\n",
    "\n",
    "    try:\n",
    "        if provider=='ollama':\n",
    "            return await generate_ollama_conversations(messages)\n",
    "        elif provider=='openai':\n",
    "            return await generate_openai_conversations(messages)\n",
    "        else: return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error generando conversación con Ollama: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "20c7088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configuración del logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('pdf_processing.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "43e9efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from filelock import FileLock\n",
    "import asyncio\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "OUTPUT_FOLDER = \"data\"\n",
    "\n",
    "class MetadataManager:\n",
    "    \"\"\"Clase para manejar archivos de metadatos JSON.\"\"\"\n",
    "    def __init__(self, index: int, output_dir: str):\n",
    "        self.folder = os.path.join(output_dir, \"metadata\")\n",
    "        self.metadata_file = os.path.join(self.folder, f\"metadata_{index:04d}.json\")\n",
    "        self.lock_file = f\"{self.metadata_file}.lock\"\n",
    "        self.index = index\n",
    "        os.makedirs(self.folder, exist_ok=True)\n",
    "\n",
    "    def exists(self) -> bool:\n",
    "        \"\"\"Verifica si el archivo de metadatos existe.\"\"\"\n",
    "        return os.path.exists(self.metadata_file)\n",
    "\n",
    "    def get(self, param: str):\n",
    "        \"\"\"Obtiene chunks, conversations, fileName, num_chunks, num_messages o num_exchanges desde el archivo de metadatos.\"\"\"\n",
    "        if not self.exists():\n",
    "            return [] if param in [\"chunks\", \"conversations\"] else 0 if param in [\"num_chunks\", \"num_messages\", \"num_exchanges\"] else \"\"\n",
    "        try:\n",
    "            with FileLock(self.lock_file):\n",
    "                with open(self.metadata_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    metadata = json.load(f)\n",
    "                \n",
    "                updated = False\n",
    "                if param == \"conversations\":\n",
    "                    conversations = metadata.get(\"conversations\", [])\n",
    "                    for i, conv in enumerate(conversations):\n",
    "                        if isinstance(conv, dict) and \"chunk_index\" not in conv:\n",
    "                            conv[\"chunk_index\"] = i\n",
    "                            updated = True\n",
    "                    conversations.sort(key=lambda x: x.get(\"chunk_index\", 0))\n",
    "                    metadata[\"conversations\"] = conversations\n",
    "                \n",
    "                if \"num_chunks\" not in metadata and \"chunks\" in metadata:\n",
    "                    metadata[\"num_chunks\"] = len(metadata.get(\"chunks\", []))\n",
    "                    updated = True\n",
    "                \n",
    "                if \"num_exchanges\" not in metadata and \"conversations\" in metadata:\n",
    "                    metadata[\"num_exchanges\"] = sum(len(conv[\"messages\"]) // 2 for conv in metadata.get(\"conversations\", []))\n",
    "                    updated = True\n",
    "                \n",
    "                if \"num_messages\" not in metadata and \"conversations\" in metadata:\n",
    "                    metadata[\"num_messages\"] = sum(len(conv[\"messages\"]) for conv in metadata.get(\"conversations\", []))\n",
    "                    updated = True\n",
    "                    \n",
    "                if \"total_conversations\" not in metadata and \"conversations\" in metadata:\n",
    "                    metadata[\"total_conversations\"] = len(conversations)\n",
    "                    updated = True\n",
    "                \n",
    "                if updated:\n",
    "                    with open(self.metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                        json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "                \n",
    "                return metadata.get(param, []) if param in [\"chunks\", \"conversations\"] else metadata.get(param, 0 if param in [\"num_chunks\", \"num_messages\", \"num_exchanges\"] else metadata.get(param,\"\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Error al leer {param} desde {self.metadata_file}: {e}\")\n",
    "            return [] if param in [\"chunks\", \"conversations\"] else 0 if param in [\"num_chunks\", \"num_messages\", \"num_exchanges\"] else \"\"\n",
    "\n",
    "    def set(self, param: str, value):\n",
    "        \"\"\"Establece chunks, conversations, fileName, num_chunks, num_messages o num_exchanges en el archivo de metadatos.\"\"\"\n",
    "        try:\n",
    "            with FileLock(self.lock_file):\n",
    "                metadata = {\"chunks\": [], \"fileName\": \"\", \"conversations\": [], \"num_chunks\": 0, \"num_messages\": 0, \"num_exchanges\": 0}\n",
    "                if self.exists():\n",
    "                    with open(self.metadata_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                        metadata = json.load(f)\n",
    "                \n",
    "                metadata[param] = value\n",
    "                if param == \"conversations\":\n",
    "                    metadata[\"conversations\"].sort(key=lambda x: x.get(\"chunk_index\", 0))\n",
    "                    metadata[\"num_messages\"] = sum(len(conv[\"messages\"]) for conv in metadata[\"conversations\"])\n",
    "                    metadata[\"num_exchanges\"] = sum(len(conv[\"messages\"]) // 2 for conv in metadata[\"conversations\"])\n",
    "                    metadata[\"total_conversations\"] = len(metadata[\"conversations\"])\n",
    "                if param == \"chunks\":\n",
    "                    metadata[\"num_chunks\"] = len(value)\n",
    "                with open(self.metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al escribir {param} en {self.metadata_file}: {e}\")\n",
    "\n",
    "    def append_conversation(self, conversation: dict):\n",
    "        \"\"\"Añade una conversación al array de conversaciones en el archivo de metadatos.\"\"\"\n",
    "        try:\n",
    "            with FileLock(self.lock_file):\n",
    "                metadata = {\"chunks\": [], \"fileName\": \"\", \"conversations\": [], \"num_chunks\": 0, \"num_messages\": 0, \"num_exchanges\": 0}\n",
    "                if self.exists():\n",
    "                    with open(self.metadata_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                        metadata = json.load(f)\n",
    "                \n",
    "                for i, conv in enumerate(metadata[\"conversations\"]):\n",
    "                    if isinstance(conv, dict) and \"chunk_index\" not in conv:\n",
    "                        conv[\"chunk_index\"] = i\n",
    "                \n",
    "                if \"num_chunks\" not in metadata:\n",
    "                    metadata[\"num_chunks\"] = len(metadata.get(\"chunks\", []))\n",
    "                \n",
    "                if \"num_exchanges\" not in metadata:\n",
    "                    metadata[\"num_exchanges\"] = sum(len(conv[\"messages\"]) // 2 for conv in metadata.get(\"conversations\", []))\n",
    "                \n",
    "                if \"num_messages\" not in metadata:\n",
    "                    metadata[\"num_messages\"] = sum(len(conv[\"messages\"]) for conv in metadata.get(\"conversations\", []))\n",
    "                \n",
    "                metadata[\"conversations\"].append(conversation)\n",
    "                metadata[\"conversations\"].sort(key=lambda x: x.get(\"chunk_index\", 0))\n",
    "                metadata[\"num_messages\"] = sum(len(conv[\"messages\"]) for conv in metadata[\"conversations\"])\n",
    "                metadata[\"num_exchanges\"] = sum(len(conv[\"messages\"]) // 2 for conv in metadata[\"conversations\"])\n",
    "                metadata[\"total_conversations\"] = len(metadata[\"conversations\"])\n",
    "                \n",
    "                with open(self.metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al añadir conversación a {self.metadata_file}: {e}\")\n",
    "\n",
    "def get_text_from_pdf(index: int, pdf_path: str, output_dir: str) -> list:\n",
    "    \"\"\"Obtiene el texto de un PDF desde la caché (JSON) o lo extrae si no existe.\"\"\"\n",
    "    metadata_manager = MetadataManager(index, output_dir)\n",
    "    \n",
    "    if metadata_manager.exists():\n",
    "        chunks = metadata_manager.get(\"chunks\")\n",
    "        file_name = metadata_manager.get(\"fileName\")\n",
    "        if chunks and file_name == os.path.basename(pdf_path):\n",
    "            return chunks\n",
    "    \n",
    "    try:\n",
    "        pages_text = extract_text_from_pdf(pdf_path)\n",
    "        if pages_text:\n",
    "            metadata_manager.set(\"chunks\", pages_text)\n",
    "            metadata_manager.set(\"fileName\", os.path.basename(pdf_path))\n",
    "            metadata_manager.set(\"conversations\", [])\n",
    "        return pages_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error al extraer texto del PDF {pdf_path}: {e}\")\n",
    "        return []\n",
    "    \n",
    "def get_conv_from_jsonl(index, output_dir):\n",
    "    metadata_manager = MetadataManager(index, output_dir)\n",
    "    jsonl_file = os.path.join(output_dir, OUTPUT_FOLDER, f\"pdf_{index:04d}.jsonl\")\n",
    "    if os.path.exists(jsonl_file):\n",
    "        try:\n",
    "            chunks = metadata_manager.get(\"chunks\")\n",
    "            with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                conversations = []\n",
    "                for i, line in enumerate(f):\n",
    "                    if line.strip():\n",
    "                        conv = json.loads(line)\n",
    "                        conv[\"source_chunk\"] = chunks[i]\n",
    "                        conv[\"chunk_index\"] = i\n",
    "                        conversations.append(conv)\n",
    "                conversations.sort(key=lambda x: x.get(\"chunk_index\", 0))\n",
    "                metadata_manager.set(\"conversations\", conversations)\n",
    "                metadata_manager.set(\"num_messages\", sum(len(conv[\"messages\"]) for conv in conversations))\n",
    "                metadata_manager.set(\"num_exchanges\", sum(len(conv[\"messages\"]) // 2 for conv in conversations))\n",
    "                metadata_manager.set(\"total_conversations\", len(conversations))\n",
    "                return conversations\n",
    "        except Exception as e:\n",
    "            print(f\"Error al leer conversaciones desde {jsonl_file}: {e}\")\n",
    "    return []\n",
    "\n",
    "async def get_conversation_from_chunk(index: int, output_dir: str, chunk: str, chunk_index: int, provider: Literal[\"ollama\", \"openai\"]):\n",
    "    \"\"\"Obtiene o genera una conversación para un fragmento de texto.\"\"\"\n",
    "    metadata_manager = MetadataManager(index, output_dir)\n",
    "    existing_conversations = metadata_manager.get(\"conversations\")\n",
    "    \n",
    "    if not existing_conversations:\n",
    "        existing_conversations = get_conv_from_jsonl(index, output_dir)\n",
    "    \n",
    "    for conv in existing_conversations:\n",
    "        if isinstance(conv, dict) and conv.get(\"source_chunk\") == chunk and conv.get(\"chunk_index\") == chunk_index:\n",
    "            return conv\n",
    "    try:\n",
    "        conversation = await generate_conversation(chunk, provider)\n",
    "        if conversation:\n",
    "            conv_dict = conversation.model_dump()\n",
    "            conv_dict[\"source_chunk\"] = chunk\n",
    "            conv_dict[\"chunk_index\"] = chunk_index\n",
    "            conv_dict[\"provider\"]= provider\n",
    "            from datetime import datetime\n",
    "            conv_dict[\"created_at\"] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            metadata_manager.append_conversation(conv_dict)\n",
    "            return conv_dict\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error al generar conversación para chunk_index {chunk_index} en PDF #{index} con {provider}: {e}\")\n",
    "        return None\n",
    "\n",
    "async def get_conv(index: int, output_dir: str, chunk: str, chunk_index: int, provider: Literal[\"ollama\", \"openai\"]):\n",
    "    result = await get_conversation_from_chunk(index, output_dir, chunk, chunk_index, provider)\n",
    "    return result if result is not None else []\n",
    "\n",
    "async def process_pdf(index: int, pdf_path: str, output_dir: str):\n",
    "    \"\"\"Procesa un PDF, genera conversaciones y las guarda en JSONL.\"\"\"\n",
    "    logger = logging.getLogger()\n",
    "    logger.info(f\"[PDF {index}] Procesando PDF: {Path(pdf_path).name}\")\n",
    "    try:\n",
    "        pages_text = get_text_from_pdf(index, pdf_path, output_dir)\n",
    "        \n",
    "        if not pages_text:\n",
    "            logger.warning(f\"[PDF {index}] No se encontraron fragmentos de texto\")\n",
    "            return\n",
    "        \n",
    "        data_dir = os.path.join(output_dir, OUTPUT_FOLDER)\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "        jsonl_file = os.path.join(data_dir, f\"pdf_{index:04d}.jsonl\")\n",
    "        \n",
    "        tasks = [(i, fragment) for i, fragment in enumerate(pages_text) if len(fragment) > 20]\n",
    "        if not tasks:\n",
    "            logger.warning(f\"[PDF {index}] No hay fragmentos válidos para procesar\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"[PDF {index}] Procesando {len(tasks)} fragmentos\")\n",
    "        \n",
    "        # Dividir tareas entre providers (50% openai, 50% ollama)\n",
    "        total_tasks = len(tasks)\n",
    "        half_tasks = total_tasks // 2\n",
    "        openai_tasks = [get_conv(index, output_dir, fragment, i, \"openai\") for i, fragment in tasks[:half_tasks]]\n",
    "        ollama_tasks = [get_conv(index, output_dir, fragment, i, \"ollama\") for i, fragment in tasks[half_tasks:]]\n",
    "        \n",
    "        # Ejecutar tareas en paralelo con límite de concurrencia\n",
    "        from asyncio import Semaphore\n",
    "        async def limited_gather(tasks, limit=10):\n",
    "            semaphore = Semaphore(limit)\n",
    "            async def sem_task(task):\n",
    "                async with semaphore:\n",
    "                    return await task\n",
    "            return await asyncio.gather(*[sem_task(task) for task in tasks], return_exceptions=True)\n",
    "        \n",
    "        task_results = await limited_gather(openai_tasks + ollama_tasks)\n",
    "        \n",
    "        if task_results:\n",
    "            valid_conversations = [conv for conv in task_results if conv is not None and conv is not TypeError]\n",
    "            if valid_conversations:\n",
    "                valid_conversations.sort(key=lambda x: x[\"chunk_index\"])\n",
    "                \n",
    "                actual_indices = [conv[\"chunk_index\"] for conv in valid_conversations]\n",
    "                if actual_indices != sorted(actual_indices):\n",
    "                    logger.warning(f\"[PDF {index}] El orden de las conversaciones no coincide con el esperado\")\n",
    "                \n",
    "                with open(jsonl_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    output_str = \"\\n\".join(json.dumps({\"messages\": conv[\"messages\"], \"topic\": conv[\"topic\"]}, ensure_ascii=False) \n",
    "                                        for conv in valid_conversations)\n",
    "                    f.write(output_str + \"\\n\")\n",
    "                    logger.info(f\"[PDF {index}] Dataset conversacional generado y guardado en JSONL\")\n",
    "            else:\n",
    "                logger.warning(f\"[PDF {index}] No se generaron conversaciones\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"[PDF {index}] Error al procesar el PDF: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2fdbb014-2ca2-4f7f-bdfe-1430423e7d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import asyncio\n",
    "import os\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "\n",
    "def process_pdf_wrapper(args):\n",
    "    index, pdf_path, output_dir = args\n",
    "    logger = logging.getLogger()\n",
    "    logger.info(f\"[PDF {index}] Inicio del procesamiento del PDF: {Path(pdf_path).name}\")\n",
    "    try:\n",
    "        asyncio.run(process_pdf(index, pdf_path, output_dir))\n",
    "        logger.info(f\"[PDF {index}] Procesamiento del PDF completado\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"[PDF {index}] Error durante el procesamiento: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def generate(pdf_files,max_workers=12):\n",
    "    output_dir = \"outputs_2\"\n",
    "    output_folder_path = Path(output_dir)\n",
    "    os.makedirs(output_folder_path, exist_ok=True)\n",
    "    \n",
    "    logger = logging.getLogger()\n",
    "    logger.info(f\"Generando Datasets - max_workers={max_workers}, # Files: {len(pdf_files)}\")\n",
    "    \n",
    "    # Dividir archivos en lotes para optimizar el uso de memoria\n",
    "    batch_size = max(1, math.ceil(len(pdf_files) / max_workers))\n",
    "    batches = [pdf_files[i:i + batch_size] for i in range(0, len(pdf_files), batch_size)]\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for batch_idx, batch in enumerate(batches):\n",
    "            logger.info(f\"Procesando lote {batch_idx + 1}/{len(batches)} con {len(batch)} archivos\")\n",
    "            tasks = [(i + batch_idx * batch_size, p, output_dir) for i, p in enumerate(batch)]\n",
    "            for _ in tqdm(\n",
    "                executor.map(process_pdf_wrapper, tasks),\n",
    "                total=len(batch),\n",
    "                desc=f\"Batch {batch_idx + 1}/{len(batches)}\"\n",
    "            ):\n",
    "                pass\n",
    "            logger.info(f\"Lote {batch_idx + 1}/{len(batches)} completado\")\n",
    "    logger.info(\"Generación de datasets finalizada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a3dadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:03,329 - INFO - Generando Datasets - max_workers=15, # Files: 89\n",
      "2025-08-12 14:02:03,333 - INFO - Procesando lote 1/15 con 6 archivos\n",
      "Batch 1/15:   0%|          | 0/6 [00:00<?, ?it/s]2025-08-12 14:02:03,452 - INFO - [PDF 0] Inicio del procesamiento del PDF: L1_XVII_cap_IV.pdf\n",
      "2025-08-12 14:02:03,452 - INFO - [PDF 1] Inicio del procesamiento del PDF: L1_IX_cap_III.pdf\n",
      "2025-08-12 14:02:03,453 - INFO - [PDF 2] Inicio del procesamiento del PDF: manual-para-la-gestion-de-riesgos-lavados-de-activos-y-financiacion-del-terrorismo (1).pdf\n",
      "2025-08-12 14:02:03,453 - INFO - [PDF 3] Inicio del procesamiento del PDF: L1_XVII_cap_III.pdf\n",
      "2025-08-12 14:02:03,453 - INFO - [PDF 4] Inicio del procesamiento del PDF: Modelo_de_Administracion_del_Riesgo_de_LAFT_y_Contrabando_web.pdf\n",
      "2025-08-12 14:02:03,454 - INFO - [PDF 5] Inicio del procesamiento del PDF: manual-para-la-gestion-de-riesgos-lavados-de-activos-y-financiacion-del-terrorismo.pdf\n",
      "2025-08-12 14:02:03,458 - INFO - [PDF 0] Procesando PDF: L1_XVII_cap_IV.pdf\n",
      "2025-08-12 14:02:03,459 - INFO - [PDF 1] Procesando PDF: L1_IX_cap_III.pdf\n",
      "2025-08-12 14:02:03,459 - INFO - [PDF 2] Procesando PDF: manual-para-la-gestion-de-riesgos-lavados-de-activos-y-financiacion-del-terrorismo (1).pdf\n",
      "2025-08-12 14:02:03,459 - INFO - [PDF 3] Procesando PDF: L1_XVII_cap_III.pdf\n",
      "2025-08-12 14:02:03,460 - INFO - [PDF 4] Procesando PDF: Modelo_de_Administracion_del_Riesgo_de_LAFT_y_Contrabando_web.pdf\n",
      "2025-08-12 14:02:03,460 - INFO - [PDF 5] Procesando PDF: manual-para-la-gestion-de-riesgos-lavados-de-activos-y-financiacion-del-terrorismo.pdf\n",
      "2025-08-12 14:02:03,470 - INFO - [PDF 2] Procesando 53 fragmentos\n",
      "2025-08-12 14:02:03,473 - INFO - [PDF 5] Procesando 53 fragmentos\n",
      "2025-08-12 14:02:03,477 - INFO - [PDF 3] Procesando 90 fragmentos\n",
      "2025-08-12 14:02:03,477 - INFO - [PDF 1] Procesando 104 fragmentos\n",
      "2025-08-12 14:02:03,480 - INFO - [PDF 0] Procesando 145 fragmentos\n",
      "2025-08-12 14:02:03,489 - INFO - [PDF 4] Procesando 592 fragmentos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:10,659 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:10,662 - INFO - Retrying request to /responses in 0.445769 seconds\n",
      "2025-08-12 14:02:10,921 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:10,924 - INFO - Retrying request to /responses in 0.448642 seconds\n",
      "2025-08-12 14:02:11,469 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:11,472 - INFO - Retrying request to /responses in 0.393622 seconds\n",
      "2025-08-12 14:02:12,767 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:12,771 - INFO - Retrying request to /responses in 0.398085 seconds\n",
      "2025-08-12 14:02:14,682 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:16,450 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:16,454 - INFO - Retrying request to /responses in 0.768227 seconds\n",
      "2025-08-12 14:02:16,944 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:16,953 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:16,956 - INFO - Retrying request to /responses in 0.909009 seconds\n",
      "2025-08-12 14:02:17,504 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:17,738 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:17,741 - INFO - Retrying request to /responses in 0.861151 seconds\n",
      "2025-08-12 14:02:18,027 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:18,714 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:19,341 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:19,345 - INFO - Retrying request to /responses in 0.824949 seconds\n",
      "2025-08-12 14:02:20,074 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:20,286 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:20,796 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:20,800 - INFO - Retrying request to /responses in 0.377410 seconds\n",
      "2025-08-12 14:02:21,678 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:22,386 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:22,389 - INFO - Retrying request to /responses in 0.476593 seconds\n",
      "2025-08-12 14:02:23,031 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:23,034 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5712. Please try again in 1.713s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:23,051 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:23,134 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:23,296 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:23,664 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:23,668 - INFO - Retrying request to /responses in 0.384902 seconds\n",
      "2025-08-12 14:02:23,778 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5741. Please try again in 1.722s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:23,821 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:23,917 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:23,922 - INFO - Retrying request to /responses in 0.442163 seconds\n",
      "2025-08-12 14:02:23,933 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:23,934 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:23,937 - INFO - Retrying request to /responses in 0.417644 seconds\n",
      "2025-08-12 14:02:24,119 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5860. Please try again in 1.758s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:24,169 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:24,335 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:24,360 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:24,698 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:25,259 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5706. Please try again in 1.711s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:25,270 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:25,279 - INFO - Retrying request to /responses in 0.391561 seconds\n",
      "2025-08-12 14:02:25,576 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:25,580 - INFO - Retrying request to /responses in 0.384991 seconds\n",
      "2025-08-12 14:02:25,938 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:25,941 - INFO - Retrying request to /responses in 0.993707 seconds\n",
      "2025-08-12 14:02:26,367 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:26,412 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:26,415 - INFO - Retrying request to /responses in 0.390776 seconds\n",
      "2025-08-12 14:02:26,641 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:27,018 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:27,501 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:27,508 - INFO - Retrying request to /responses in 0.770208 seconds\n",
      "2025-08-12 14:02:27,686 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:27,689 - INFO - Retrying request to /responses in 0.469087 seconds\n",
      "2025-08-12 14:02:27,876 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:28,155 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:28,416 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:28,419 - INFO - Retrying request to /responses in 0.493819 seconds\n",
      "2025-08-12 14:02:28,531 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:28,535 - INFO - Retrying request to /responses in 0.463468 seconds\n",
      "2025-08-12 14:02:28,881 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:28,885 - INFO - Retrying request to /responses in 0.381801 seconds\n",
      "2025-08-12 14:02:28,937 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:28,941 - INFO - Retrying request to /responses in 0.475724 seconds\n",
      "2025-08-12 14:02:29,056 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:29,059 - INFO - Retrying request to /responses in 0.876408 seconds\n",
      "2025-08-12 14:02:29,096 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:29,216 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:29,220 - INFO - Retrying request to /responses in 0.808313 seconds\n",
      "2025-08-12 14:02:29,235 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:29,238 - INFO - Retrying request to /responses in 0.393716 seconds\n",
      "2025-08-12 14:02:29,336 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:29,479 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:29,483 - INFO - Retrying request to /responses in 0.966432 seconds\n",
      "2025-08-12 14:02:29,512 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:29,515 - INFO - Retrying request to /responses in 0.448492 seconds\n",
      "2025-08-12 14:02:29,836 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:29,839 - INFO - Retrying request to /responses in 0.469872 seconds\n",
      "2025-08-12 14:02:29,840 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:29,989 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:30,126 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:30,129 - INFO - Retrying request to /responses in 0.907473 seconds\n",
      "2025-08-12 14:02:30,309 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:30,479 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:30,570 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:30,705 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:30,710 - INFO - Retrying request to /responses in 0.388238 seconds\n",
      "2025-08-12 14:02:30,911 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:31,034 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:31,046 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:31,049 - INFO - Retrying request to /responses in 0.862091 seconds\n",
      "2025-08-12 14:02:31,113 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:31,116 - INFO - Retrying request to /responses in 0.394284 seconds\n",
      "2025-08-12 14:02:31,401 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:31,410 - INFO - Retrying request to /responses in 0.408101 seconds\n",
      "2025-08-12 14:02:31,552 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:31,710 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:31,776 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:31,955 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:32,188 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:32,512 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:32,515 - INFO - Retrying request to /responses in 0.793743 seconds\n",
      "2025-08-12 14:02:32,677 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5745. Please try again in 1.723s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:32,702 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:32,752 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:32,755 - INFO - Retrying request to /responses in 0.441465 seconds\n",
      "2025-08-12 14:02:32,840 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5734. Please try again in 1.72s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:32,862 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:32,865 - INFO - Retrying request to /responses in 0.453944 seconds\n",
      "2025-08-12 14:02:33,177 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:33,181 - INFO - Retrying request to /responses in 0.420600 seconds\n",
      "2025-08-12 14:02:33,207 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:33,210 - INFO - Retrying request to /responses in 0.463917 seconds\n",
      "2025-08-12 14:02:33,249 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:33,252 - INFO - Retrying request to /responses in 0.751284 seconds\n",
      "2025-08-12 14:02:33,820 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:33,854 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:33,855 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:33,857 - INFO - Retrying request to /responses in 0.383856 seconds\n",
      "2025-08-12 14:02:33,965 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:33,968 - INFO - Retrying request to /responses in 0.866114 seconds\n",
      "2025-08-12 14:02:34,356 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:34,362 - INFO - Retrying request to /responses in 0.830886 seconds\n",
      "2025-08-12 14:02:34,576 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:34,579 - INFO - Retrying request to /responses in 0.409772 seconds\n",
      "2025-08-12 14:02:34,673 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:34,862 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5714. Please try again in 1.714s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:35,238 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:35,241 - INFO - Retrying request to /responses in 0.940524 seconds\n",
      "2025-08-12 14:02:35,314 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5797. Please try again in 1.739s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:35,318 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5718. Please try again in 1.715s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:35,412 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:35,416 - INFO - Retrying request to /responses in 0.888577 seconds\n",
      "2025-08-12 14:02:35,472 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:35,475 - INFO - Retrying request to /responses in 0.488905 seconds\n",
      "2025-08-12 14:02:35,579 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:35,583 - INFO - Retrying request to /responses in 0.845059 seconds\n",
      "2025-08-12 14:02:35,609 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:35,612 - INFO - Retrying request to /responses in 0.974531 seconds\n",
      "2025-08-12 14:02:35,731 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:35,770 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:36,132 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:36,136 - INFO - Retrying request to /responses in 0.444466 seconds\n",
      "2025-08-12 14:02:36,184 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:36,186 - INFO - Retrying request to /responses in 0.465231 seconds\n",
      "2025-08-12 14:02:36,190 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:36,203 - INFO - Retrying request to /responses in 0.422707 seconds\n",
      "2025-08-12 14:02:36,265 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5776. Please try again in 1.732s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:36,320 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:36,324 - INFO - Retrying request to /responses in 0.853759 seconds\n",
      "2025-08-12 14:02:36,513 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:36,517 - INFO - Retrying request to /responses in 0.786122 seconds\n",
      "2025-08-12 14:02:36,711 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:36,714 - INFO - Retrying request to /responses in 0.382909 seconds\n",
      "2025-08-12 14:02:36,755 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:36,758 - INFO - Retrying request to /responses in 0.470379 seconds\n",
      "2025-08-12 14:02:36,776 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:36,925 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:36,933 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:36,951 - INFO - Retrying request to /responses in 0.390172 seconds\n",
      "2025-08-12 14:02:36,985 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:37,333 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:37,336 - INFO - Retrying request to /responses in 0.838601 seconds\n",
      "2025-08-12 14:02:37,439 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:37,443 - INFO - Retrying request to /responses in 0.456116 seconds\n",
      "2025-08-12 14:02:37,589 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:37,595 - INFO - Retrying request to /responses in 0.476621 seconds\n",
      "2025-08-12 14:02:37,750 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:37,805 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:37,809 - INFO - Retrying request to /responses in 0.475818 seconds\n",
      "2025-08-12 14:02:37,901 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:37,908 - INFO - Retrying request to /responses in 0.396523 seconds\n",
      "2025-08-12 14:02:37,984 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:37,987 - INFO - Retrying request to /responses in 0.777479 seconds\n",
      "2025-08-12 14:02:38,159 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:38,163 - INFO - Retrying request to /responses in 0.486202 seconds\n",
      "2025-08-12 14:02:38,309 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:38,313 - INFO - Retrying request to /responses in 0.963773 seconds\n",
      "2025-08-12 14:02:38,316 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5717. Please try again in 1.715s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:38,531 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:38,534 - INFO - Retrying request to /responses in 0.379177 seconds\n",
      "2025-08-12 14:02:38,595 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:38,598 - INFO - Retrying request to /responses in 0.479565 seconds\n",
      "2025-08-12 14:02:38,607 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:38,610 - INFO - Retrying request to /responses in 0.904396 seconds\n",
      "2025-08-12 14:02:38,656 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:38,660 - INFO - Retrying request to /responses in 0.421664 seconds\n",
      "2025-08-12 14:02:38,805 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5718. Please try again in 1.715s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:38,847 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5708. Please try again in 1.712s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:39,089 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:39,098 - INFO - Retrying request to /responses in 0.977493 seconds\n",
      "2025-08-12 14:02:39,264 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:39,321 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:39,324 - INFO - Retrying request to /responses in 0.433413 seconds\n",
      "2025-08-12 14:02:39,683 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:39,687 - INFO - Retrying request to /responses in 0.752757 seconds\n",
      "2025-08-12 14:02:39,863 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:39,866 - INFO - Retrying request to /responses in 0.954505 seconds\n",
      "2025-08-12 14:02:40,440 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:40,443 - INFO - Retrying request to /responses in 0.426020 seconds\n",
      "2025-08-12 14:02:40,454 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:40,525 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5728. Please try again in 1.718s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:40,662 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:40,665 - INFO - Retrying request to /responses in 0.777375 seconds\n",
      "2025-08-12 14:02:40,790 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5763. Please try again in 1.728s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:41,122 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5725. Please try again in 1.717s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:41,219 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:41,234 - INFO - Retrying request to /responses in 0.845266 seconds\n",
      "2025-08-12 14:02:41,484 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:41,488 - INFO - Retrying request to /responses in 0.872393 seconds\n",
      "2025-08-12 14:02:41,575 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:41,627 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5707. Please try again in 1.712s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:41,686 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:41,690 - INFO - Retrying request to /responses in 0.417379 seconds\n",
      "2025-08-12 14:02:41,793 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:41,808 - INFO - Retrying request to /responses in 0.904520 seconds\n",
      "2025-08-12 14:02:42,072 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:42,076 - INFO - Retrying request to /responses in 0.484618 seconds\n",
      "2025-08-12 14:02:42,080 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5720. Please try again in 1.716s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:42,104 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:42,109 - INFO - Retrying request to /responses in 0.776745 seconds\n",
      "2025-08-12 14:02:42,176 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:42,179 - INFO - Retrying request to /responses in 0.473145 seconds\n",
      "2025-08-12 14:02:42,180 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:42,183 - INFO - Retrying request to /responses in 0.479418 seconds\n",
      "2025-08-12 14:02:42,213 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:42,412 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5721. Please try again in 1.716s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:42,484 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:42,489 - INFO - Retrying request to /responses in 0.883513 seconds\n",
      "2025-08-12 14:02:42,570 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5707. Please try again in 1.712s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:42,577 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:42,580 - INFO - Retrying request to /responses in 0.898465 seconds\n",
      "2025-08-12 14:02:42,668 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:42,672 - INFO - Retrying request to /responses in 0.767972 seconds\n",
      "2025-08-12 14:02:42,949 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:42,952 - INFO - Retrying request to /responses in 0.904161 seconds\n",
      "2025-08-12 14:02:43,375 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5711. Please try again in 1.713s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:43,390 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:43,418 - INFO - Retrying request to /responses in 0.755265 seconds\n",
      "2025-08-12 14:02:43,500 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:43,510 - INFO - Retrying request to /responses in 0.795935 seconds\n",
      "2025-08-12 14:02:43,519 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:43,521 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:43,522 - INFO - Retrying request to /responses in 0.375647 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5707. Please try again in 1.712s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:43,688 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:43,778 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:43,837 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:44,094 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:44,098 - INFO - Retrying request to /responses in 0.823269 seconds\n",
      "2025-08-12 14:02:44,118 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:44,127 - INFO - Retrying request to /responses in 0.977040 seconds\n",
      "2025-08-12 14:02:44,185 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:44,298 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5780. Please try again in 1.734s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:44,360 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:44,364 - INFO - Retrying request to /responses in 0.865438 seconds\n",
      "2025-08-12 14:02:44,406 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:44,410 - INFO - Retrying request to /responses in 0.809024 seconds\n",
      "2025-08-12 14:02:44,466 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:44,470 - INFO - Retrying request to /responses in 0.875263 seconds\n",
      "2025-08-12 14:02:44,690 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5711. Please try again in 1.713s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:44,762 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5754. Please try again in 1.726s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:44,841 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:44,846 - INFO - Retrying request to /responses in 0.438779 seconds\n",
      "2025-08-12 14:02:45,003 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:45,010 - INFO - Retrying request to /responses in 0.446745 seconds\n",
      "2025-08-12 14:02:45,279 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:45,282 - INFO - Retrying request to /responses in 0.842476 seconds\n",
      "2025-08-12 14:02:45,316 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5745. Please try again in 1.723s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:45,592 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:45,959 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:45,963 - INFO - Retrying request to /responses in 0.403891 seconds\n",
      "2025-08-12 14:02:46,125 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:46,129 - INFO - Retrying request to /responses in 0.925202 seconds\n",
      "2025-08-12 14:02:46,234 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:46,260 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5763. Please try again in 1.728s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:46,390 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5844. Please try again in 1.753s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:46,453 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 6136. Please try again in 1.84s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:46,832 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:46,836 - INFO - Retrying request to /responses in 0.396848 seconds\n",
      "2025-08-12 14:02:47,439 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:47,443 - INFO - Retrying request to /responses in 0.811717 seconds\n",
      "2025-08-12 14:02:47,472 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5740. Please try again in 1.722s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:47,557 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5737. Please try again in 1.721s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:47,648 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:47,652 - INFO - Retrying request to /responses in 0.866269 seconds\n",
      "2025-08-12 14:02:47,844 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5721. Please try again in 1.716s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:47,998 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5747. Please try again in 1.724s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:48,018 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:48,018 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:48,022 - INFO - Retrying request to /responses in 0.986091 seconds\n",
      "2025-08-12 14:02:48,022 - INFO - Retrying request to /responses in 0.930990 seconds\n",
      "2025-08-12 14:02:48,081 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5708. Please try again in 1.712s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:48,572 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5774. Please try again in 1.732s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:48,830 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:48,833 - INFO - Retrying request to /responses in 0.375885 seconds\n",
      "2025-08-12 14:02:48,972 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5739. Please try again in 1.721s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:49,232 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5729. Please try again in 1.718s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:49,361 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5731. Please try again in 1.719s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:49,432 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:49,435 - INFO - Retrying request to /responses in 0.461009 seconds\n",
      "2025-08-12 14:02:49,445 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5721. Please try again in 1.716s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:49,531 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:49,534 - INFO - Retrying request to /responses in 0.992408 seconds\n",
      "2025-08-12 14:02:49,639 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5817. Please try again in 1.745s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:49,987 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:49,990 - INFO - Retrying request to /responses in 0.464714 seconds\n",
      "2025-08-12 14:02:50,138 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:50,142 - INFO - Retrying request to /responses in 0.493949 seconds\n",
      "2025-08-12 14:02:50,264 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:50,267 - INFO - Retrying request to /responses in 0.830185 seconds\n",
      "2025-08-12 14:02:50,605 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:50,609 - INFO - Retrying request to /responses in 0.777575 seconds\n",
      "2025-08-12 14:02:50,793 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5734. Please try again in 1.72s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:50,889 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5771. Please try again in 1.731s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:50,910 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:50,913 - INFO - Retrying request to /responses in 0.379736 seconds\n",
      "2025-08-12 14:02:51,025 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5737. Please try again in 1.721s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:51,352 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:51,356 - INFO - Retrying request to /responses in 0.861193 seconds\n",
      "2025-08-12 14:02:51,455 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:51,619 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5759. Please try again in 1.727s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:51,663 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:51,711 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5715. Please try again in 1.714s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:51,865 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5745. Please try again in 1.723s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:51,992 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:51,996 - INFO - Retrying request to /responses in 0.485230 seconds\n",
      "2025-08-12 14:02:52,257 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:02:52,750 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:52,754 - INFO - Retrying request to /responses in 0.807978 seconds\n",
      "2025-08-12 14:02:53,355 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5733. Please try again in 1.719s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:53,876 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5736. Please try again in 1.72s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:54,287 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:54,291 - INFO - Retrying request to /responses in 0.952342 seconds\n",
      "2025-08-12 14:02:54,853 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5784. Please try again in 1.735s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:54,897 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:54,900 - INFO - Retrying request to /responses in 0.799685 seconds\n",
      "2025-08-12 14:02:54,948 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:54,951 - INFO - Retrying request to /responses in 0.995357 seconds\n",
      "2025-08-12 14:02:55,242 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:55,246 - INFO - Retrying request to /responses in 0.959849 seconds\n",
      "2025-08-12 14:02:55,819 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5720. Please try again in 1.716s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:56,407 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:56,411 - INFO - Retrying request to /responses in 0.420792 seconds\n",
      "2025-08-12 14:02:56,770 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5719. Please try again in 1.715s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:56,809 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5697. Please try again in 1.709s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:57,026 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:57,030 - INFO - Retrying request to /responses in 0.466794 seconds\n",
      "2025-08-12 14:02:57,165 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5817. Please try again in 1.745s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:57,296 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5726. Please try again in 1.717s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:57,624 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:57,628 - INFO - Retrying request to /responses in 0.768460 seconds\n",
      "2025-08-12 14:02:57,963 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:57,967 - INFO - Retrying request to /responses in 0.440961 seconds\n",
      "2025-08-12 14:02:58,374 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:58,378 - INFO - Retrying request to /responses in 0.457872 seconds\n",
      "2025-08-12 14:02:58,686 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:58,690 - INFO - Retrying request to /responses in 0.825202 seconds\n",
      "2025-08-12 14:02:59,154 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 198169, Requested 5777. Please try again in 1.183s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:02:59,601 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:02:59,609 - INFO - Retrying request to /responses in 0.483480 seconds\n",
      "2025-08-12 14:03:00,692 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5741. Please try again in 1.722s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:00,885 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:01,366 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:01,831 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:01,835 - INFO - Retrying request to /responses in 0.487271 seconds\n",
      "2025-08-12 14:03:01,850 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:01,854 - INFO - Retrying request to /responses in 0.413454 seconds\n",
      "2025-08-12 14:03:02,322 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5720. Please try again in 1.716s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:02,964 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:03,023 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:03,027 - INFO - Retrying request to /responses in 0.955342 seconds\n",
      "2025-08-12 14:03:03,192 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:03,196 - INFO - Retrying request to /responses in 0.781117 seconds\n",
      "2025-08-12 14:03:03,250 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:03,254 - INFO - Retrying request to /responses in 0.474783 seconds\n",
      "2025-08-12 14:03:03,341 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5719. Please try again in 1.715s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:03,649 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:03,653 - INFO - Retrying request to /responses in 0.894775 seconds\n",
      "2025-08-12 14:03:03,738 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:03,741 - INFO - Retrying request to /responses in 0.975827 seconds\n",
      "2025-08-12 14:03:04,052 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:04,469 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:04,933 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:05,297 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5707. Please try again in 1.712s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:05,550 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:05,553 - INFO - Retrying request to /responses in 0.434491 seconds\n",
      "2025-08-12 14:03:06,378 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:06,382 - INFO - Retrying request to /responses in 0.841309 seconds\n",
      "2025-08-12 14:03:06,416 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:06,419 - INFO - Retrying request to /responses in 0.384549 seconds\n",
      "2025-08-12 14:03:06,699 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:06,717 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:06,887 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:06,891 - INFO - Retrying request to /responses in 0.461692 seconds\n",
      "2025-08-12 14:03:07,006 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:07,250 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:07,254 - INFO - Retrying request to /responses in 0.417567 seconds\n",
      "2025-08-12 14:03:07,295 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:07,299 - INFO - Retrying request to /responses in 0.459203 seconds\n",
      "2025-08-12 14:03:07,441 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:07,894 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:07,918 - INFO - Retrying request to /responses in 0.781843 seconds\n",
      "2025-08-12 14:03:08,239 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:08,243 - INFO - Retrying request to /responses in 0.474131 seconds\n",
      "2025-08-12 14:03:08,286 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:08,290 - INFO - Retrying request to /responses in 0.889110 seconds\n",
      "2025-08-12 14:03:08,608 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:08,839 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:08,892 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:08,896 - INFO - Retrying request to /responses in 0.458341 seconds\n",
      "2025-08-12 14:03:09,401 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:09,410 - INFO - Retrying request to /responses in 0.490746 seconds\n",
      "2025-08-12 14:03:09,440 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:09,444 - INFO - Retrying request to /responses in 0.919638 seconds\n",
      "2025-08-12 14:03:09,934 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:10,277 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5798. Please try again in 1.739s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:10,329 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5726. Please try again in 1.717s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:10,434 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5932. Please try again in 1.779s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:10,612 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:10,620 - INFO - Retrying request to /responses in 0.433609 seconds\n",
      "2025-08-12 14:03:10,725 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:10,728 - INFO - Retrying request to /responses in 0.780811 seconds\n",
      "2025-08-12 14:03:10,871 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:10,874 - INFO - Retrying request to /responses in 0.493795 seconds\n",
      "2025-08-12 14:03:11,130 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:11,717 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:11,721 - INFO - Retrying request to /responses in 0.459639 seconds\n",
      "2025-08-12 14:03:11,894 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5929. Please try again in 1.778s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:12,486 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:12,490 - INFO - Retrying request to /responses in 0.774128 seconds\n",
      "2025-08-12 14:03:12,499 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:12,567 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:12,571 - INFO - Retrying request to /responses in 0.494621 seconds\n",
      "2025-08-12 14:03:12,740 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:12,744 - INFO - Retrying request to /responses in 0.436912 seconds\n",
      "2025-08-12 14:03:12,813 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:12,817 - INFO - Retrying request to /responses in 0.853680 seconds\n",
      "2025-08-12 14:03:13,171 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:13,216 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:13,220 - INFO - Retrying request to /responses in 0.398532 seconds\n",
      "2025-08-12 14:03:13,281 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:13,356 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:13,360 - INFO - Retrying request to /responses in 0.917026 seconds\n",
      "2025-08-12 14:03:13,454 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:13,457 - INFO - Retrying request to /responses in 0.750162 seconds\n",
      "2025-08-12 14:03:14,372 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5916. Please try again in 1.774s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:14,491 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:14,501 - INFO - Retrying request to /responses in 0.755983 seconds\n",
      "2025-08-12 14:03:14,504 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:14,511 - INFO - Retrying request to /responses in 0.836820 seconds\n",
      "2025-08-12 14:03:14,826 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 6013. Please try again in 1.803s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:14,965 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:14,968 - INFO - Retrying request to /responses in 0.460938 seconds\n",
      "2025-08-12 14:03:15,122 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:15,126 - INFO - Retrying request to /responses in 0.497462 seconds\n",
      "2025-08-12 14:03:15,290 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:15,331 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:15,338 - INFO - Retrying request to /responses in 0.963385 seconds\n",
      "2025-08-12 14:03:15,382 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:15,621 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:15,624 - INFO - Retrying request to /responses in 0.411753 seconds\n",
      "2025-08-12 14:03:16,192 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:16,194 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5973. Please try again in 1.791s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:16,203 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:16,209 - INFO - Retrying request to /responses in 0.490662 seconds\n",
      "2025-08-12 14:03:16,310 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:16,314 - INFO - Retrying request to /responses in 0.415363 seconds\n",
      "2025-08-12 14:03:16,385 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:16,388 - INFO - Retrying request to /responses in 0.825788 seconds\n",
      "2025-08-12 14:03:16,444 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5922. Please try again in 1.776s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:16,463 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:16,805 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:16,809 - INFO - Retrying request to /responses in 0.809447 seconds\n",
      "2025-08-12 14:03:17,169 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:17,173 - INFO - Retrying request to /responses in 0.483821 seconds\n",
      "2025-08-12 14:03:17,523 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:17,527 - INFO - Retrying request to /responses in 0.402598 seconds\n",
      "2025-08-12 14:03:17,667 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:17,675 - INFO - Retrying request to /responses in 0.856195 seconds\n",
      "2025-08-12 14:03:17,874 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:17,877 - INFO - Retrying request to /responses in 0.461235 seconds\n",
      "2025-08-12 14:03:17,925 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:18,060 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:18,119 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:18,168 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:18,170 - INFO - Retrying request to /responses in 0.442037 seconds\n",
      "2025-08-12 14:03:18,341 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5937. Please try again in 1.781s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:18,633 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:18,637 - INFO - Retrying request to /responses in 0.388747 seconds\n",
      "2025-08-12 14:03:18,723 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:18,726 - INFO - Retrying request to /responses in 0.955049 seconds\n",
      "2025-08-12 14:03:18,799 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:18,809 - INFO - Retrying request to /responses in 0.970180 seconds\n",
      "2025-08-12 14:03:18,846 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:18,982 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5715. Please try again in 1.714s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:19,923 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5916. Please try again in 1.774s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:20,190 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:20,219 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:20,224 - INFO - Retrying request to /responses in 0.979082 seconds\n",
      "2025-08-12 14:03:20,297 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:20,301 - INFO - Retrying request to /responses in 0.435830 seconds\n",
      "2025-08-12 14:03:20,374 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:20,377 - INFO - Retrying request to /responses in 0.794149 seconds\n",
      "2025-08-12 14:03:20,810 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:20,814 - INFO - Retrying request to /responses in 0.383919 seconds\n",
      "2025-08-12 14:03:20,867 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:20,868 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5915. Please try again in 1.774s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:20,879 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5722. Please try again in 1.716s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:21,598 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:21,620 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5786. Please try again in 1.735s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:21,668 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:21,924 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:21,928 - INFO - Retrying request to /responses in 0.852219 seconds\n",
      "2025-08-12 14:03:22,039 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:22,043 - INFO - Retrying request to /responses in 0.469324 seconds\n",
      "2025-08-12 14:03:22,126 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:22,130 - INFO - Retrying request to /responses in 0.883112 seconds\n",
      "2025-08-12 14:03:22,522 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:22,526 - INFO - Retrying request to /responses in 0.453131 seconds\n",
      "2025-08-12 14:03:22,769 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:22,773 - INFO - Retrying request to /responses in 0.982257 seconds\n",
      "2025-08-12 14:03:22,826 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5903. Please try again in 1.77s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:23,222 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:23,226 - INFO - Retrying request to /responses in 0.965538 seconds\n",
      "2025-08-12 14:03:23,370 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:23,374 - INFO - Retrying request to /responses in 0.975176 seconds\n",
      "2025-08-12 14:03:23,535 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:23,677 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:23,743 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:23,747 - INFO - Retrying request to /responses in 0.461105 seconds\n",
      "2025-08-12 14:03:23,961 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:23,965 - INFO - Retrying request to /responses in 0.414580 seconds\n",
      "2025-08-12 14:03:24,005 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:24,010 - INFO - Retrying request to /responses in 0.959921 seconds\n",
      "2025-08-12 14:03:24,075 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5712. Please try again in 1.713s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:24,193 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:24,197 - INFO - Retrying request to /responses in 0.776067 seconds\n",
      "2025-08-12 14:03:24,303 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:24,304 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:24,309 - INFO - Retrying request to /responses in 0.881955 seconds\n",
      "2025-08-12 14:03:24,309 - INFO - Retrying request to /responses in 0.427567 seconds\n",
      "2025-08-12 14:03:24,338 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:24,341 - INFO - Retrying request to /responses in 0.434220 seconds\n",
      "2025-08-12 14:03:25,323 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:25,448 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5733. Please try again in 1.719s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:25,600 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:25,609 - INFO - Retrying request to /responses in 0.872616 seconds\n",
      "2025-08-12 14:03:25,714 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:25,718 - INFO - Retrying request to /responses in 0.393376 seconds\n",
      "2025-08-12 14:03:26,470 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:26,533 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:26,536 - INFO - Retrying request to /responses in 0.443029 seconds\n",
      "2025-08-12 14:03:26,781 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5723. Please try again in 1.716s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:26,882 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:26,885 - INFO - Retrying request to /responses in 0.402903 seconds\n",
      "2025-08-12 14:03:26,892 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5713. Please try again in 1.713s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:26,931 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:26,934 - INFO - Retrying request to /responses in 0.468382 seconds\n",
      "2025-08-12 14:03:27,136 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:27,139 - INFO - Retrying request to /responses in 0.756751 seconds\n",
      "2025-08-12 14:03:27,147 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:27,363 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5712. Please try again in 1.713s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:27,525 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:27,528 - INFO - Retrying request to /responses in 0.965870 seconds\n",
      "2025-08-12 14:03:27,620 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:27,623 - INFO - Retrying request to /responses in 0.477402 seconds\n",
      "2025-08-12 14:03:27,730 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:28,029 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5714. Please try again in 1.714s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:28,445 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:28,634 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:28,638 - INFO - Retrying request to /responses in 0.415358 seconds\n",
      "2025-08-12 14:03:28,695 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5922. Please try again in 1.776s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:29,007 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:29,011 - INFO - Retrying request to /responses in 0.949325 seconds\n",
      "2025-08-12 14:03:29,102 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:29,216 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:29,222 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:29,234 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:29,238 - INFO - Retrying request to /responses in 0.423628 seconds\n",
      "2025-08-12 14:03:29,326 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:29,331 - INFO - Retrying request to /responses in 0.925587 seconds\n",
      "2025-08-12 14:03:29,626 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5710. Please try again in 1.713s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:29,823 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:29,826 - INFO - Retrying request to /responses in 0.392001 seconds\n",
      "2025-08-12 14:03:29,849 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:30,074 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5755. Please try again in 1.726s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:30,103 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5994. Please try again in 1.798s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:30,219 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:30,223 - INFO - Retrying request to /responses in 0.977617 seconds\n",
      "2025-08-12 14:03:30,257 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:30,261 - INFO - Retrying request to /responses in 0.856258 seconds\n",
      "2025-08-12 14:03:30,484 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5714. Please try again in 1.714s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:30,850 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:30,922 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:30,997 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:31,000 - INFO - Retrying request to /responses in 0.498834 seconds\n",
      "2025-08-12 14:03:31,046 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:31,049 - INFO - Retrying request to /responses in 0.875829 seconds\n",
      "2025-08-12 14:03:31,432 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:31,436 - INFO - Retrying request to /responses in 0.457274 seconds\n",
      "2025-08-12 14:03:31,804 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5707. Please try again in 1.712s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:31,900 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:31,909 - INFO - Retrying request to /responses in 0.862769 seconds\n",
      "2025-08-12 14:03:32,512 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:32,515 - INFO - Retrying request to /responses in 0.492845 seconds\n",
      "2025-08-12 14:03:32,531 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:32,534 - INFO - Retrying request to /responses in 0.812184 seconds\n",
      "2025-08-12 14:03:32,767 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5717. Please try again in 1.715s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:32,949 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:32,953 - INFO - Retrying request to /responses in 0.964239 seconds\n",
      "2025-08-12 14:03:33,203 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:33,211 - INFO - Retrying request to /responses in 0.755609 seconds\n",
      "2025-08-12 14:03:33,329 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:33,353 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:33,356 - INFO - Retrying request to /responses in 0.396606 seconds\n",
      "2025-08-12 14:03:33,597 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5789. Please try again in 1.736s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:33,685 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:33,688 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:33,689 - INFO - Retrying request to /responses in 0.465842 seconds\n",
      "2025-08-12 14:03:33,691 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:33,694 - INFO - Retrying request to /responses in 0.988450 seconds\n",
      "2025-08-12 14:03:34,351 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:34,356 - INFO - Retrying request to /responses in 0.412455 seconds\n",
      "2025-08-12 14:03:34,413 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:34,416 - INFO - Retrying request to /responses in 0.497126 seconds\n",
      "2025-08-12 14:03:34,919 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5699. Please try again in 1.709s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:34,939 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5707. Please try again in 1.712s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:34,974 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:34,977 - INFO - Retrying request to /responses in 0.395985 seconds\n",
      "2025-08-12 14:03:35,012 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:35,015 - INFO - Retrying request to /responses in 0.403669 seconds\n",
      "2025-08-12 14:03:35,435 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:35,564 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:35,567 - INFO - Retrying request to /responses in 0.484603 seconds\n",
      "2025-08-12 14:03:35,584 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:35,588 - INFO - Retrying request to /responses in 0.443650 seconds\n",
      "2025-08-12 14:03:35,689 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5708. Please try again in 1.712s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:35,895 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:35,898 - INFO - Retrying request to /responses in 0.453448 seconds\n",
      "2025-08-12 14:03:35,948 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5717. Please try again in 1.715s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:36,039 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:36,044 - INFO - Retrying request to /responses in 0.766047 seconds\n",
      "2025-08-12 14:03:36,363 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:36,366 - INFO - Retrying request to /responses in 0.856613 seconds\n",
      "2025-08-12 14:03:36,557 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:36,561 - INFO - Retrying request to /responses in 0.377159 seconds\n",
      "2025-08-12 14:03:36,707 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:36,710 - INFO - Retrying request to /responses in 0.497212 seconds\n",
      "2025-08-12 14:03:36,722 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:36,725 - INFO - Retrying request to /responses in 0.924096 seconds\n",
      "2025-08-12 14:03:37,032 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:37,035 - INFO - Retrying request to /responses in 0.425678 seconds\n",
      "2025-08-12 14:03:37,110 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5918. Please try again in 1.775s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:37,473 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:37,477 - INFO - Retrying request to /responses in 0.942947 seconds\n",
      "2025-08-12 14:03:37,559 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:37,563 - INFO - Retrying request to /responses in 0.932524 seconds\n",
      "2025-08-12 14:03:37,569 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:37,607 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5911. Please try again in 1.773s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:37,977 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:38,135 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:38,138 - INFO - Retrying request to /responses in 0.907924 seconds\n",
      "2025-08-12 14:03:38,175 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:38,262 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:38,894 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:38,898 - INFO - Retrying request to /responses in 0.382532 seconds\n",
      "2025-08-12 14:03:39,052 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:39,194 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:39,198 - INFO - Retrying request to /responses in 0.389901 seconds\n",
      "2025-08-12 14:03:39,216 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5714. Please try again in 1.714s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:39,335 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5908. Please try again in 1.772s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:39,365 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:39,368 - INFO - Retrying request to /responses in 0.433575 seconds\n",
      "2025-08-12 14:03:39,551 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5982. Please try again in 1.794s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:39,905 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:40,023 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:40,026 - INFO - Retrying request to /responses in 0.911865 seconds\n",
      "2025-08-12 14:03:40,063 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:40,067 - INFO - Retrying request to /responses in 0.487850 seconds\n",
      "2025-08-12 14:03:40,240 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:40,243 - INFO - Retrying request to /responses in 0.496895 seconds\n",
      "2025-08-12 14:03:40,255 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:40,258 - INFO - Retrying request to /responses in 0.774086 seconds\n",
      "2025-08-12 14:03:40,257 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:40,755 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:40,759 - INFO - Retrying request to /responses in 0.485319 seconds\n",
      "2025-08-12 14:03:40,939 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:40,943 - INFO - Retrying request to /responses in 0.450664 seconds\n",
      "2025-08-12 14:03:40,967 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:40,971 - INFO - Retrying request to /responses in 0.829624 seconds\n",
      "2025-08-12 14:03:41,191 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:41,194 - INFO - Retrying request to /responses in 0.778634 seconds\n",
      "2025-08-12 14:03:41,256 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:41,259 - INFO - Retrying request to /responses in 0.454651 seconds\n",
      "2025-08-12 14:03:41,355 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:41,358 - INFO - Retrying request to /responses in 0.784123 seconds\n",
      "2025-08-12 14:03:41,386 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:41,389 - INFO - Retrying request to /responses in 0.935807 seconds\n",
      "2025-08-12 14:03:41,405 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:41,408 - INFO - Retrying request to /responses in 0.845151 seconds\n",
      "2025-08-12 14:03:41,657 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5724. Please try again in 1.717s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:41,857 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:41,860 - INFO - Retrying request to /responses in 0.874040 seconds\n",
      "2025-08-12 14:03:42,086 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:42,090 - INFO - Retrying request to /responses in 0.752623 seconds\n",
      "2025-08-12 14:03:42,403 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5733. Please try again in 1.719s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:42,646 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:42,661 - INFO - Retrying request to /responses in 0.893177 seconds\n",
      "2025-08-12 14:03:42,945 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5929. Please try again in 1.778s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:42,976 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:42,980 - INFO - Retrying request to /responses in 0.377423 seconds\n",
      "2025-08-12 14:03:43,344 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5703. Please try again in 1.71s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:43,350 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:43,484 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:43,486 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:43,488 - INFO - Retrying request to /responses in 0.420831 seconds\n",
      "2025-08-12 14:03:43,488 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:43,489 - INFO - Retrying request to /responses in 0.459369 seconds\n",
      "2025-08-12 14:03:43,492 - INFO - Retrying request to /responses in 0.453762 seconds\n",
      "2025-08-12 14:03:43,687 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:43,691 - INFO - Retrying request to /responses in 0.492716 seconds\n",
      "2025-08-12 14:03:44,312 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:44,400 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:44,409 - INFO - Retrying request to /responses in 0.434603 seconds\n",
      "2025-08-12 14:03:44,430 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:44,432 - INFO - Retrying request to /responses in 0.798921 seconds\n",
      "2025-08-12 14:03:44,786 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5909. Please try again in 1.772s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:45,005 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5919. Please try again in 1.775s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:45,069 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:45,073 - INFO - Retrying request to /responses in 0.434073 seconds\n",
      "2025-08-12 14:03:45,234 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:45,255 - INFO - Retrying request to /responses in 0.960252 seconds\n",
      "2025-08-12 14:03:45,285 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:45,288 - INFO - Retrying request to /responses in 0.935863 seconds\n",
      "2025-08-12 14:03:45,598 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:45,668 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:45,671 - INFO - Retrying request to /responses in 0.753934 seconds\n",
      "2025-08-12 14:03:45,692 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:45,844 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:45,848 - INFO - Retrying request to /responses in 0.496278 seconds\n",
      "2025-08-12 14:03:46,122 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:46,127 - INFO - Retrying request to /responses in 0.823597 seconds\n",
      "2025-08-12 14:03:46,732 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:46,735 - INFO - Retrying request to /responses in 0.978464 seconds\n",
      "2025-08-12 14:03:46,844 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5762. Please try again in 1.728s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:46,923 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:46,972 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:46,975 - INFO - Retrying request to /responses in 0.386668 seconds\n",
      "2025-08-12 14:03:46,999 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5906. Please try again in 1.771s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:47,456 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5910. Please try again in 1.773s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:47,681 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5908. Please try again in 1.772s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:47,686 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:47,689 - INFO - Retrying request to /responses in 0.471919 seconds\n",
      "2025-08-12 14:03:47,920 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5756. Please try again in 1.726s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:48,065 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5722. Please try again in 1.716s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:48,097 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5910. Please try again in 1.773s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:48,154 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:48,158 - INFO - Retrying request to /responses in 0.816107 seconds\n",
      "2025-08-12 14:03:48,278 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5903. Please try again in 1.77s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:48,294 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:48,297 - INFO - Retrying request to /responses in 0.925732 seconds\n",
      "2025-08-12 14:03:48,392 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5695. Please try again in 1.708s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:48,445 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:48,497 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:48,500 - INFO - Retrying request to /responses in 0.926959 seconds\n",
      "2025-08-12 14:03:48,674 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:48,678 - INFO - Retrying request to /responses in 0.425700 seconds\n",
      "2025-08-12 14:03:48,750 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:48,753 - INFO - Retrying request to /responses in 0.450914 seconds\n",
      "2025-08-12 14:03:48,909 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5951. Please try again in 1.785s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:49,007 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:49,320 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:49,324 - INFO - Retrying request to /responses in 0.986309 seconds\n",
      "2025-08-12 14:03:49,456 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:49,459 - INFO - Retrying request to /responses in 0.402457 seconds\n",
      "2025-08-12 14:03:49,460 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:49,464 - INFO - Retrying request to /responses in 0.427197 seconds\n",
      "2025-08-12 14:03:49,891 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:49,994 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:49,998 - INFO - Retrying request to /responses in 0.829903 seconds\n",
      "2025-08-12 14:03:50,350 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:50,354 - INFO - Retrying request to /responses in 0.987821 seconds\n",
      "2025-08-12 14:03:50,436 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:50,439 - INFO - Retrying request to /responses in 0.471696 seconds\n",
      "2025-08-12 14:03:50,487 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:50,490 - INFO - Retrying request to /responses in 0.781232 seconds\n",
      "2025-08-12 14:03:50,790 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:50,813 - INFO - Retrying request to /responses in 0.838105 seconds\n",
      "2025-08-12 14:03:50,960 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5985. Please try again in 1.795s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:51,312 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:51,315 - INFO - Retrying request to /responses in 0.489393 seconds\n",
      "2025-08-12 14:03:51,663 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:51,666 - INFO - Retrying request to /responses in 0.428187 seconds\n",
      "2025-08-12 14:03:51,811 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:51,815 - INFO - Retrying request to /responses in 0.442688 seconds\n",
      "2025-08-12 14:03:51,945 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:52,005 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5895. Please try again in 1.768s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:52,041 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:52,183 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:52,219 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:52,224 - INFO - Retrying request to /responses in 0.839211 seconds\n",
      "2025-08-12 14:03:52,503 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5715. Please try again in 1.714s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:52,554 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:52,583 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:52,586 - INFO - Retrying request to /responses in 0.421284 seconds\n",
      "2025-08-12 14:03:52,595 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5913. Please try again in 1.773s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:52,616 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:52,618 - INFO - Retrying request to /responses in 0.413656 seconds\n",
      "2025-08-12 14:03:52,784 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5903. Please try again in 1.77s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:52,847 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:52,850 - INFO - Retrying request to /responses in 0.852174 seconds\n",
      "2025-08-12 14:03:52,884 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:52,885 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:52,889 - INFO - Retrying request to /responses in 0.491121 seconds\n",
      "2025-08-12 14:03:54,015 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:54,018 - INFO - Retrying request to /responses in 0.395261 seconds\n",
      "2025-08-12 14:03:54,412 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:54,546 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:54,549 - INFO - Retrying request to /responses in 0.393261 seconds\n",
      "2025-08-12 14:03:54,761 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:54,765 - INFO - Retrying request to /responses in 0.780221 seconds\n",
      "2025-08-12 14:03:54,959 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5904. Please try again in 1.771s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:55,009 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:55,012 - INFO - Retrying request to /responses in 0.752997 seconds\n",
      "2025-08-12 14:03:55,502 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:55,509 - INFO - Retrying request to /responses in 0.856460 seconds\n",
      "2025-08-12 14:03:55,542 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5913. Please try again in 1.773s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:55,692 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:55,787 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5910. Please try again in 1.773s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:55,815 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:55,818 - INFO - Retrying request to /responses in 0.892773 seconds\n",
      "2025-08-12 14:03:55,857 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:55,860 - INFO - Retrying request to /responses in 0.468015 seconds\n",
      "2025-08-12 14:03:56,040 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:56,044 - INFO - Retrying request to /responses in 0.440303 seconds\n",
      "2025-08-12 14:03:56,219 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5723. Please try again in 1.716s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:56,654 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:56,888 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5733. Please try again in 1.719s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:56,892 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:56,895 - INFO - Retrying request to /responses in 0.425717 seconds\n",
      "2025-08-12 14:03:56,977 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:56,980 - INFO - Retrying request to /responses in 0.377270 seconds\n",
      "2025-08-12 14:03:57,091 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:57,095 - INFO - Retrying request to /responses in 0.409725 seconds\n",
      "2025-08-12 14:03:57,133 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:57,136 - INFO - Retrying request to /responses in 0.818616 seconds\n",
      "2025-08-12 14:03:57,176 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:57,179 - INFO - Retrying request to /responses in 0.811361 seconds\n",
      "2025-08-12 14:03:57,214 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:57,216 - INFO - Retrying request to /responses in 0.823640 seconds\n",
      "2025-08-12 14:03:57,361 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:57,367 - INFO - Retrying request to /responses in 0.939388 seconds\n",
      "2025-08-12 14:03:57,388 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5735. Please try again in 1.72s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:57,521 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5732. Please try again in 1.719s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:57,760 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5725. Please try again in 1.717s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:58,047 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:58,051 - INFO - Retrying request to /responses in 0.785511 seconds\n",
      "2025-08-12 14:03:58,164 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:58,257 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:03:58,259 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:58,262 - INFO - Retrying request to /responses in 0.401869 seconds\n",
      "2025-08-12 14:03:58,649 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:58,653 - INFO - Retrying request to /responses in 0.781142 seconds\n",
      "2025-08-12 14:03:59,125 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:59,129 - INFO - Retrying request to /responses in 0.425303 seconds\n",
      "2025-08-12 14:03:59,161 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5920. Please try again in 1.776s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:03:59,244 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:59,247 - INFO - Retrying request to /responses in 0.786342 seconds\n",
      "2025-08-12 14:03:59,825 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:59,828 - INFO - Retrying request to /responses in 0.404680 seconds\n",
      "2025-08-12 14:03:59,866 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:59,869 - INFO - Retrying request to /responses in 0.475400 seconds\n",
      "2025-08-12 14:03:59,932 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:03:59,935 - INFO - Retrying request to /responses in 0.968014 seconds\n",
      "2025-08-12 14:04:00,057 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:00,539 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:00,541 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:00,543 - INFO - Retrying request to /responses in 0.833821 seconds\n",
      "2025-08-12 14:04:00,545 - INFO - Retrying request to /responses in 0.424283 seconds\n",
      "2025-08-12 14:04:01,013 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:01,202 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5781. Please try again in 1.734s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:01,295 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:01,299 - INFO - Retrying request to /responses in 0.492914 seconds\n",
      "2025-08-12 14:04:01,332 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5929. Please try again in 1.778s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:01,460 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:01,464 - INFO - Retrying request to /responses in 0.899439 seconds\n",
      "2025-08-12 14:04:01,553 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:01,557 - INFO - Retrying request to /responses in 0.498916 seconds\n",
      "2025-08-12 14:04:01,652 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:01,753 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5728. Please try again in 1.718s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:01,882 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5899. Please try again in 1.769s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:01,951 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:01,954 - INFO - Retrying request to /responses in 0.883803 seconds\n",
      "2025-08-12 14:04:02,291 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:02,295 - INFO - Retrying request to /responses in 0.446988 seconds\n",
      "2025-08-12 14:04:02,391 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:02,405 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:02,409 - INFO - Retrying request to /responses in 0.429228 seconds\n",
      "2025-08-12 14:04:02,534 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:02,879 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:02,883 - INFO - Retrying request to /responses in 0.430278 seconds\n",
      "2025-08-12 14:04:03,113 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:03,117 - INFO - Retrying request to /responses in 0.435022 seconds\n",
      "2025-08-12 14:04:03,216 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5747. Please try again in 1.724s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:03,397 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:03,400 - INFO - Retrying request to /responses in 0.985192 seconds\n",
      "2025-08-12 14:04:03,439 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:03,442 - INFO - Retrying request to /responses in 0.940487 seconds\n",
      "2025-08-12 14:04:03,589 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5710. Please try again in 1.713s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:03,605 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:03,624 - INFO - Retrying request to /responses in 0.997414 seconds\n",
      "2025-08-12 14:04:03,937 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5952. Please try again in 1.785s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:04,174 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:04,178 - INFO - Retrying request to /responses in 0.449541 seconds\n",
      "2025-08-12 14:04:04,315 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5918. Please try again in 1.775s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:04,450 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:04,470 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:04,473 - INFO - Retrying request to /responses in 0.394516 seconds\n",
      "2025-08-12 14:04:04,671 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:04,674 - INFO - Retrying request to /responses in 0.899246 seconds\n",
      "2025-08-12 14:04:04,877 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:04,879 - INFO - Retrying request to /responses in 0.759296 seconds\n",
      "2025-08-12 14:04:05,004 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5746. Please try again in 1.723s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:05,169 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5958. Please try again in 1.787s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:05,276 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:05,279 - INFO - Retrying request to /responses in 0.984014 seconds\n",
      "2025-08-12 14:04:05,899 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:05,902 - INFO - Retrying request to /responses in 0.441596 seconds\n",
      "2025-08-12 14:04:06,129 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:06,132 - INFO - Retrying request to /responses in 0.454449 seconds\n",
      "2025-08-12 14:04:06,416 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:06,420 - INFO - Retrying request to /responses in 0.496930 seconds\n",
      "2025-08-12 14:04:06,977 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:06,980 - INFO - Retrying request to /responses in 0.480861 seconds\n",
      "2025-08-12 14:04:07,192 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:07,194 - INFO - Retrying request to /responses in 0.405026 seconds\n",
      "2025-08-12 14:04:07,242 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:07,245 - INFO - Retrying request to /responses in 0.997600 seconds\n",
      "2025-08-12 14:04:07,297 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5786. Please try again in 1.735s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:07,427 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:07,430 - INFO - Retrying request to /responses in 0.919261 seconds\n",
      "2025-08-12 14:04:07,475 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5891. Please try again in 1.767s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:07,817 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:07,822 - INFO - Retrying request to /responses in 0.922965 seconds\n",
      "2025-08-12 14:04:08,119 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:08,122 - INFO - Retrying request to /responses in 0.441915 seconds\n",
      "2025-08-12 14:04:08,143 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:08,146 - INFO - Retrying request to /responses in 0.960820 seconds\n",
      "2025-08-12 14:04:08,515 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:08,519 - INFO - Retrying request to /responses in 0.468122 seconds\n",
      "2025-08-12 14:04:09,058 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:09,061 - INFO - Retrying request to /responses in 0.383954 seconds\n",
      "2025-08-12 14:04:09,148 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5720. Please try again in 1.716s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:09,308 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:09,311 - INFO - Retrying request to /responses in 0.825005 seconds\n",
      "2025-08-12 14:04:09,647 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:09,651 - INFO - Retrying request to /responses in 0.954241 seconds\n",
      "2025-08-12 14:04:09,701 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5761. Please try again in 1.728s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:09,783 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:09,786 - INFO - Retrying request to /responses in 0.442791 seconds\n",
      "2025-08-12 14:04:09,973 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5911. Please try again in 1.773s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:09,996 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:09,999 - INFO - Retrying request to /responses in 0.848336 seconds\n",
      "2025-08-12 14:04:10,040 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5758. Please try again in 1.727s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:10,302 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5727. Please try again in 1.718s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:10,765 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:10,905 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:10,909 - INFO - Retrying request to /responses in 0.409305 seconds\n",
      "2025-08-12 14:04:10,946 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:10,947 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:10,950 - INFO - Retrying request to /responses in 0.470838 seconds\n",
      "2025-08-12 14:04:11,173 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:11,176 - INFO - Retrying request to /responses in 0.379311 seconds\n",
      "2025-08-12 14:04:11,240 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:11,244 - INFO - Retrying request to /responses in 0.896809 seconds\n",
      "2025-08-12 14:04:11,498 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 196866, Requested 5795. Please try again in 798ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:11,803 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:11,813 - INFO - Retrying request to /responses in 0.871322 seconds\n",
      "2025-08-12 14:04:12,023 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 195009, Requested 5724. Please try again in 219ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:12,027 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:12,031 - INFO - Retrying request to /responses in 0.805930 seconds\n",
      "2025-08-12 14:04:12,210 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:12,214 - INFO - Retrying request to /responses in 0.385863 seconds\n",
      "2025-08-12 14:04:12,276 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:12,420 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:12,423 - INFO - Retrying request to /responses in 0.912588 seconds\n",
      "2025-08-12 14:04:13,333 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5722. Please try again in 1.716s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:13,681 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:13,684 - INFO - Retrying request to /responses in 0.489189 seconds\n",
      "2025-08-12 14:04:13,711 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:13,720 - INFO - Retrying request to /responses in 0.818059 seconds\n",
      "2025-08-12 14:04:14,071 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:14,074 - INFO - Retrying request to /responses in 0.840882 seconds\n",
      "2025-08-12 14:04:14,210 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5919. Please try again in 1.775s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:14,286 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:14,290 - INFO - Retrying request to /responses in 0.407822 seconds\n",
      "2025-08-12 14:04:14,346 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5929. Please try again in 1.778s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:14,426 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5730. Please try again in 1.719s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:14,763 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:14,767 - INFO - Retrying request to /responses in 0.974259 seconds\n",
      "2025-08-12 14:04:14,904 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5928. Please try again in 1.778s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:14,937 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:14,940 - INFO - Retrying request to /responses in 0.439481 seconds\n",
      "2025-08-12 14:04:15,080 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:15,084 - INFO - Retrying request to /responses in 0.383960 seconds\n",
      "2025-08-12 14:04:15,351 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:15,355 - INFO - Retrying request to /responses in 0.376815 seconds\n",
      "2025-08-12 14:04:15,493 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:15,496 - INFO - Retrying request to /responses in 0.449627 seconds\n",
      "2025-08-12 14:04:15,693 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:15,881 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:15,884 - INFO - Retrying request to /responses in 0.839212 seconds\n",
      "2025-08-12 14:04:16,150 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5921. Please try again in 1.776s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:16,385 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:16,389 - INFO - Retrying request to /responses in 0.842117 seconds\n",
      "2025-08-12 14:04:16,452 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5931. Please try again in 1.779s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:16,843 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5813. Please try again in 1.743s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:16,850 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:16,850 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:16,853 - INFO - Retrying request to /responses in 0.499552 seconds\n",
      "2025-08-12 14:04:16,853 - INFO - Retrying request to /responses in 0.889664 seconds\n",
      "2025-08-12 14:04:17,118 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:17,121 - INFO - Retrying request to /responses in 0.826406 seconds\n",
      "2025-08-12 14:04:17,157 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:17,161 - INFO - Retrying request to /responses in 0.395798 seconds\n",
      "2025-08-12 14:04:17,741 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5977. Please try again in 1.793s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:18,208 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:18,212 - INFO - Retrying request to /responses in 0.837385 seconds\n",
      "2025-08-12 14:04:18,756 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:18,759 - INFO - Retrying request to /responses in 0.477075 seconds\n",
      "2025-08-12 14:04:18,876 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5731. Please try again in 1.719s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:18,892 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:18,896 - INFO - Retrying request to /responses in 0.486232 seconds\n",
      "2025-08-12 14:04:19,170 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:19,174 - INFO - Retrying request to /responses in 0.480123 seconds\n",
      "2025-08-12 14:04:19,177 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5717. Please try again in 1.715s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:19,318 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:19,448 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:19,679 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:19,682 - INFO - Retrying request to /responses in 0.890472 seconds\n",
      "2025-08-12 14:04:19,750 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:19,753 - INFO - Retrying request to /responses in 0.406695 seconds\n",
      "2025-08-12 14:04:19,895 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5943. Please try again in 1.782s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:20,610 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:20,614 - INFO - Retrying request to /responses in 0.891324 seconds\n",
      "2025-08-12 14:04:20,629 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5722. Please try again in 1.716s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:20,732 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:20,853 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:20,856 - INFO - Retrying request to /responses in 0.376355 seconds\n",
      "2025-08-12 14:04:20,867 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5906. Please try again in 1.771s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:20,915 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:21,137 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:21,140 - INFO - Retrying request to /responses in 0.784692 seconds\n",
      "2025-08-12 14:04:21,319 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:21,322 - INFO - Retrying request to /responses in 0.766880 seconds\n",
      "2025-08-12 14:04:21,854 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5719. Please try again in 1.715s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:22,324 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5740. Please try again in 1.722s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:22,390 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:22,393 - INFO - Retrying request to /responses in 0.483173 seconds\n",
      "2025-08-12 14:04:22,434 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:22,437 - INFO - Retrying request to /responses in 0.954635 seconds\n",
      "2025-08-12 14:04:22,501 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:22,508 - INFO - Retrying request to /responses in 0.388182 seconds\n",
      "2025-08-12 14:04:22,707 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:22,711 - INFO - Retrying request to /responses in 0.919041 seconds\n",
      "2025-08-12 14:04:22,943 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5942. Please try again in 1.782s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:22,975 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5743. Please try again in 1.722s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:23,369 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:23,373 - INFO - Retrying request to /responses in 0.487810 seconds\n",
      "2025-08-12 14:04:23,488 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:23,540 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:23,544 - INFO - Retrying request to /responses in 0.764387 seconds\n",
      "2025-08-12 14:04:24,352 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:24,355 - INFO - Retrying request to /responses in 0.792061 seconds\n",
      "2025-08-12 14:04:24,755 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:24,759 - INFO - Retrying request to /responses in 0.482637 seconds\n",
      "2025-08-12 14:04:24,857 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:24,861 - INFO - Retrying request to /responses in 0.980736 seconds\n",
      "2025-08-12 14:04:25,025 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5982. Please try again in 1.794s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:25,057 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:25,061 - INFO - Retrying request to /responses in 0.787566 seconds\n",
      "2025-08-12 14:04:25,092 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5916. Please try again in 1.774s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:25,163 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:25,167 - INFO - Retrying request to /responses in 0.457774 seconds\n",
      "2025-08-12 14:04:25,239 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:25,242 - INFO - Retrying request to /responses in 0.398905 seconds\n",
      "2025-08-12 14:04:25,349 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:25,467 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:25,471 - INFO - Retrying request to /responses in 0.497491 seconds\n",
      "2025-08-12 14:04:25,539 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:25,543 - INFO - Retrying request to /responses in 0.457135 seconds\n",
      "2025-08-12 14:04:25,791 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:25,795 - INFO - Retrying request to /responses in 0.491572 seconds\n",
      "2025-08-12 14:04:25,830 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:26,185 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:26,189 - INFO - Retrying request to /responses in 0.411381 seconds\n",
      "2025-08-12 14:04:26,256 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:26,259 - INFO - Retrying request to /responses in 0.376145 seconds\n",
      "2025-08-12 14:04:26,531 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5957. Please try again in 1.787s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:26,660 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:26,668 - INFO - Retrying request to /responses in 0.398017 seconds\n",
      "2025-08-12 14:04:26,785 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5923. Please try again in 1.776s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:26,983 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:27,598 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:27,603 - INFO - Retrying request to /responses in 0.807506 seconds\n",
      "2025-08-12 14:04:27,730 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:27,734 - INFO - Retrying request to /responses in 0.389597 seconds\n",
      "2025-08-12 14:04:27,771 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:27,774 - INFO - Retrying request to /responses in 0.482092 seconds\n",
      "2025-08-12 14:04:27,865 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5954. Please try again in 1.786s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:28,209 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:28,213 - INFO - Retrying request to /responses in 0.405336 seconds\n",
      "2025-08-12 14:04:28,280 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:28,284 - INFO - Retrying request to /responses in 0.772499 seconds\n",
      "2025-08-12 14:04:28,320 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5733. Please try again in 1.719s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:28,700 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:28,708 - INFO - Retrying request to /responses in 0.960533 seconds\n",
      "2025-08-12 14:04:29,089 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5991. Please try again in 1.797s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:29,266 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5920. Please try again in 1.776s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:29,873 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:30,056 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:30,060 - INFO - Retrying request to /responses in 0.476650 seconds\n",
      "2025-08-12 14:04:30,111 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5926. Please try again in 1.777s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:30,237 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:30,240 - INFO - Retrying request to /responses in 0.788193 seconds\n",
      "2025-08-12 14:04:30,633 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 199243, Requested 5918. Please try again in 1.548s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:30,873 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:30,876 - INFO - Retrying request to /responses in 0.917428 seconds\n",
      "2025-08-12 14:04:30,884 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:30,887 - INFO - Retrying request to /responses in 0.968265 seconds\n",
      "2025-08-12 14:04:30,944 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:30,948 - INFO - Retrying request to /responses in 0.905015 seconds\n",
      "2025-08-12 14:04:31,574 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:31,578 - INFO - Retrying request to /responses in 0.404506 seconds\n",
      "2025-08-12 14:04:31,602 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:31,608 - INFO - Retrying request to /responses in 0.779699 seconds\n",
      "2025-08-12 14:04:31,611 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:31,613 - INFO - Retrying request to /responses in 0.771258 seconds\n",
      "2025-08-12 14:04:31,956 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:31,960 - INFO - Retrying request to /responses in 0.788484 seconds\n",
      "2025-08-12 14:04:31,964 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:31,967 - INFO - Retrying request to /responses in 0.759470 seconds\n",
      "2025-08-12 14:04:32,233 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:32,236 - INFO - Retrying request to /responses in 0.460828 seconds\n",
      "2025-08-12 14:04:32,284 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:32,511 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:32,728 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:32,884 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:32,888 - INFO - Retrying request to /responses in 0.398518 seconds\n",
      "2025-08-12 14:04:33,017 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5727. Please try again in 1.718s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:33,226 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:33,229 - INFO - Retrying request to /responses in 0.962124 seconds\n",
      "2025-08-12 14:04:33,266 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:33,289 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:33,292 - INFO - Retrying request to /responses in 0.852449 seconds\n",
      "2025-08-12 14:04:33,470 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:33,474 - INFO - Retrying request to /responses in 0.408683 seconds\n",
      "2025-08-12 14:04:33,779 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:33,783 - INFO - Retrying request to /responses in 0.842069 seconds\n",
      "2025-08-12 14:04:33,867 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:33,870 - INFO - Retrying request to /responses in 0.412581 seconds\n",
      "2025-08-12 14:04:35,102 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:35,110 - INFO - Retrying request to /responses in 0.463805 seconds\n",
      "2025-08-12 14:04:35,127 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:35,131 - INFO - Retrying request to /responses in 0.956593 seconds\n",
      "2025-08-12 14:04:35,512 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 6009. Please try again in 1.802s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:35,675 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 196889, Requested 5718. Please try again in 782ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:35,833 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:35,836 - INFO - Retrying request to /responses in 0.429213 seconds\n",
      "2025-08-12 14:04:36,450 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:37,100 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:37,110 - INFO - Retrying request to /responses in 0.752101 seconds\n",
      "2025-08-12 14:04:37,112 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 199648, Requested 5722. Please try again in 1.611s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:37,162 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 195314, Requested 5927. Please try again in 372ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:37,680 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 199558, Requested 5918. Please try again in 1.642s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:37,804 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 197317, Requested 5755. Please try again in 921ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:37,844 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 197620, Requested 5722. Please try again in 1.002s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:38,118 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 196271, Requested 5741. Please try again in 603ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:38,467 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:38,471 - INFO - Retrying request to /responses in 0.479995 seconds\n",
      "2025-08-12 14:04:38,604 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:38,608 - INFO - Retrying request to /responses in 0.443335 seconds\n",
      "2025-08-12 14:04:38,834 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:38,837 - INFO - Retrying request to /responses in 0.462250 seconds\n",
      "2025-08-12 14:04:38,866 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:38,879 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5703. Please try again in 1.71s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:39,148 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:39,393 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:39,397 - INFO - Retrying request to /responses in 0.840072 seconds\n",
      "2025-08-12 14:04:40,079 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:40,083 - INFO - Retrying request to /responses in 0.402065 seconds\n",
      "2025-08-12 14:04:40,197 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:40,201 - INFO - Retrying request to /responses in 0.414731 seconds\n",
      "2025-08-12 14:04:40,224 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:40,228 - INFO - Retrying request to /responses in 0.950809 seconds\n",
      "2025-08-12 14:04:40,231 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 196884, Requested 5939. Please try again in 846ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:40,256 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:40,258 - INFO - Retrying request to /responses in 0.867167 seconds\n",
      "2025-08-12 14:04:40,393 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5716. Please try again in 1.714s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:41,497 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 199235, Requested 5753. Please try again in 1.496s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:42,219 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:42,224 - INFO - Retrying request to /responses in 0.377245 seconds\n",
      "2025-08-12 14:04:42,321 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:42,325 - INFO - Retrying request to /responses in 0.445766 seconds\n",
      "2025-08-12 14:04:42,651 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:42,655 - INFO - Retrying request to /responses in 0.401802 seconds\n",
      "2025-08-12 14:04:44,079 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:44,083 - INFO - Retrying request to /responses in 0.472624 seconds\n",
      "2025-08-12 14:04:44,266 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:44,308 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:44,311 - INFO - Retrying request to /responses in 0.802411 seconds\n",
      "2025-08-12 14:04:44,452 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:44,455 - INFO - Retrying request to /responses in 0.786120 seconds\n",
      "2025-08-12 14:04:44,495 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:44,498 - INFO - Retrying request to /responses in 0.879752 seconds\n",
      "2025-08-12 14:04:44,528 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:44,532 - INFO - Retrying request to /responses in 0.441527 seconds\n",
      "2025-08-12 14:04:45,066 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:45,069 - INFO - Retrying request to /responses in 0.430746 seconds\n",
      "2025-08-12 14:04:45,306 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:45,328 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:45,332 - INFO - Retrying request to /responses in 0.423807 seconds\n",
      "2025-08-12 14:04:45,837 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:45,841 - INFO - Retrying request to /responses in 0.872211 seconds\n",
      "2025-08-12 14:04:45,904 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 197465, Requested 5775. Please try again in 972ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:45,974 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 197252, Requested 5741. Please try again in 897ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:46,199 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:46,208 - INFO - Retrying request to /responses in 0.457982 seconds\n",
      "2025-08-12 14:04:48,423 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:48,427 - INFO - Retrying request to /responses in 0.920992 seconds\n",
      "2025-08-12 14:04:49,060 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:49,583 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:49,587 - INFO - Retrying request to /responses in 0.426641 seconds\n",
      "2025-08-12 14:04:50,870 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:50,874 - INFO - Retrying request to /responses in 0.885941 seconds\n",
      "2025-08-12 14:04:51,082 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-5-nano: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-5-nano in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 196692, Requested 5923. Please try again in 784ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:51,094 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:51,098 - INFO - Retrying request to /responses in 0.959871 seconds\n",
      "2025-08-12 14:04:51,432 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:51,453 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:51,455 - INFO - Retrying request to /responses in 0.407606 seconds\n",
      "2025-08-12 14:04:51,920 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:51,924 - INFO - Retrying request to /responses in 0.941061 seconds\n",
      "2025-08-12 14:04:52,763 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:53,825 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:53,829 - INFO - Retrying request to /responses in 0.444739 seconds\n",
      "2025-08-12 14:04:54,241 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 198226, Requested 5813. Please try again in 1.211s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:54,346 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:55,117 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:55,297 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:04:56,478 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:56,481 - INFO - Retrying request to /responses in 0.461350 seconds\n",
      "2025-08-12 14:04:56,666 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 197320, Requested 5760. Please try again in 924ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:04:57,212 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:57,215 - INFO - Retrying request to /responses in 0.399410 seconds\n",
      "2025-08-12 14:04:57,827 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:57,831 - INFO - Retrying request to /responses in 0.871452 seconds\n",
      "2025-08-12 14:04:59,129 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:04:59,133 - INFO - Retrying request to /responses in 0.837012 seconds\n",
      "2025-08-12 14:05:00,636 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:00,638 - INFO - Retrying request to /responses in 0.387380 seconds\n",
      "2025-08-12 14:05:00,847 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:01,018 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:02,158 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:02,789 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:02,792 - INFO - Retrying request to /responses in 0.913347 seconds\n",
      "2025-08-12 14:05:03,659 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:03,832 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:05,337 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5764. Please try again in 1.729s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:05:05,504 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:05,511 - INFO - Retrying request to /responses in 0.430911 seconds\n",
      "2025-08-12 14:05:06,898 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:08,350 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:08,353 - INFO - Retrying request to /responses in 0.411993 seconds\n",
      "2025-08-12 14:05:08,530 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:08,997 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:09,644 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:09,648 - INFO - Retrying request to /responses in 0.439108 seconds\n",
      "2025-08-12 14:05:09,804 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:11,271 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:11,628 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:11,632 - INFO - Retrying request to /responses in 0.896557 seconds\n",
      "2025-08-12 14:05:11,981 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:12,436 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:12,540 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:13,582 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:13,586 - INFO - Retrying request to /responses in 0.390117 seconds\n",
      "2025-08-12 14:05:13,896 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:14,083 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:14,629 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:15,336 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:15,338 - INFO - Retrying request to /responses in 0.848547 seconds\n",
      "2025-08-12 14:05:15,934 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:17,414 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:17,829 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:18,625 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:18,628 - INFO - Retrying request to /responses in 0.400083 seconds\n",
      "2025-08-12 14:05:18,816 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:18,819 - INFO - Retrying request to /responses in 0.770140 seconds\n",
      "2025-08-12 14:05:19,203 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:20,005 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:20,009 - INFO - Retrying request to /responses in 0.446844 seconds\n",
      "2025-08-12 14:05:21,193 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:21,721 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:21,770 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 199183, Requested 5731. Please try again in 1.474s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:05:21,967 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:22,683 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:24,157 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:24,161 - INFO - Retrying request to /responses in 0.786757 seconds\n",
      "2025-08-12 14:05:24,548 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:24,973 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:25,155 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:25,487 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:25,491 - INFO - Retrying request to /responses in 0.934883 seconds\n",
      "2025-08-12 14:05:25,598 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 195353, Requested 5758. Please try again in 333ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:05:26,064 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:26,149 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:26,626 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:28,661 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:29,102 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:29,327 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:29,868 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:29,872 - INFO - Retrying request to /responses in 0.420376 seconds\n",
      "2025-08-12 14:05:29,942 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:30,610 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5838. Please try again in 1.751s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:05:30,817 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:30,916 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:30,920 - INFO - Retrying request to /responses in 0.380191 seconds\n",
      "2025-08-12 14:05:31,889 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:32,112 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:32,634 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:34,104 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:34,597 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:35,204 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:35,208 - INFO - Retrying request to /responses in 0.477100 seconds\n",
      "2025-08-12 14:05:35,839 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:35,955 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:35,959 - INFO - Retrying request to /responses in 0.414713 seconds\n",
      "2025-08-12 14:05:36,155 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:36,216 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:36,736 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:36,740 - INFO - Retrying request to /responses in 0.493917 seconds\n",
      "2025-08-12 14:05:36,827 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:36,830 - INFO - Retrying request to /responses in 0.497747 seconds\n",
      "2025-08-12 14:05:37,309 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:37,313 - INFO - Retrying request to /responses in 0.982279 seconds\n",
      "2025-08-12 14:05:38,207 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:39,308 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:39,314 - INFO - Retrying request to /responses in 0.432041 seconds\n",
      "2025-08-12 14:05:40,921 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:40,926 - INFO - Retrying request to /responses in 0.443130 seconds\n",
      "2025-08-12 14:05:41,105 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:41,402 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:41,410 - INFO - Retrying request to /responses in 0.857354 seconds\n",
      "2025-08-12 14:05:42,191 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:42,195 - INFO - Retrying request to /responses in 0.442518 seconds\n",
      "2025-08-12 14:05:42,801 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5743. Please try again in 1.722s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:05:43,150 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:43,154 - INFO - Retrying request to /responses in 0.983428 seconds\n",
      "2025-08-12 14:05:44,255 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:45,464 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:45,578 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:45,834 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:46,278 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:47,643 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:47,647 - INFO - Retrying request to /responses in 0.929638 seconds\n",
      "2025-08-12 14:05:48,113 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 195430, Requested 5725. Please try again in 346ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:05:48,398 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:49,148 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:50,480 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit hit for gpt-4o-mini: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1SttmZMm9hEuBw7UWMxZimku on tokens per min (TPM): Limit 200000, Used 200000, Requested 5723. Please try again in 1.716s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:05:50,958 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:51,565 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:51,569 - INFO - Retrying request to /responses in 0.458713 seconds\n",
      "2025-08-12 14:05:53,033 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:53,307 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:53,311 - INFO - Retrying request to /responses in 0.410521 seconds\n",
      "2025-08-12 14:05:53,860 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:55,832 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:58,007 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:05:59,457 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-12 14:05:59,461 - INFO - Retrying request to /responses in 0.441866 seconds\n",
      "2025-08-12 14:06:01,928 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:02,945 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:03,821 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:04,247 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:07,914 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:07,941 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:08,786 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:09,903 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:12,357 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:13,234 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:17,768 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:19,589 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:19,980 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:21,760 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:21,905 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:23,238 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:24,508 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:25,111 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:26,796 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:27,721 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:28,584 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:30,106 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:31,253 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:31,665 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:32,760 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:34,480 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:39,184 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:39,279 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:42,379 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:44,989 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:45,917 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:46,201 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:50,207 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:50,744 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:51,729 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:54,033 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:54,603 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:06:58,625 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:00,900 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:01,957 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:02,204 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:02,945 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:06,932 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:11,086 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:12,716 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:17,550 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:17,628 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:17,923 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:20,824 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:21,300 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:21,610 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:22,935 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:24,545 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:25,270 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:31,935 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:32,854 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:33,932 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:35,851 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:36,275 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:36,858 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:43,500 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:49,523 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:51,970 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:52,446 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:53,314 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:54,997 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:55,242 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:07:57,667 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:02,265 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:02,650 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:02,742 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:03,985 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:07,316 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:07,880 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:10,042 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:10,269 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:12,874 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:14,705 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:17,567 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:18,543 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:19,559 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:23,650 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:24,433 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:25,436 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:26,175 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:26,325 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:26,482 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:28,180 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:30,638 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:33,797 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:35,433 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:37,109 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:39,257 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:41,938 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:46,317 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:50,580 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:54,305 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:56,052 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:56,416 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:08:58,386 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:09:01,178 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:09:05,425 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:09:12,750 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:09:16,056 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:09:16,347 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:09:16,670 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:09:17,222 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:09:19,176 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:09:22,865 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:09:28,710 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:09:30,824 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:09:35,135 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:09:35,416 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:09:40,187 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:09:40,538 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:09:42,669 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:09:42,914 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:09:44,938 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:09:45,115 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:09:45,276 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:09:48,096 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:09:49,968 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:09:53,744 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:09:56,493 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:09:56,934 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:09:57,321 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:09:57,953 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:10:06,257 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:10:06,446 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:10:07,578 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:10:08,962 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:10:14,083 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:10:15,579 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:10:15,602 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:10:18,886 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:10:22,289 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:10:27,912 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:10:28,465 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:10:29,443 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:10:30,020 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:10:32,327 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:10:32,594 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:10:32,866 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:10:33,718 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:10:37,444 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:10:42,207 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:10:44,730 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:10:48,876 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:10:49,981 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:10:53,547 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:10:54,717 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:10:55,506 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:10:58,529 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:11:01,127 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:11:03,625 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:11:06,840 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:11:07,507 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:11:08,899 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:11:09,580 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:11:10,966 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:11:16,225 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:11:20,569 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:11:20,790 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:11:25,659 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:11:29,274 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:11:29,439 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:11:32,049 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:11:32,263 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:11:33,672 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:11:36,834 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:11:37,176 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:11:39,189 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:11:41,320 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:11:42,709 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:11:45,143 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:11:47,509 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:11:53,347 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:12:01,158 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:12:01,533 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:12:02,368 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:12:09,789 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:12:11,469 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:12:12,785 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:12:13,924 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:12:22,400 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:12:28,399 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:12:32,036 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:12:33,386 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:12:37,694 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:12:40,774 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:12:43,877 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:12:47,723 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:12:49,437 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:12:50,514 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:12:51,220 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:12:52,663 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:12:53,390 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:12:55,527 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:12:56,291 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:12:56,859 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:12:58,840 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:12:59,272 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:13:00,412 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:13:03,688 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:13:09,071 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:13:10,105 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:13:18,264 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:13:20,467 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:13:21,382 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:13:24,068 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:13:25,804 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:13:27,122 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:13:30,942 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:13:33,509 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:13:34,304 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:13:41,682 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:13:42,902 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:13:43,506 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:13:45,513 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:13:48,438 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:13:52,257 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:13:52,395 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:13:56,127 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:13:56,558 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:13:58,836 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:00,399 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:01,890 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:04,128 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:04,718 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:07,463 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:14,344 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:15,434 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:15,466 - ERROR - [PDF 5] Error al procesar el PDF: list indices must be integers or slices, not str\n",
      "2025-08-12 14:14:15,472 - INFO - [PDF 5] Procesamiento del PDF completado\n",
      "2025-08-12 14:14:15,704 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:16,043 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:17,683 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:18,751 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:24,765 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:25,107 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:27,994 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:29,996 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:31,583 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:32,336 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:33,662 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:36,660 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:38,786 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:39,953 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:41,011 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:42,614 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:43,495 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:49,558 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:50,647 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:51,794 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:52,052 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:52,834 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:14:59,300 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:00,390 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:01,548 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:03,802 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:05,513 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:09,507 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:09,640 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:10,683 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:13,297 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:14,497 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:15,412 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:17,785 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:19,187 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:21,938 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:22,801 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:23,437 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:26,584 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:28,724 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:31,686 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:32,093 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:35,827 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:36,042 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:36,086 - ERROR - [PDF 2] Error al procesar el PDF: list indices must be integers or slices, not str\n",
      "2025-08-12 14:15:36,093 - INFO - [PDF 2] Procesamiento del PDF completado\n",
      "2025-08-12 14:15:36,814 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:38,154 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:40,370 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:40,449 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:44,616 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:47,839 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:48,181 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:49,154 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:54,025 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:57,191 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:58,042 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:15:59,249 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:02,147 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:02,168 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:03,316 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:06,897 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:08,027 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:09,474 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:12,495 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:18,005 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:18,714 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:20,459 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:22,218 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:24,165 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:26,121 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:29,834 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:30,016 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:33,868 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:35,209 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:35,909 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:36,452 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:36,718 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:37,608 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating conversation with OpenAI gpt-4o-mini: 1 validation error for Conversation\n",
      "  Invalid JSON: EOF while parsing a string at line 9 column 1567 [type=json_invalid, input_value='{\\n  \"messages\": [\\n    ...ntecedentes del cliente', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:16:40,594 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:43,853 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:46,621 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:49,024 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:51,716 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:52,256 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:52,481 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:53,798 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:54,295 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:57,569 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:16:58,989 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:02,882 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:03,034 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:07,123 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:09,798 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:10,798 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:11,897 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:13,187 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:15,762 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:19,921 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:21,579 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:23,312 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:23,928 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:28,321 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:28,958 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:31,600 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:34,533 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:35,289 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:35,711 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:36,885 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:41,602 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:42,922 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:48,295 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:50,069 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:50,326 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:51,710 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:51,980 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:53,613 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:54,850 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:17:58,168 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:18:01,710 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:18:05,558 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:18:06,632 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:18:07,419 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:18:07,878 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:18:10,180 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:18:12,877 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:18:13,241 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:18:17,376 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:18:18,216 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:18:20,872 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:18:21,872 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:18:25,276 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:18:28,049 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:18:29,136 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:18:33,154 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:18:36,298 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:18:53,903 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:18:59,756 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:19:04,398 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:19:10,435 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:19:14,771 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-12 14:19:20,205 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "files = get_files()\n",
    "generate(files,max_workers=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a4ebf-86b7-4ce0-a89a-975ee4a86092",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79e48dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.11/site-packages (4.0.0)\n",
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.11/site-packages (0.34.3)\n",
      "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.11/site-packages (8.1.7)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.11/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.11/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in ./.venv/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (1.1.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (9.4.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./.venv/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade datasets huggingface_hub ipywidgets pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1685376-51e6-4287-a74e-26a1e3deffb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/fine-tunning-models/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading data: 100%|██████████| 64/64 [00:00<00:00, 374909.85files/s]\n",
      "Generating train split: 4682 examples [00:00, 17601.63 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages', 'topic'],\n",
       "        num_rows: 4682\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"./outputs/data/*.jsonl\")\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "151e67bb-0111-4991-b87d-e724b183ea9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 55.02ba/s]\n",
      "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            \n",
      "\u001b[A\n",
      "Processing Files (0 / 1)                :  14%|█▎        |  524kB / 3.86MB, 1.31MB/s  \n",
      "Processing Files (0 / 1)                :  27%|██▋       | 1.05MB / 3.86MB, 1.75MB/s  \n",
      "Processing Files (0 / 1)                :  95%|█████████▌| 3.67MB / 3.86MB, 4.59MB/s  \n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████| 3.86MB / 3.86MB, 3.21MB/s  \n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████| 3.86MB / 3.86MB, 2.76MB/s  \n",
      "New Data Upload                         : 100%|██████████| 3.86MB / 3.86MB, 2.76MB/s  \n",
      "                                        : 100%|██████████| 3.86MB / 3.86MB            \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.87s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/jeanmcm/b_risks/commit/6f625bb43e6b588893a70d8769613ed11130179a', commit_message='Upload dataset', commit_description='', oid='6f625bb43e6b588893a70d8769613ed11130179a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/jeanmcm/b_risks', endpoint='https://huggingface.co', repo_type='dataset', repo_id='jeanmcm/b_risks'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "hf_token = os.environ.get(\"HUGGING_FACE_KEY\")\n",
    "dataset.push_to_hub(\"jeanmcm/b_risks\",token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f0891",
   "metadata": {},
   "source": [
    "# Testing RAG with Open WebUI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ce0bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Respuesta en streaming:</b> El riesgo financiero se refiere a las posibles pérdidas o daños económicos que pueden afectar a una empresa, individuo u organización debido a sus actividades financieras. Esto puede incluir la incertidumbre sobre los rendimientos de los activos financieros, el valor de los bienes y servicios, o la capacidad de un individuo o organización para generar ingresos.\n",
       "\n",
       "En el contexto del sistema financiero, las instituciones deben conocer a fondo las características particulares de las actividades económicas de sus clientes y las operaciones que realizan para identificar y gestionar los riesgos financieros asociados. Esto es fundamental para prevenir la ocurrencia de incumplimientos en las regulaciones financieras y minimizar el impacto de cualquier pérdida o daño.\n",
       "\n",
       "Según el artículo 43 del \"Bsoft Documents\", las instituciones del sistema financiero deben contar con una unidad de cumplimiento dirigida por un oficial de cumplimiento, que debe tener formación profesional en áreas como administración, contaduría, derecho o economía. Este equipo es responsable de garantizar que las políticas y procedimientos financieros sean adecuados y cumplan con los requisitos legales.\n",
       "\n",
       "Por lo tanto, el riesgo financiero se considera un aspecto crítico para las instituciones del sistema financiero, ya que puede tener graves consecuencias en su estabilidad y solvencia. Es importante que estas entidades tomen medidas proactivas para identificar, evaluar y gestionar los riesgos financieros asociados con sus actividades y operaciones.\n",
       "\n",
       "No se incluye ninguna cita porque no hay un id atributo en el source tag correspondiente a esta información, pero la respuesta se basa en el contexto proporcionado.."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "from IPython.display import display, clear_output, HTML\n",
    "\n",
    "url = \"https://vwlppjjfa98c9x-8080.proxy.runpod.net\"\n",
    "api_key =\"sk-05568562f28844fe997cadf960a346cd\"\n",
    "\n",
    "messages =  [{\"role\": \"user\", \"content\": \"Que es el Riesgo Financiero?\"}]\n",
    "\n",
    "try:\n",
    "    # Realizar la solicitud con stream=True\n",
    "    with requests.post(f\"{url}/api/chat/completions\", stream=True,headers={\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"Authorization\": f\"Bearer {api_key}\"\n",
    "    },json={\n",
    "      \"model\":\"bosft-riesgos-rag-model\",\n",
    "      \"messages\":messages,\n",
    "      \"stream\":True\n",
    "      }) as response:\n",
    "        response.raise_for_status()\n",
    "        # Variable para almacenar la salida acumulada\n",
    "        accumulated_output = \"\"\n",
    "\n",
    "        # Iterar sobre las líneas de la respuesta\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                # Decodificar la línea\n",
    "                decoded_line = line.decode('utf-8').strip()\n",
    "                # Si la línea comienza con \"data:\", extraer el contenido\n",
    "                if decoded_line.startswith(\"data:\"):\n",
    "                    decoded_line = decoded_line[5:].strip()  # Quitar \"data: \"\n",
    "\n",
    "                # Ignorar líneas vacías o marcadores de fin como \"[DONE]\"\n",
    "                if not decoded_line or decoded_line == \"[DONE]\" :\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    # Parsear si es JSON\n",
    "                    data = json.loads(decoded_line)\n",
    "                    if \"choices\" not in data: continue\n",
    "                    \n",
    "                    delta = data['choices'][0]['delta']\n",
    "                    if \"content\" in delta: new_data = delta['content']\n",
    "                except json.JSONDecodeError:\n",
    "                    # Si no es JSON, usar la línea como texto\n",
    "                    new_data = decoded_line\n",
    "\n",
    "                # Acumular y mostrar la salida dinámicamente\n",
    "                if new_data:\n",
    "                    accumulated_output += new_data\n",
    "                    # Limpiar la salida anterior y mostrar la nueva\n",
    "                    clear_output(wait=True)\n",
    "                    display(HTML(f\"<b>Respuesta en streaming:</b> {accumulated_output}\"))\n",
    "                    time.sleep(0.1)  # Pequeña pausa para visibilidad\n",
    "                    \n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"\\nError en la solicitud: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e64806a-ac56-417a-8dd9-175c3a8447a1",
   "metadata": {},
   "source": [
    "# Testing with Flowise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797cb1c0-bb15-4c97-9e0c-0156e8b90df1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
