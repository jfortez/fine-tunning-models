{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "067529bb",
   "metadata": {},
   "source": [
    "# Generacion de Datasets utilizando RLHF para fine tunning\n",
    "\n",
    "## Procedimiento\n",
    "\n",
    "- Recoleccion de Datos: Obtener Informacion mediante documentos de textos seleccionables (no escaneados), mediante web, redes sociales, scrapping,etc\n",
    "- Extraer Texto: Usa PyMuPDF para extraer texto de PDFs no escaneados, procesando en paralelo con ProcessPoolExecutor para manejar grandes volúmenes (>100, >10,000).\n",
    "- Limpiar Datos: Elimina espacios múltiples y caracteres no deseados con expresiones regulares, asegurando texto coherente.\n",
    "- Validar: Verifica que los fragmentos extraídos no estén vacíos y tengan una longitud mínima (por ejemplo, 50 caracteres).\n",
    "- Generar Conversaciones: Divide el texto en párrafos, usa un modelo como meta-llama/Llama-3.2-1B-Instruct para generar diálogos conversacionales específicos al contenido, con al menos 4 intercambios por conversación, en formato {\"messages\": [{\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}], \"topic\": \"...\"}.\n",
    "- Estructurar Dataset: Guarda las conversaciones en archivos JSONL con dos columnas: \"messages\" y \"topic\".\n",
    "Cargar y Subir: Carga el dataset con load_dataset(\"json\", data_files=\"path/*.jsonl\") y súbelo a un repositorio privado en Hugging Face Hub con push_to_hub(\"username/dataset_name\", private=True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45295ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in /workspace/venv/lib/python3.11/site-packages (1.26.3)\n",
      "Requirement already satisfied: nltk in /workspace/venv/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: ollama in /workspace/venv/lib/python3.11/site-packages (0.5.1)\n",
      "Collecting ollama\n",
      "  Downloading ollama-0.5.3-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: openai in /workspace/venv/lib/python3.11/site-packages (1.99.1)\n",
      "Collecting openai\n",
      "  Downloading openai-1.99.3-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: click in /workspace/venv/lib/python3.11/site-packages (from nltk) (8.2.2)\n",
      "Requirement already satisfied: joblib in /workspace/venv/lib/python3.11/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /workspace/venv/lib/python3.11/site-packages (from nltk) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in /workspace/venv/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: httpx>=0.27 in /workspace/venv/lib/python3.11/site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in /workspace/venv/lib/python3.11/site-packages (from ollama) (2.11.7)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /workspace/venv/lib/python3.11/site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /workspace/venv/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /workspace/venv/lib/python3.11/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /workspace/venv/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /workspace/venv/lib/python3.11/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /workspace/venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /workspace/venv/lib/python3.11/site-packages (from httpx>=0.27->ollama) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /workspace/venv/lib/python3.11/site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /workspace/venv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /workspace/venv/lib/python3.11/site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /workspace/venv/lib/python3.11/site-packages (from pydantic>=2.9->ollama) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /workspace/venv/lib/python3.11/site-packages (from pydantic>=2.9->ollama) (0.4.1)\n",
      "Downloading ollama-0.5.3-py3-none-any.whl (13 kB)\n",
      "Downloading openai-1.99.3-py3-none-any.whl (785 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.8/785.8 kB\u001b[0m \u001b[31m187.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: openai, ollama\n",
      "\u001b[2K  Attempting uninstall: openai\n",
      "\u001b[2K    Found existing installation: openai 1.99.1\n",
      "\u001b[2K    Uninstalling openai-1.99.1:━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [openai]\n",
      "\u001b[2K      Successfully uninstalled openai-1.99.1━━━━\u001b[0m \u001b[32m0/2\u001b[0m [openai]\n",
      "\u001b[2K  Attempting uninstall: ollama━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [openai]\n",
      "\u001b[2K    Found existing installation: ollama 0.5.10m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [ollama]\n",
      "\u001b[2K    Uninstalling ollama-0.5.1:90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [ollama]\n",
      "\u001b[2K      Successfully uninstalled ollama-0.5.1━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [ollama]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [ollama]2m1/2\u001b[0m [ollama]\n",
      "\u001b[1A\u001b[2KSuccessfully installed ollama-0.5.1 openai-1.99.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pymupdf nltk ollama openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fa351d6-c527-423f-9d2e-417d0ef2b8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4760f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OLLAMA_MODEL = \"llama3.1:8b\"\n",
    "TEMPERATURE = 0.7  # Equilibrio entre creatividad y coherencia\n",
    "TOP_P = 0.9  # Filtrado de núcleo para diversidad\n",
    "MIN_CONVERSATION_LENGTH = 3 \n",
    "\n",
    "# Constantes\n",
    "MIN_FRAGMENT_LENGTH = 500\n",
    "MAX_FRAGMENT_LENGTH = 2000\n",
    "REPEAT_THRESHOLD = 0.3\n",
    "MIN_BLOCK_LENGTH = 30  # Reducido de 50 a 30 para incluir bloques más cortos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62be033f",
   "metadata": {},
   "source": [
    "### Recoleccion de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b45b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Verificar si la carpeta existe\n",
    "\n",
    "def get_files():\n",
    "    folder_url = \"./docs\"\n",
    "    folder = Path(folder_url)\n",
    "\n",
    "\n",
    "    if not folder.exists() or not folder.is_dir():\n",
    "        print(\"Invalid Folder\")\n",
    "        \n",
    "    # Obtener todos los archivos de la carpeta\n",
    "    files = [f for f in folder.rglob(\"*\") if f.is_file()]\n",
    "\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce4f060",
   "metadata": {},
   "source": [
    "### Extraccion de Texto y Limpieza de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ed080bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import re\n",
    "import os\n",
    "import hashlib\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text, repeated_blocks=None):\n",
    "    \"\"\"Limpia el texto, eliminando caracteres repetitivos, texto redundante y caracteres no deseados.\"\"\"\n",
    "    # Eliminar caracteres de control no deseados (excepto \\n)\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]', '', text)\n",
    "    # Eliminar patrones repetitivos como ----, ...., ****\n",
    "    text = re.sub(r'([^\\w\\s])\\1{2,}|\\s*[.]{3,}\\s*', '', text)\n",
    "    # Normalizar espacios múltiples\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    # Eliminar espacios al inicio y final de cada línea\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    # Filtrar líneas duplicadas, números, correos y contenido administrativo\n",
    "    seen_lines = set()\n",
    "    unique_lines = []\n",
    "    for line in lines:\n",
    "        if line not in seen_lines and \\\n",
    "           not re.match(r'^\\d+$', line) and \\\n",
    "           not re.match(r'.*@(.*\\.)+.*', line) and \\\n",
    "           not re.match(r'^(Tel|Fax|E-mail|www\\.).*', line, re.IGNORECASE) and \\\n",
    "           (repeated_blocks is None or line not in repeated_blocks):\n",
    "            unique_lines.append(line)\n",
    "            seen_lines.add(line)\n",
    "    return '\\n'.join(unique_lines)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extrae texto de un PDF en una sola pasada, preservando el texto del inicio de la página,\n",
    "    eliminando encabezados, pies de página y contenido irrelevante,\n",
    "    dividiendo en fragmentos de 500 a 2000 caracteres con metadata corregida.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = pymupdf.open(pdf_path)\n",
    "        total_pages = len(doc)\n",
    "        chunks = []\n",
    "        current_chunk = []  # Lista de (párrafo, página)\n",
    "        current_chunk_length = 0\n",
    "        filename = os.path.basename(pdf_path)\n",
    "        block_counter = Counter()\n",
    "\n",
    "        for page_number in range(1, total_pages + 1):\n",
    "            page = doc[page_number - 1]\n",
    "            page_height = page.rect.height\n",
    "            blocks = page.get_text(\"blocks\")\n",
    "            page_text = []\n",
    "\n",
    "            # Procesar bloques y filtrar encabezados/pies de página\n",
    "            for block in blocks:\n",
    "                text = block[4]\n",
    "                y0, y1 = block[1], block[3]\n",
    "                # Excluir pies de página (parte inferior de la página)\n",
    "                if y1 > 0.95 * page_height:  # Relajado de 0.95 a 0.9\n",
    "                    continue\n",
    "                # Excluir encabezados solo si son repetitivos\n",
    "                if y0 < 0.05 * page_height:  # Relajado de 0.05 a 0.1\n",
    "                    block_counter[text] += 1\n",
    "                    if block_counter[text] > total_pages * REPEAT_THRESHOLD:\n",
    "                        continue\n",
    "                if text and len(text) >= MIN_BLOCK_LENGTH:\n",
    "                    block_counter[text] += 1\n",
    "                    page_text.append((text, page_number))\n",
    "\n",
    "            # Si no hay texto válido en la página, continuar\n",
    "            if not page_text:\n",
    "                continue\n",
    "\n",
    "            # Acumular párrafos con su número de página\n",
    "            for paragraph, page in page_text:\n",
    "                current_chunk.append((paragraph, page))\n",
    "                current_chunk_length += len(paragraph) + 2  # +2 por \"\\n\\n\"\n",
    "\n",
    "                # Si el fragmento alcanza la longitud mínima, procesarlo\n",
    "                if current_chunk_length >= MIN_FRAGMENT_LENGTH:\n",
    "                    chunk_text = \"\\n\\n\".join(p for p, _ in current_chunk)\n",
    "                    cleaned_chunk = clean_text(chunk_text, None)\n",
    "                    cleaned_length = len(cleaned_chunk)\n",
    "\n",
    "                    # Obtener el rango de páginas del fragmento\n",
    "                    page_numbers = sorted(set(page for _, page in current_chunk))\n",
    "                    start_page = page_numbers[0] if page_numbers else page_number\n",
    "                    end_page = page_numbers[-1] if page_numbers else page_number\n",
    "\n",
    "                    # Dividir fragmentos largos\n",
    "                    while cleaned_length > MAX_FRAGMENT_LENGTH:\n",
    "                        sub_chunk = cleaned_chunk[:MAX_FRAGMENT_LENGTH]\n",
    "                        last_paragraph_end = sub_chunk.rfind(\"\\n\\n\")\n",
    "                        if last_paragraph_end == -1:\n",
    "                            last_paragraph_end = MAX_FRAGMENT_LENGTH\n",
    "                        chunk_to_add = cleaned_chunk[:last_paragraph_end].strip()\n",
    "\n",
    "                        # Calcular el número de páginas para el subfragmento\n",
    "                        chars_so_far = 0\n",
    "                        sub_chunk_pages = []\n",
    "                        for paragraph, page in current_chunk:\n",
    "                            chars_so_far += len(paragraph) + 2\n",
    "                            if chars_so_far <= last_paragraph_end:\n",
    "                                sub_chunk_pages.append(page)\n",
    "                            else:\n",
    "                                break\n",
    "                        sub_start_page = min(sub_chunk_pages) if sub_chunk_pages else page_number\n",
    "                        sub_end_page = max(sub_chunk_pages) if sub_chunk_pages else page_number\n",
    "\n",
    "                        metadata = (\n",
    "                            f\"# FILENAME: {filename} | CHARACTERS: {len(chunk_to_add)} | \"\n",
    "                            f\"PAGES: {sub_start_page}-{sub_end_page}/{total_pages}\\n\\n\"\n",
    "                        )\n",
    "                        chunks.append((metadata + chunk_to_add, hashlib.md5(chunk_to_add.encode()).hexdigest()))\n",
    "                        cleaned_chunk = cleaned_chunk[last_paragraph_end:].strip()\n",
    "                        cleaned_length = len(cleaned_chunk)\n",
    "\n",
    "                        # Actualizar current_chunk para los párrafos restantes\n",
    "                        remaining_chunk = []\n",
    "                        chars_so_far = 0\n",
    "                        for paragraph, page in current_chunk:\n",
    "                            chars_so_far += len(paragraph) + 2\n",
    "                            if chars_so_far > last_paragraph_end:\n",
    "                                remaining_chunk.append((paragraph, page))\n",
    "                        current_chunk = remaining_chunk\n",
    "                        current_chunk_length = cleaned_length\n",
    "                        page_numbers = sorted(set(page for _, page in current_chunk))\n",
    "                        start_page = page_numbers[0] if page_numbers else page_number\n",
    "\n",
    "                    # Añadir el fragmento completo\n",
    "                    if cleaned_length >= MIN_FRAGMENT_LENGTH:\n",
    "                        metadata = (\n",
    "                            f\"# FILENAME: {filename} | CHARACTERS: {cleaned_length} | \"\n",
    "                            f\"PAGES: {start_page}-{end_page}/{total_pages}\\n\\n\"\n",
    "                        )\n",
    "                        chunks.append((metadata + cleaned_chunk, hashlib.md5(cleaned_chunk.encode()).hexdigest()))\n",
    "                        current_chunk = []\n",
    "                        current_chunk_length = 0\n",
    "\n",
    "                    else:\n",
    "                        current_chunk = [(cleaned_chunk, page_numbers[-1])] if page_numbers else []\n",
    "                        current_chunk_length = cleaned_length\n",
    "\n",
    "        # Añadir el fragmento final si cumple con la longitud mínima\n",
    "        if current_chunk and current_chunk_length >= MIN_FRAGMENT_LENGTH:\n",
    "            chunk_text = \"\\n\\n\".join(p for p, _ in current_chunk)\n",
    "            cleaned_chunk = clean_text(chunk_text, None)\n",
    "            cleaned_length = len(cleaned_chunk)\n",
    "            if cleaned_length >= MIN_FRAGMENT_LENGTH:\n",
    "                page_numbers = sorted(set(page for _, page in current_chunk))\n",
    "                start_page = page_numbers[0] if page_numbers else total_pages\n",
    "                end_page = page_numbers[-1] if page_numbers else total_pages\n",
    "                metadata = (\n",
    "                    f\"# FILENAME: {filename} | CHARACTERS: {cleaned_length} | \"\n",
    "                    f\"PAGES: {start_page}-{end_page}/{total_pages}\\n\\n\"\n",
    "                )\n",
    "                chunks.append((metadata + cleaned_chunk, hashlib.md5(cleaned_chunk.encode()).hexdigest()))\n",
    "\n",
    "        doc.close()\n",
    "\n",
    "        # Filtrar bloques repetitivos y duplicados\n",
    "        repeated_blocks = {text for text, count in block_counter.items() if count > total_pages * REPEAT_THRESHOLD}\n",
    "        final_chunks = []\n",
    "        seen_hashes = set()\n",
    "\n",
    "        for chunk, chunk_hash in chunks:\n",
    "            chunk_text = '\\n'.join(line for line in chunk.splitlines() if not line.startswith('#'))\n",
    "            cleaned_chunk = clean_text(chunk_text, repeated_blocks)\n",
    "            if len(cleaned_chunk) >= MIN_FRAGMENT_LENGTH and chunk_hash not in seen_hashes:\n",
    "                # Actualizar la longitud en la metadata después de la limpieza final\n",
    "                metadata_lines = chunk.splitlines()[0]\n",
    "                metadata = re.sub(r'CHARACTERS: \\d+', f'CHARACTERS: {len(cleaned_chunk)}', metadata_lines)\n",
    "                final_chunks.append(f\"{metadata}\\n\\n{cleaned_chunk}\")\n",
    "                seen_hashes.add(chunk_hash)\n",
    "\n",
    "        return final_chunks if final_chunks else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {pdf_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a912cd27",
   "metadata": {},
   "source": [
    "### Generar Conversaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c7a947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "class Message(BaseModel):\n",
    "    role: str = Field(..., pattern=\"^(user|assistant)$\")\n",
    "    content: str\n",
    "\n",
    "class Conversation(BaseModel):\n",
    "    messages: List[Message] = Field(..., min_items=MIN_CONVERSATION_LENGTH)\n",
    "    topic: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "82198409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompts(fragment: str):\n",
    "  INSTRUCTIONS = \"\"\"Eres un generador de conversaciones optimizadas para entrenamiento supervisado (SFT) con técnicas avanzadas de prompt engineering. A partir de un fragmento de texto, construye una conversación natural, coherente y estructurada entre un usuario y un asistente para entrenar modelos como LLaMA 3.1 o GPT-4o-mini usando `SFTTrainer` de Hugging Face. El objetivo es mejorar la comprensión lectora, coherencia y respuestas naturales, incluso si el fragmento es incompleto o incoherente.\n",
    "\n",
    "<instrucciones>\n",
    "\n",
    "1. **Formato de salida**:\n",
    "- Devuelve un JSON válido con:\n",
    "  - `messages`: Lista de mensajes con estructura { \"role\": \"user\" | \"assistant\", \"content\": \"...\" }. **No incluir mensajes con rol \"system\" en la salida.**\n",
    "  - `topic`: Cadena corta que resume el contenido (máx. 50 caracteres).\n",
    "\n",
    "2. **Estructura de la conversación**:\n",
    "- Genera 2 a 4 intercambios (4 a 8 mensajes) para mayor profundidad.\n",
    "- Empieza con \"user\" y alterna roles.\n",
    "- Mensajes largos, fluidos, humanos, evitando respuestas genéricas.\n",
    "- Usa chain-of-thought en respuestas del asistente para desglosar razonamientos.\n",
    "- Aplica patrones de prompts de la lista PROMPT_PATTERNS, seleccionando el más adecuado según el fragmento y variando para diversificar las interacciones.\n",
    "\n",
    "3. **Usuario**:\n",
    "- Evita frases como “según el texto”, “en el fragmento”, “de qué trata”.\n",
    "- Mensajes deben incluir:\n",
    "  - Preguntas inferenciales, técnicas, abiertas, resúmenes, cartas, o solicitudes de acción basadas en PROMPT_PATTERNS.\n",
    "  - Hipótesis, afirmaciones intuitivas o contexto adicional.\n",
    "- Estilo conversacional, curioso, humano, variando tonos (curioso, preocupado, analítico, formal, creativo).\n",
    "\n",
    "4. **Asistente**:\n",
    "- Responde con claridad, precisión y detalle, sin inventar información fuera del fragmento.\n",
    "- Usa chain-of-thought: explica el razonamiento paso a paso.\n",
    "- Integra contexto adicional del usuario, manteniendo coherencia.\n",
    "- Si el fragmento es incoherente, genera una conversación coherente usando la información disponible, haciendo suposiciones razonables y explícalas.\n",
    "- Respuestas deben ser útiles, naturales y entrenables para SFT.\n",
    "- **Formato Markdown**: Usa Markdown en las respuestas del asistente (encabezados, listas, texto en negritas, cursivas, etc.) para estructurar la información de forma clara y visualmente organizada, especialmente en respuestas largas.\n",
    "\n",
    "5. **Técnicas de Prompt Engineering**:\n",
    "- **Few-shot learning**: Incluye un ejemplo por cada patrón en PROMPT_PATTERNS con fragmentos explícitos (cortos y largos).\n",
    "- **Chain-of-thought**: Desglosa el razonamiento en pasos claros.\n",
    "- **Diversidad**: Varía tono, complejidad y patrones de prompts.\n",
    "- **Robustez**: Maneja fragmentos ambiguos o incoherentes con claridad.\n",
    "- **Coherencia mejorada**: Valida consistencia con el fragmento y contexto del usuario.\n",
    "\n",
    "<PROMPT_PATTERNS>\n",
    "- \"Me intriga {tema}. ¿Puedes explicármelo detalladamente?\"\n",
    "- \"En pocas palabras, resúmeme cómo funciona {tema}.\"\n",
    "- \"Crea una carta formal sobre {tema} dirigida a {destinatario}.\"\n",
    "- \"Basado en {contexto}, explícame qué significa {tema}.\"\n",
    "- \"Quiero mejorar en {tema}. ¿Qué consejos prácticos me das?\"\n",
    "- \"Agrega una sección sobre {tema} a este informe: {contexto}.\"\n",
    "- \"No entiendo {tema}. ¿Puedes desglosarlo en pasos simples?\"\n",
    "- \"Imagina que soy nuevo en {tema}. ¿Cómo me lo explicarías?\"\n",
    "- \"Podrías darme un resumen sobre {tema} y sus implicaciones?\"\n",
    "- \"Estoy preocupado por {tema}. ¿Qué debería saber al respecto?\"\n",
    "- \"Explícame {tema} como si fuera un niño de 10 años.\"\n",
    "- \"Redacta un correo informal sobre {tema} para un colega.\"\n",
    "</PROMPT_PATTERNS>\n",
    "\"\"\"\n",
    "\n",
    "  EXAMPLES = \"\"\"\n",
    "<ejemplos>\n",
    "\n",
    "**Ejemplo 1 — Técnico (dispersión de luz, patrón: explicación detallada, fragmento largo)**:\n",
    "<fragment>\n",
    "La dispersión de Rayleigh explica por qué el cielo aparece azul durante el día. Las partículas en la atmósfera dispersan preferentemente las longitudes de onda cortas, como el azul, debido a su interacción con la luz solar. Durante el atardecer, el cielo adquiere tonos rojos porque la luz atraviesa más atmósfera, dispersando el azul y dejando predominar las longitudes de onda largas. Este fenómeno depende de la densidad atmosférica y el ángulo de incidencia de la luz solar.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Me intriga por qué el cielo es azul de día y rojo al atardecer. ¿Puedes explicármelo detalladamente?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"# Dispersión de Rayleigh y el Color del Cielo\\n\\nPara entender por qué el cielo cambia de color, desglosemos el fenómeno paso a paso:\\n\\n1. **Composición de la luz solar**: La luz del sol parece blanca, pero está formada por todas las longitudes de onda visibles, desde el rojo (largo) hasta el violeta (corto).\\n\\n2. **Dispersión de Rayleigh**: En la atmósfera, las partículas (como moléculas de nitrógeno y oxígeno) dispersan la luz. La dispersión es más intensa para longitudes de onda cortas, como el azul, porque es inversamente proporcional a la **cuarta potencia** de la longitud de onda. Esto significa que el azul se esparce mucho más que el rojo.\\n\\n3. **Cielo diurno**: Durante el día, la luz solar atraviesa una capa relativamente delgada de atmósfera. El azul se dispersa en todas direcciones, haciendo que el cielo se vea azul. Aunque el violeta se dispersa más, nuestros ojos son menos sensibles a él, y parte es absorbida, por lo que el azul domina.\\n\\n4. **Atardecer**: Al atardecer, el sol está más bajo, y la luz atraviesa una capa más gruesa de atmósfera. Esto dispersa casi todo el azul, permitiendo que las longitudes de onda largas (rojo, naranja) lleguen a nuestros ojos, dando tonos cálidos.\\n\\n5. **Factores adicionales**: La densidad atmosférica y el ángulo de incidencia afectan la intensidad del fenómeno. Por ejemplo, más partículas (como polvo) pueden intensificar los colores del atardecer.\\n\\nEn resumen, la dispersión de Rayleigh explica el azul diurno y los tonos rojos del atardecer, influenciada por la física de la luz y la geometría de la Tierra.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"¿Por qué a veces el cielo se ve más violeta en ciertos atardeceres?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"A veces, el cielo puede parecer violeta en el atardecer por la presencia de partículas adicionales, como polvo o contaminación, que dispersan más longitudes de onda cortas, incluyendo el violeta. Además, si la atmósfera está muy limpia y el sol está en un ángulo específico, el violeta puede ser más visible porque menos se absorbe. Sin embargo, nuestros ojos suelen percibir más el azul, así que el violeta es menos común.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Dispersión de luz y color del cielo\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 2 — Técnico (dispersión de luz, patrón: resumen breve, fragmento corto)**:\n",
    "<fragment>\n",
    "La dispersión de Rayleigh hace que el cielo sea azul al dispersar más las longitudes de onda cortas.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"En pocas palabras, resúmeme cómo funciona la dispersión de luz en el cielo.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"La dispersión de Rayleigh ocurre cuando las partículas atmosféricas dispersan más las longitudes de onda cortas, como el azul, haciendo que el cielo se vea azul durante el día.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Dispersión de luz en el cielo\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 3 — Contextual (política monetaria, patrón: carta formal, fragmento largo)**:\n",
    "<fragment>\n",
    "Subir las tasas de interés es una herramienta del banco central para controlar la inflación. Al encarecer los préstamos, se reduce el consumo y la inversión, lo que desacelera la economía y disminuye la presión sobre los precios. Sin embargo, un aumento excesivo puede llevar a una recesión, afectando el empleo y el crecimiento económico. Los bancos centrales deben monitorear indicadores como el consumo y el desempleo para ajustar las tasas cuidadosamente.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Crea una carta formal sobre el impacto de las tasas de interés, dirigida al director de mi banco.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"**Asunto**: Preocupación por el Impacto de las Tasas de Interés\\n\\nEstimado Director,\\n\\nMe dirijo a usted para expresar mi preocupación sobre el reciente aumento de las tasas de interés y su impacto en los clientes. Entiendo que el banco central sube las tasas para controlar la inflación, encareciendo los préstamos, lo que reduce el consumo y la presión sobre los precios. Sin embargo, esto afecta directamente a los consumidores, elevando el costo de hipotecas y créditos, y podría desencadenar una recesión si las tasas suben demasiado.\\n\\nSolicito información sobre cómo el banco planea apoyar a los clientes en este contexto, como opciones de refinanciamiento o asesoramiento financiero. Espero su respuesta en un plazo de 7 días hábiles.\\n\\nAtentamente,  \\n[Nombre del Cliente]\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Agrega una mención sobre el impacto en las pequeñas empresas.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"**Asunto**: Preocupación por el Impacto de las Tasas de Interés\\n\\nEstimado Director,\\n\\nMe dirijo a usted para expresar mi preocupación sobre el aumento de las tasas de interés. Estas medidas, destinadas a controlar la inflación al encarecer los préstamos, reducen el consumo y la inversión, afectando tanto a los consumidores como a las pequeñas empresas, que enfrentan mayores costos de financiamiento. Esto podría desencadenar una recesión, impactando el empleo y el crecimiento económico.\\n\\nSolicito información sobre cómo el banco apoyará a los clientes, incluyendo a pequeñas empresas, con opciones como refinanciamiento o asesoramiento financiero. Espero su respuesta en 7 días hábiles.\\n\\nAtentamente,  \\n[Nombre del Cliente]\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Impacto de tasas de interés\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 4 — Incoherente (fragmento ambiguo, patrón: contexto adicional, fragmento corto)**:\n",
    "<fragment>\n",
    "...fluctuaciones de energía... inestabilidad...\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Basado en problemas con el parpadeo de luces en mi casa, explícame qué significan fluctuaciones de energía.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Las fluctuaciones de energía son cambios en el voltaje eléctrico, causando inestabilidad como el parpadeo de luces. Pueden deberse a cableado defectuoso o sobrecarga en la red. Usa un regulador de voltaje o consulta a un electricista.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Fluctuaciones de energía\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 5 — Casual (salud visual, patrón: consejos prácticos, fragmento largo)**:\n",
    "<fragment>\n",
    "La exposición prolongada a la luz azul de pantallas puede interrumpir la producción de melatonina, afectando el ciclo de sueño. Esto ocurre porque la luz azul engaña al cerebro, haciéndole creer que es de día, lo que retrasa el inicio del sueño. También puede causar fatiga visual y molestias como sequedad ocular.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Quiero mejorar mi sueño porque paso mucho tiempo frente a pantallas. ¿Qué consejos prácticos me das?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"# Mejorar el Sueño y Proteger la Vista frente a Pantallas\\n\\nLa luz azul de las pantallas puede afectar tu sueño y ojos, pero hay formas prácticas de mitigar esto. Aquí van algunos consejos:\\n\\n- **Usa el modo nocturno**: Activa el filtro de luz azul en tus dispositivos para reducir su emisión, especialmente por la noche.\\n- **Evita pantallas antes de dormir**: Intenta no usar dispositivos al menos 1 hora antes de acostarte para que la melatonina, la hormona del sueño, se produzca normalmente.\\n- **Aplica la regla 20-20-20**: Cada 20 minutos, mira algo a 20 pies de distancia durante 20 segundos para descansar tus ojos y evitar fatiga visual.\\n- **Considera gafas con filtro**: Si trabajas mucho con pantallas, unas gafas con filtro de luz azul pueden reducir molestias como sequedad ocular.\\n- **Mantén un ambiente adecuado**: Usa iluminación suave en tu espacio de trabajo para reducir el contraste con la pantalla.\\n\\n*Por qué funciona*: La luz azul engaña al cerebro, haciéndole creer que es de día, lo que inhibe la melatonina y retrasa el sueño. Estos pasos ayudan a proteger tu ciclo de sueño y la salud de tus ojos.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"¿Qué hago si no puedo evitar las pantallas por trabajo?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Si no puedes evitar pantallas, refuerza las medidas: usa siempre el modo nocturno, haz pausas frecuentes con la regla 20-20-20, y considera gafas con filtro de luz azul. También ajusta el brillo de la pantalla para que sea similar al de tu entorno y evita trabajar en completa oscuridad.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Luz azul y sueño\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 6 — Contextual (criptografía, patrón: informe, fragmento largo)**:\n",
    "<fragment>\n",
    "La criptografía de clave pública utiliza un par de claves: una pública para cifrar y una privada para descifrar. Esto asegura que solo el destinatario con la clave privada pueda acceder al mensaje, garantizando confidencialidad y autenticidad. Es ampliamente usada en transacciones seguras en línea.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Agrega una sección sobre criptografía de clave pública a este informe: 'Seguridad en transacciones digitales'.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"## Criptografía de Clave Pública en Transacciones Digitales\\n\\nLa **criptografía de clave pública** es un pilar fundamental para la seguridad en transacciones digitales. Funciona mediante un par de claves:\\n\\n- **Clave pública**: Se comparte libremente y permite cifrar mensajes, convirtiéndolos en texto ilegible.\\n- **Clave privada**: Solo la posee el destinatario y permite descifrar el mensaje.\\n\\nEste mecanismo asegura:\\n\\n1. **Confidencialidad**: Solo el destinatario con la clave privada puede leer el mensaje.\\n2. **Autenticidad**: Si el remitente firma con su clave privada, el receptor verifica su identidad con la clave pública.\\n\\n*Ejemplo práctico*: En compras online, la criptografía de clave pública protege los datos de tu tarjeta, asegurando que solo el vendedor autorizado los descifre. Es esencial para protocolos como HTTPS y sistemas de pago seguro.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Criptografía en transacciones\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 7 — Técnico (cambio climático, patrón: pasos simples, fragmento corto)**:\n",
    "<fragment>\n",
    "El cambio climático aumenta las temperaturas globales debido a los gases de efecto invernadero.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"No entiendo el cambio climático. ¿Puedes desglosarlo en pasos simples?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"1. Quemamos combustibles como carbón y petróleo.\\n2. Esto libera gases de efecto invernadero, como el dióxido de carbono.\\n3. Los gases atrapan calor en la atmósfera.\\n4. El planeta se calienta, causando cambio climático.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Cambio climático explicado\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 8 — Casual (nutrición, patrón: explicación para principiantes, fragmento largo)**:\n",
    "<fragment>\n",
    "El consumo excesivo de azúcar puede aumentar el riesgo de diabetes tipo 2 y obesidad. Reducir la ingesta de bebidas azucaradas y alimentos procesados es clave para una dieta saludable. Los azúcares añadidos en productos como refrescos, dulces y salsas comerciales suelen ser los principales culpables.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Imagina que soy nuevo en nutrición. ¿Cómo me explicarías lo del azúcar?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Piénsalo como gasolina para tu cuerpo: el azúcar te da energía, pero demasiada puede hacerte daño. Hay azúcar natural, como en las frutas, que está bien en moderación. Pero el problema es el **azúcar añadido**, que encuentras en refrescos, dulces o salsas. Comer mucho de esto puede subir tu peso y aumentar el riesgo de diabetes tipo 2, porque tu cuerpo lucha para manejar tanto azúcar. Para comer mejor, cambia refrescos por agua, elige snacks como nueces y lee etiquetas para evitar productos con azúcares ocultos.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Azúcar y salud\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 9 — Incoherente (fragmento sin contexto, patrón: resumen e implicaciones, fragmento corto)**:\n",
    "<fragment>\n",
    "...error crítico en el núcleo...\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Podrías darme un resumen sobre un error crítico en el núcleo y sus implicaciones?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"# Error Crítico en el Núcleo\\n\\n**Resumen**: Un error crítico en el núcleo es un fallo grave en el sistema operativo, que actúa como el cerebro de tu computadora. Puede ocurrir por hardware defectuoso, software corrupto o malware.\\n\\n**Implicaciones**:\\n- **Pérdida de datos**: Los archivos pueden perderse si el sistema no guarda correctamente.\\n- **Interrupciones**: El sistema puede congelarse o reiniciarse.\\n- **Costos**: Reparaciones o recuperación de datos pueden ser necesarias.\\n\\n**Acción**: Usa software de recuperación como TestDisk y haz backups regulares para minimizar riesgos.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Error crítico en sistema\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 10 — Contextual (economía, patrón: preocupación, fragmento largo)**:\n",
    "<fragment>\n",
    "La inflación persistente puede erosionar el poder adquisitivo, haciendo que los bienes y servicios sean más caros. Los bancos centrales intentan controlarla con políticas monetarias, pero esto puede tener efectos secundarios en el empleo y el crecimiento económico.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Estoy preocupado por la inflación. ¿Qué debería saber al respecto?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"# Entendiendo la Inflación\\n\\nLa inflación ocurre cuando los precios de bienes y servicios suben, reduciendo tu **poder adquisitivo** (es decir, con el mismo dinero compras menos). Aquí está lo que debes saber:\\n\\n- **Causas**: Puede ser por alta demanda, costos de producción elevados o políticas monetarias expansivas.\\n- **Impacto**: Tus ahorros valen menos, y cosas como alimentos o gasolina se encarecen.\\n- **Control**: Los bancos centrales suben tasas de interés para reducir el gasto, pero esto puede ralentizar la economía, afectando empleos.\\n\\n**Qué hacer**:\\n- **Ahorra inteligentemente**: Invierte en activos que superen la inflación, como bonos ajustados.\\n- **Presupuesto**: Prioriza gastos esenciales y reduce los no necesarios.\\n- **Infórmate**: Sigue las políticas del banco central para anticipar cambios.\\n\\nHabla con un asesor financiero si quieres estrategias específicas.\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Inflación y poder adquisitivo\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 11 — Técnico (energía renovable, patrón: explicación para niño, fragmento corto)**:\n",
    "<fragment>\n",
    "La energía solar usa paneles para convertir la luz del sol en electricidad.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Explícame la energía solar como si fuera un niño de 10 años.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Imagina que el sol es como una lámpara gigante que brilla todo el día. La **energía solar** usa placas especiales, como espejos mágicos, que atrapan la luz del sol y la convierten en electricidad para encender luces, televisions o cargar tu tablet. ¡Es como usar el poder del sol para hacer cosas cool!\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Energía solar simplificada\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "**Ejemplo 12 — Casual (trabajo remoto, patrón: correo informal, fragmento largo)**:\n",
    "<fragment>\n",
    "El trabajo remoto puede aumentar la productividad, pero también presenta desafíos como la falta de interacción social y dificultades para desconectarse del trabajo. Herramientas como Zoom y Slack facilitan la colaboración, pero requieren una buena gestión del tiempo.\n",
    "</fragment>\n",
    "<response>\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Redacta un correo informal sobre trabajo remoto para un colega.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"**Asunto**: Tips para el Trabajo Remoto\\n\\nHola [Nombre],\\n\\nEstuve pensando en cómo sacarle provecho al trabajo desde casa. Puede ser súper productivo porque evitas el tráfico, pero a veces se siente raro sin las charlas en la oficina, ¿no? Herramientas como Zoom o Slack ayudan a mantenernos conectados, pero la clave está en organizarse bien. Intenta poner horarios fijos para trabajar y desconectarte, así no terminas respondiendo correos a medianoche. ¿Tú cómo lo llevas? ¡Cuéntame si tienes algún truco!\\n\\nSaludos,  \\n[Tu Nombre]\"\n",
    "    }\n",
    "  ],\n",
    "  \"topic\": \"Trabajo remoto y productividad\"\n",
    "}\n",
    "</response>\n",
    "\n",
    "</ejemplos>\n",
    "\"\"\"\n",
    "\n",
    "  system_content = f\"{INSTRUCTIONS}\\n{EXAMPLES}\"\n",
    "  return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_content\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": fragment\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e408c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_genterated_conv(conversation:Conversation):\n",
    "  # Validar que la conversación tenga al menos un intercambio completo\n",
    "  if len(conversation.messages) < 2:\n",
    "      print(f\"Error: Conversación inválida, número de mensajes insuficiente: {len(conversation.messages)}\")\n",
    "      return None\n",
    "  # Si el número de mensajes es impar, eliminar el último (asumiendo que es del usuario)\n",
    "  if len(conversation.messages) % 2 != 0 and len(conversation.messages) > 2:\n",
    "      conversation.messages = conversation.messages[:-1]\n",
    "      \n",
    "  return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e353ea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import AsyncClient\n",
    "\n",
    "ollama_client = AsyncClient()\n",
    "\n",
    "async def generate_ollama_conversations(messages):\n",
    "    try:\n",
    "        # Llama a Ollama con roles system y user\n",
    "        response = await ollama_client.chat(\n",
    "            model=OLLAMA_MODEL,\n",
    "            messages=messages,\n",
    "            options={\n",
    "                \"temperature\": TEMPERATURE,\n",
    "                \"top_p\": TOP_P,\n",
    "                \n",
    "                \n",
    "            },\n",
    "            format=Conversation.model_json_schema()  # Especifica el esquema JSON\n",
    "            \n",
    "        )\n",
    "        conversation = Conversation.model_validate_json(response.message.content)\n",
    "\n",
    "        return validate_genterated_conv(conversation)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generando conversación con Ollama: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "16928054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncClient, RateLimitError\n",
    "\n",
    "openai_client = AsyncClient(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "async def generate_openai_conversations(messages, model=\"gpt-4o-mini\"):\n",
    "    try:\n",
    "        response = await openai_client.responses.parse(\n",
    "            model=model,\n",
    "            input=messages,\n",
    "            text_format=Conversation\n",
    "        )\n",
    "        conversation = response.output_parsed\n",
    "        return validate_genterated_conv(conversation) if conversation else None\n",
    "    except RateLimitError as e:\n",
    "        print(f\"Rate limit hit for {model}: {e}\")\n",
    "        next_model = \"gpt-5-nano\"\n",
    "        if next_model != model:\n",
    "            return await generate_openai_conversations(messages, next_model)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating conversation with OpenAI {model}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab2c9167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "async def generate_conversation(fragment:str, provider: Literal[\"ollama\", \"openai\"]):\n",
    "    \"\"\"\n",
    "    Genera una conversación estructurada en formato JSON usando Ollama, basada en un fragmento de texto.\n",
    "    Optimizado para múltiples iteraciones en la generación de datasets para fine-tuning.\n",
    "    \"\"\"\n",
    "    if not fragment or len(fragment) < MIN_FRAGMENT_LENGTH:\n",
    "        print(f\"Error: Fragmento demasiado corto ({len(fragment)} caracteres).\")\n",
    "        return None\n",
    "\n",
    "    # Extraer el texto sin la metadata (líneas que comienzan con '#')\n",
    "    fragment_content = '\\n'.join(line for line in fragment.splitlines() if not line.startswith('#')).strip()\n",
    "    if len(fragment_content) < MIN_FRAGMENT_LENGTH:\n",
    "        print(f\"Error: Contenido útil del fragmento demasiado corto ({len(fragment_content)} caracteres).\")\n",
    "        return None\n",
    "\n",
    "    # Prompt optimizado para múltiples iteraciones\n",
    "    messages = get_prompts(fragment_content)\n",
    "\n",
    "    try:\n",
    "        if provider=='ollama':\n",
    "            return await generate_ollama_conversations(messages)\n",
    "        elif provider=='openai':\n",
    "            return await generate_openai_conversations(messages)\n",
    "        else: return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error generando conversación con Ollama: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "20c7088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configuración del logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('pdf_processing.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "43e9efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from filelock import FileLock\n",
    "import asyncio\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "OUTPUT_FOLDER = \"data\"\n",
    "\n",
    "class MetadataManager:\n",
    "    \"\"\"Clase para manejar archivos de metadatos JSON.\"\"\"\n",
    "    def __init__(self, index: int, output_dir: str):\n",
    "        self.folder = os.path.join(output_dir, \"metadata\")\n",
    "        self.metadata_file = os.path.join(self.folder, f\"metadata_{index:04d}.json\")\n",
    "        self.lock_file = f\"{self.metadata_file}.lock\"\n",
    "        self.index = index\n",
    "        os.makedirs(self.folder, exist_ok=True)\n",
    "\n",
    "    def exists(self) -> bool:\n",
    "        \"\"\"Verifica si el archivo de metadatos existe.\"\"\"\n",
    "        return os.path.exists(self.metadata_file)\n",
    "\n",
    "    def get(self, param: str):\n",
    "        \"\"\"Obtiene chunks, conversations, fileName, num_chunks, num_messages o num_exchanges desde el archivo de metadatos.\"\"\"\n",
    "        if not self.exists():\n",
    "            return [] if param in [\"chunks\", \"conversations\"] else 0 if param in [\"num_chunks\", \"num_messages\", \"num_exchanges\"] else \"\"\n",
    "        try:\n",
    "            with FileLock(self.lock_file):\n",
    "                with open(self.metadata_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    metadata = json.load(f)\n",
    "                \n",
    "                updated = False\n",
    "                if param == \"conversations\":\n",
    "                    conversations = metadata.get(\"conversations\", [])\n",
    "                    for i, conv in enumerate(conversations):\n",
    "                        if isinstance(conv, dict) and \"chunk_index\" not in conv:\n",
    "                            conv[\"chunk_index\"] = i\n",
    "                            updated = True\n",
    "                    conversations.sort(key=lambda x: x.get(\"chunk_index\", 0))\n",
    "                    metadata[\"conversations\"] = conversations\n",
    "                \n",
    "                if \"num_chunks\" not in metadata and \"chunks\" in metadata:\n",
    "                    metadata[\"num_chunks\"] = len(metadata.get(\"chunks\", []))\n",
    "                    updated = True\n",
    "                \n",
    "                if \"num_exchanges\" not in metadata and \"conversations\" in metadata:\n",
    "                    metadata[\"num_exchanges\"] = sum(len(conv[\"messages\"]) // 2 for conv in metadata.get(\"conversations\", []))\n",
    "                    updated = True\n",
    "                \n",
    "                if \"num_messages\" not in metadata and \"conversations\" in metadata:\n",
    "                    metadata[\"num_messages\"] = sum(len(conv[\"messages\"]) for conv in metadata.get(\"conversations\", []))\n",
    "                    updated = True\n",
    "                    \n",
    "                if \"total_conversations\" not in metadata and \"conversations\" in metadata:\n",
    "                    metadata[\"total_conversations\"] = len(conversations)\n",
    "                    updated = True\n",
    "                \n",
    "                if updated:\n",
    "                    with open(self.metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                        json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "                \n",
    "                return metadata.get(param, []) if param in [\"chunks\", \"conversations\"] else metadata.get(param, 0 if param in [\"num_chunks\", \"num_messages\", \"num_exchanges\"] else metadata.get(param,\"\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Error al leer {param} desde {self.metadata_file}: {e}\")\n",
    "            return [] if param in [\"chunks\", \"conversations\"] else 0 if param in [\"num_chunks\", \"num_messages\", \"num_exchanges\"] else \"\"\n",
    "\n",
    "    def set(self, param: str, value):\n",
    "        \"\"\"Establece chunks, conversations, fileName, num_chunks, num_messages o num_exchanges en el archivo de metadatos.\"\"\"\n",
    "        try:\n",
    "            with FileLock(self.lock_file):\n",
    "                metadata = {\"chunks\": [], \"fileName\": \"\", \"conversations\": [], \"num_chunks\": 0, \"num_messages\": 0, \"num_exchanges\": 0}\n",
    "                if self.exists():\n",
    "                    with open(self.metadata_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                        metadata = json.load(f)\n",
    "                \n",
    "                metadata[param] = value\n",
    "                if param == \"conversations\":\n",
    "                    metadata[\"conversations\"].sort(key=lambda x: x.get(\"chunk_index\", 0))\n",
    "                    metadata[\"num_messages\"] = sum(len(conv[\"messages\"]) for conv in metadata[\"conversations\"])\n",
    "                    metadata[\"num_exchanges\"] = sum(len(conv[\"messages\"]) // 2 for conv in metadata[\"conversations\"])\n",
    "                    metadata[\"total_conversations\"] = len(metadata[\"conversations\"])\n",
    "                if param == \"chunks\":\n",
    "                    metadata[\"num_chunks\"] = len(value)\n",
    "                with open(self.metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al escribir {param} en {self.metadata_file}: {e}\")\n",
    "\n",
    "    def append_conversation(self, conversation: dict):\n",
    "        \"\"\"Añade una conversación al array de conversaciones en el archivo de metadatos.\"\"\"\n",
    "        try:\n",
    "            with FileLock(self.lock_file):\n",
    "                metadata = {\"chunks\": [], \"fileName\": \"\", \"conversations\": [], \"num_chunks\": 0, \"num_messages\": 0, \"num_exchanges\": 0}\n",
    "                if self.exists():\n",
    "                    with open(self.metadata_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                        metadata = json.load(f)\n",
    "                \n",
    "                for i, conv in enumerate(metadata[\"conversations\"]):\n",
    "                    if isinstance(conv, dict) and \"chunk_index\" not in conv:\n",
    "                        conv[\"chunk_index\"] = i\n",
    "                \n",
    "                if \"num_chunks\" not in metadata:\n",
    "                    metadata[\"num_chunks\"] = len(metadata.get(\"chunks\", []))\n",
    "                \n",
    "                if \"num_exchanges\" not in metadata:\n",
    "                    metadata[\"num_exchanges\"] = sum(len(conv[\"messages\"]) // 2 for conv in metadata.get(\"conversations\", []))\n",
    "                \n",
    "                if \"num_messages\" not in metadata:\n",
    "                    metadata[\"num_messages\"] = sum(len(conv[\"messages\"]) for conv in metadata.get(\"conversations\", []))\n",
    "                \n",
    "                metadata[\"conversations\"].append(conversation)\n",
    "                metadata[\"conversations\"].sort(key=lambda x: x.get(\"chunk_index\", 0))\n",
    "                metadata[\"num_messages\"] = sum(len(conv[\"messages\"]) for conv in metadata[\"conversations\"])\n",
    "                metadata[\"num_exchanges\"] = sum(len(conv[\"messages\"]) // 2 for conv in metadata[\"conversations\"])\n",
    "                metadata[\"total_conversations\"] = len(metadata[\"conversations\"])\n",
    "                \n",
    "                with open(self.metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al añadir conversación a {self.metadata_file}: {e}\")\n",
    "\n",
    "def get_text_from_pdf(index: int, pdf_path: str, output_dir: str) -> list:\n",
    "    \"\"\"Obtiene el texto de un PDF desde la caché (JSON) o lo extrae si no existe.\"\"\"\n",
    "    metadata_manager = MetadataManager(index, output_dir)\n",
    "    \n",
    "    if metadata_manager.exists():\n",
    "        chunks = metadata_manager.get(\"chunks\")\n",
    "        file_name = metadata_manager.get(\"fileName\")\n",
    "        if chunks and file_name == os.path.basename(pdf_path):\n",
    "            return chunks\n",
    "    \n",
    "    try:\n",
    "        pages_text = extract_text_from_pdf(pdf_path)\n",
    "        if pages_text:\n",
    "            metadata_manager.set(\"chunks\", pages_text)\n",
    "            metadata_manager.set(\"fileName\", os.path.basename(pdf_path))\n",
    "            metadata_manager.set(\"conversations\", [])\n",
    "        return pages_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error al extraer texto del PDF {pdf_path}: {e}\")\n",
    "        return []\n",
    "    \n",
    "def get_conv_from_jsonl(index, output_dir):\n",
    "    metadata_manager = MetadataManager(index, output_dir)\n",
    "    jsonl_file = os.path.join(output_dir, OUTPUT_FOLDER, f\"pdf_{index:04d}.jsonl\")\n",
    "    if os.path.exists(jsonl_file):\n",
    "        try:\n",
    "            chunks = metadata_manager.get(\"chunks\")\n",
    "            with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                conversations = []\n",
    "                for i, line in enumerate(f):\n",
    "                    if line.strip():\n",
    "                        conv = json.loads(line)\n",
    "                        conv[\"source_chunk\"] = chunks[i]\n",
    "                        conv[\"chunk_index\"] = i\n",
    "                        conversations.append(conv)\n",
    "                conversations.sort(key=lambda x: x.get(\"chunk_index\", 0))\n",
    "                metadata_manager.set(\"conversations\", conversations)\n",
    "                metadata_manager.set(\"num_messages\", sum(len(conv[\"messages\"]) for conv in conversations))\n",
    "                metadata_manager.set(\"num_exchanges\", sum(len(conv[\"messages\"]) // 2 for conv in conversations))\n",
    "                metadata_manager.set(\"total_conversations\", len(conversations))\n",
    "                return conversations\n",
    "        except Exception as e:\n",
    "            print(f\"Error al leer conversaciones desde {jsonl_file}: {e}\")\n",
    "    return []\n",
    "\n",
    "async def get_conversation_from_chunk(index: int, output_dir: str, chunk: str, chunk_index: int, provider: Literal[\"ollama\", \"openai\"]):\n",
    "    \"\"\"Obtiene o genera una conversación para un fragmento de texto.\"\"\"\n",
    "    metadata_manager = MetadataManager(index, output_dir)\n",
    "    existing_conversations = metadata_manager.get(\"conversations\")\n",
    "    \n",
    "    if not existing_conversations:\n",
    "        existing_conversations = get_conv_from_jsonl(index, output_dir)\n",
    "    \n",
    "    for conv in existing_conversations:\n",
    "        if isinstance(conv, dict) and conv.get(\"source_chunk\") == chunk and conv.get(\"chunk_index\") == chunk_index:\n",
    "            return conv\n",
    "    try:\n",
    "        conversation = await generate_conversation(chunk, provider)\n",
    "        if conversation:\n",
    "            conv_dict = conversation.model_dump()\n",
    "            conv_dict[\"source_chunk\"] = chunk\n",
    "            conv_dict[\"chunk_index\"] = chunk_index\n",
    "            conv_dict[\"provider\"]= provider\n",
    "            from datetime import datetime\n",
    "            conv_dict[\"created_at\"] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            metadata_manager.append_conversation(conv_dict)\n",
    "            return conv_dict\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error al generar conversación para chunk_index {chunk_index} en PDF #{index} con {provider}: {e}\")\n",
    "        return None\n",
    "\n",
    "async def get_conv(index: int, output_dir: str, chunk: str, chunk_index: int, provider: Literal[\"ollama\", \"openai\"]):\n",
    "    result = await get_conversation_from_chunk(index, output_dir, chunk, chunk_index, provider)\n",
    "    return result if result is not None else []\n",
    "\n",
    "async def process_pdf(index: int, pdf_path: str, output_dir: str):\n",
    "    \"\"\"Procesa un PDF, genera conversaciones y las guarda en JSONL.\"\"\"\n",
    "    logger = logging.getLogger()\n",
    "    logger.info(f\"[PDF {index}] Procesando PDF: {Path(pdf_path).name}\")\n",
    "    try:\n",
    "        pages_text = get_text_from_pdf(index, pdf_path, output_dir)\n",
    "        \n",
    "        if not pages_text:\n",
    "            logger.warning(f\"[PDF {index}] No se encontraron fragmentos de texto\")\n",
    "            return\n",
    "        \n",
    "        data_dir = os.path.join(output_dir, OUTPUT_FOLDER)\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "        jsonl_file = os.path.join(data_dir, f\"pdf_{index:04d}.jsonl\")\n",
    "        \n",
    "        tasks = [(i, fragment) for i, fragment in enumerate(pages_text) if len(fragment) > 20]\n",
    "        if not tasks:\n",
    "            logger.warning(f\"[PDF {index}] No hay fragmentos válidos para procesar\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"[PDF {index}] Procesando {len(tasks)} fragmentos\")\n",
    "        \n",
    "        # Dividir tareas entre providers (50% openai, 50% ollama)\n",
    "        total_tasks = len(tasks)\n",
    "        half_tasks = total_tasks // 2\n",
    "        openai_tasks = [get_conv(index, output_dir, fragment, i, \"openai\") for i, fragment in tasks[:half_tasks]]\n",
    "        ollama_tasks = [get_conv(index, output_dir, fragment, i, \"ollama\") for i, fragment in tasks[half_tasks:]]\n",
    "        \n",
    "        # Ejecutar tareas en paralelo con límite de concurrencia\n",
    "        from asyncio import Semaphore\n",
    "        async def limited_gather(tasks, limit=10):\n",
    "            semaphore = Semaphore(limit)\n",
    "            async def sem_task(task):\n",
    "                async with semaphore:\n",
    "                    return await task\n",
    "            return await asyncio.gather(*[sem_task(task) for task in tasks], return_exceptions=True)\n",
    "        \n",
    "        task_results = await limited_gather(openai_tasks + ollama_tasks)\n",
    "        \n",
    "        if task_results:\n",
    "            valid_conversations = [conv for conv in task_results if conv is not None and conv is not TypeError]\n",
    "            if valid_conversations:\n",
    "                valid_conversations.sort(key=lambda x: x[\"chunk_index\"])\n",
    "                \n",
    "                actual_indices = [conv[\"chunk_index\"] for conv in valid_conversations]\n",
    "                if actual_indices != sorted(actual_indices):\n",
    "                    logger.warning(f\"[PDF {index}] El orden de las conversaciones no coincide con el esperado\")\n",
    "                \n",
    "                with open(jsonl_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    output_str = \"\\n\".join(json.dumps({\"messages\": conv[\"messages\"], \"topic\": conv[\"topic\"]}, ensure_ascii=False) \n",
    "                                        for conv in valid_conversations)\n",
    "                    f.write(output_str + \"\\n\")\n",
    "                    logger.info(f\"[PDF {index}] Dataset conversacional generado y guardado en JSONL\")\n",
    "            else:\n",
    "                logger.warning(f\"[PDF {index}] No se generaron conversaciones\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"[PDF {index}] Error al procesar el PDF: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2fdbb014-2ca2-4f7f-bdfe-1430423e7d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import asyncio\n",
    "import os\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "\n",
    "def process_pdf_wrapper(args):\n",
    "    index, pdf_path, output_dir = args\n",
    "    logger = logging.getLogger()\n",
    "    logger.info(f\"[PDF {index}] Inicio del procesamiento del PDF: {Path(pdf_path).name}\")\n",
    "    try:\n",
    "        asyncio.run(process_pdf(index, pdf_path, output_dir))\n",
    "        logger.info(f\"[PDF {index}] Procesamiento del PDF completado\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"[PDF {index}] Error durante el procesamiento: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def generate(pdf_files,max_workers=12):\n",
    "    output_dir = \"outputs\"\n",
    "    output_folder_path = Path(output_dir)\n",
    "    os.makedirs(output_folder_path, exist_ok=True)\n",
    "    \n",
    "    logger = logging.getLogger()\n",
    "    logger.info(f\"Generando Datasets - max_workers={max_workers}, # Files: {len(pdf_files)}\")\n",
    "    \n",
    "    # Dividir archivos en lotes para optimizar el uso de memoria\n",
    "    batch_size = max(1, math.ceil(len(pdf_files) / max_workers))\n",
    "    batches = [pdf_files[i:i + batch_size] for i in range(0, len(pdf_files), batch_size)]\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for batch_idx, batch in enumerate(batches):\n",
    "            logger.info(f\"Procesando lote {batch_idx + 1}/{len(batches)} con {len(batch)} archivos\")\n",
    "            tasks = [(i + batch_idx * batch_size, p, output_dir) for i, p in enumerate(batch)]\n",
    "            for _ in tqdm(\n",
    "                executor.map(process_pdf_wrapper, tasks),\n",
    "                total=len(batch),\n",
    "                desc=f\"Batch {batch_idx + 1}/{len(batches)}\"\n",
    "            ):\n",
    "                pass\n",
    "            logger.info(f\"Lote {batch_idx + 1}/{len(batches)} completado\")\n",
    "    logger.info(\"Generación de datasets finalizada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a3a3dadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 10:46:59,284 - INFO - Generando Datasets - max_workers=15, # Files: 89\n",
      "2025-08-08 10:46:59,288 - INFO - Procesando lote 1/15 con 6 archivos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1/15:   0%|          | 0/6 [00:00<?, ?it/s]2025-08-08 10:46:59,511 - INFO - [PDF 0] Inicio del procesamiento del PDF: L1_XVII_cap_IV.pdf\n",
      "2025-08-08 10:46:59,513 - INFO - [PDF 2] Inicio del procesamiento del PDF: manual-para-la-gestion-de-riesgos-lavados-de-activos-y-financiacion-del-terrorismo (1).pdf\n",
      "2025-08-08 10:46:59,513 - INFO - [PDF 4] Inicio del procesamiento del PDF: Modelo_de_Administracion_del_Riesgo_de_LAFT_y_Contrabando_web.pdf\n",
      "2025-08-08 10:46:59,512 - INFO - [PDF 1] Inicio del procesamiento del PDF: L1_IX_cap_III.pdf\n",
      "2025-08-08 10:46:59,514 - INFO - [PDF 5] Inicio del procesamiento del PDF: manual-para-la-gestion-de-riesgos-lavados-de-activos-y-financiacion-del-terrorismo.pdf\n",
      "2025-08-08 10:46:59,513 - INFO - [PDF 3] Inicio del procesamiento del PDF: L1_XVII_cap_III.pdf\n",
      "2025-08-08 10:46:59,519 - INFO - [PDF 2] Procesando PDF: manual-para-la-gestion-de-riesgos-lavados-de-activos-y-financiacion-del-terrorismo (1).pdf\n",
      "2025-08-08 10:46:59,518 - INFO - [PDF 0] Procesando PDF: L1_XVII_cap_IV.pdf\n",
      "2025-08-08 10:46:59,520 - INFO - [PDF 5] Procesando PDF: manual-para-la-gestion-de-riesgos-lavados-de-activos-y-financiacion-del-terrorismo.pdf\n",
      "2025-08-08 10:46:59,520 - INFO - [PDF 4] Procesando PDF: Modelo_de_Administracion_del_Riesgo_de_LAFT_y_Contrabando_web.pdf\n",
      "2025-08-08 10:46:59,520 - INFO - [PDF 3] Procesando PDF: L1_XVII_cap_III.pdf\n",
      "2025-08-08 10:46:59,521 - INFO - [PDF 1] Procesando PDF: L1_IX_cap_III.pdf\n",
      "2025-08-08 10:46:59,541 - INFO - [PDF 5] Procesando 53 fragmentos\n",
      "2025-08-08 10:46:59,542 - INFO - [PDF 3] Procesando 90 fragmentos\n",
      "2025-08-08 10:46:59,547 - INFO - [PDF 0] Procesando 145 fragmentos\n",
      "2025-08-08 10:46:59,552 - INFO - [PDF 1] Procesando 104 fragmentos\n",
      "2025-08-08 10:46:59,611 - INFO - [PDF 4] Procesando 592 fragmentos\n",
      "2025-08-08 10:46:59,687 - INFO - [PDF 2] Procesando 53 fragmentos\n",
      "2025-08-08 10:47:00,001 - INFO - [PDF 5] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:47:00,017 - INFO - [PDF 5] Procesamiento del PDF completado\n",
      "2025-08-08 10:47:00,552 - INFO - [PDF 3] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:47:00,560 - INFO - [PDF 3] Procesamiento del PDF completado\n",
      "2025-08-08 10:47:01,105 - INFO - [PDF 1] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:47:01,123 - INFO - [PDF 1] Procesamiento del PDF completado\n",
      "2025-08-08 10:47:01,126 - INFO - [PDF 0] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:47:01,138 - INFO - [PDF 0] Procesamiento del PDF completado\n",
      "Batch 1/15:  17%|█▋        | 1/6 [00:01<00:08,  1.62s/it]2025-08-08 10:47:14,017 - INFO - [PDF 4] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:47:14,029 - INFO - [PDF 4] Procesamiento del PDF completado\n",
      "2025-08-08 10:47:16,268 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:47:19,145 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:47:19,236 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:47:19,335 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:47:24,249 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:47:26,042 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:47:27,782 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:47:29,786 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:47:30,723 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:47:34,148 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:47:37,050 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:47:37,517 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:47:41,475 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:47:43,090 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:47:47,034 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:47:50,915 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:47:51,235 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:47:52,803 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:47:53,390 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:47:59,286 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:47:59,563 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:48:05,160 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:48:05,290 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:48:07,955 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:48:10,489 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:48:19,536 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:48:21,208 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:48:27,418 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:48:38,477 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:48:47,322 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:48:55,574 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:49:02,459 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:49:14,438 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:49:21,382 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:49:24,557 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:49:27,883 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:49:29,796 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:49:33,956 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:49:38,281 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:49:53,959 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:50:06,945 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:50:11,035 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:50:17,253 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:50:23,002 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:50:32,175 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:50:38,207 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:50:44,347 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:50:49,807 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:50:57,252 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:51:01,045 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:51:05,520 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:51:10,044 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:51:17,338 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:51:17,388 - INFO - [PDF 2] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:51:17,398 - INFO - [PDF 2] Procesamiento del PDF completado\n",
      "Batch 1/15: 100%|██████████| 6/6 [04:17<00:00, 42.98s/it]\n",
      "2025-08-08 10:51:17,404 - INFO - Lote 1/15 completado\n",
      "2025-08-08 10:51:17,407 - INFO - Procesando lote 2/15 con 6 archivos\n",
      "Batch 2/15:   0%|          | 0/6 [00:00<?, ?it/s]2025-08-08 10:51:17,411 - INFO - [PDF 9] Inicio del procesamiento del PDF: Guía-para-la-elaboración-del-Manual-de-Prevención-de-Lavado-de-Activos-y-del-Financiamiento-de-Delitos-ESFPS.pdf\n",
      "2025-08-08 10:51:17,411 - INFO - [PDF 7] Inicio del procesamiento del PDF: RML BANCO CENTRAL_2.pdf\n",
      "2025-08-08 10:51:17,411 - INFO - [PDF 6] Inicio del procesamiento del PDF: L1_VIII_cap_I.pdf\n",
      "2025-08-08 10:51:17,411 - INFO - [PDF 10] Inicio del procesamiento del PDF: manual_riesgos_mercado_liquidez_9_jul_10.pdf\n",
      "2025-08-08 10:51:17,411 - INFO - [PDF 8] Inicio del procesamiento del PDF: Tipologias_de_lavado_de_activos_2020_al_12_Noviembre.pdf\n",
      "2025-08-08 10:51:17,416 - INFO - [PDF 7] Procesando PDF: RML BANCO CENTRAL_2.pdf\n",
      "2025-08-08 10:51:17,416 - INFO - [PDF 6] Procesando PDF: L1_VIII_cap_I.pdf\n",
      "2025-08-08 10:51:17,420 - INFO - [PDF 8] Procesando PDF: Tipologias_de_lavado_de_activos_2020_al_12_Noviembre.pdf\n",
      "2025-08-08 10:51:17,429 - INFO - [PDF 6] Procesando 9 fragmentos\n",
      "2025-08-08 10:51:17,429 - INFO - [PDF 7] Procesando 18 fragmentos\n",
      "2025-08-08 10:51:17,412 - INFO - [PDF 11] Inicio del procesamiento del PDF: manual_estructuras_datos_generalidades_1_abr_14.pdf\n",
      "2025-08-08 10:51:17,450 - INFO - [PDF 11] Procesando PDF: manual_estructuras_datos_generalidades_1_abr_14.pdf\n",
      "2025-08-08 10:51:17,452 - INFO - [PDF 8] Procesando 108 fragmentos\n",
      "2025-08-08 10:51:17,461 - INFO - [PDF 6] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:51:17,417 - INFO - [PDF 9] Procesando PDF: Guía-para-la-elaboración-del-Manual-de-Prevención-de-Lavado-de-Activos-y-del-Financiamiento-de-Delitos-ESFPS.pdf\n",
      "2025-08-08 10:51:17,420 - INFO - [PDF 10] Procesando PDF: manual_riesgos_mercado_liquidez_9_jul_10.pdf\n",
      "2025-08-08 10:51:17,466 - INFO - [PDF 11] Procesando 20 fragmentos\n",
      "2025-08-08 10:51:17,467 - INFO - [PDF 6] Procesamiento del PDF completado\n",
      "2025-08-08 10:51:17,489 - INFO - [PDF 10] Procesando 56 fragmentos\n",
      "2025-08-08 10:51:17,491 - INFO - [PDF 7] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:51:17,498 - INFO - [PDF 7] Procesamiento del PDF completado\n",
      "2025-08-08 10:51:17,507 - INFO - [PDF 9] Procesando 125 fragmentos\n",
      "2025-08-08 10:51:17,547 - INFO - [PDF 11] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:51:17,551 - INFO - [PDF 11] Procesamiento del PDF completado\n",
      "2025-08-08 10:51:17,789 - INFO - [PDF 10] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:51:17,797 - INFO - [PDF 10] Procesamiento del PDF completado\n",
      "2025-08-08 10:51:18,545 - INFO - [PDF 8] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:51:18,556 - INFO - [PDF 8] Procesamiento del PDF completado\n",
      "Batch 2/15:  50%|█████     | 3/6 [00:01<00:01,  2.61it/s]2025-08-08 10:51:18,874 - INFO - [PDF 9] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:51:18,885 - INFO - [PDF 9] Procesamiento del PDF completado\n",
      "Batch 2/15: 100%|██████████| 6/6 [00:01<00:00,  4.06it/s]\n",
      "2025-08-08 10:51:18,891 - INFO - Lote 2/15 completado\n",
      "2025-08-08 10:51:18,892 - INFO - Procesando lote 3/15 con 6 archivos\n",
      "Batch 3/15:   0%|          | 0/6 [00:00<?, ?it/s]2025-08-08 10:51:18,896 - INFO - [PDF 16] Inicio del procesamiento del PDF: Spanish-Money-Laundering-National-Risk-Assessment-Guidance.pdf.coredownload.inline.pdf\n",
      "2025-08-08 10:51:18,897 - INFO - [PDF 15] Inicio del procesamiento del PDF: L1_IX_cap_IV.pdf\n",
      "2025-08-08 10:51:18,897 - INFO - [PDF 17] Inicio del procesamiento del PDF: RML BANCO CENTRAL.pdf\n",
      "2025-08-08 10:51:18,897 - INFO - [PDF 13] Inicio del procesamiento del PDF: RESUMEN-EJECUTIVO.pdf\n",
      "2025-08-08 10:51:18,897 - INFO - [PDF 14] Inicio del procesamiento del PDF: RESOLUCION UIF.pdf\n",
      "2025-08-08 10:51:18,899 - INFO - [PDF 16] Procesando PDF: Spanish-Money-Laundering-National-Risk-Assessment-Guidance.pdf.coredownload.inline.pdf\n",
      "2025-08-08 10:51:18,900 - INFO - [PDF 17] Procesando PDF: RML BANCO CENTRAL.pdf\n",
      "2025-08-08 10:51:18,900 - INFO - [PDF 15] Procesando PDF: L1_IX_cap_IV.pdf\n",
      "2025-08-08 10:51:18,902 - INFO - [PDF 13] Procesando PDF: RESUMEN-EJECUTIVO.pdf\n",
      "2025-08-08 10:51:18,902 - INFO - [PDF 14] Procesando PDF: RESOLUCION UIF.pdf\n",
      "2025-08-08 10:51:18,916 - INFO - [PDF 17] Procesando 2 fragmentos\n",
      "2025-08-08 10:51:18,896 - INFO - [PDF 12] Inicio del procesamiento del PDF: RECOMENDACIONES GAFI 49.pdf\n",
      "2025-08-08 10:51:18,920 - INFO - [PDF 13] Procesando 30 fragmentos\n",
      "2025-08-08 10:51:18,921 - INFO - [PDF 15] Procesando 35 fragmentos\n",
      "2025-08-08 10:51:18,923 - INFO - [PDF 17] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:51:18,924 - INFO - [PDF 12] Procesando PDF: RECOMENDACIONES GAFI 49.pdf\n",
      "2025-08-08 10:51:18,927 - INFO - [PDF 17] Procesamiento del PDF completado\n",
      "2025-08-08 10:51:18,929 - WARNING - [PDF 14] No se encontraron fragmentos de texto\n",
      "2025-08-08 10:51:18,931 - INFO - [PDF 14] Procesamiento del PDF completado\n",
      "2025-08-08 10:51:18,933 - INFO - [PDF 16] Procesando 221 fragmentos\n",
      "2025-08-08 10:51:18,949 - INFO - [PDF 12] Procesando 56 fragmentos\n",
      "2025-08-08 10:51:19,089 - INFO - [PDF 13] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:51:19,096 - INFO - [PDF 13] Procesamiento del PDF completado\n",
      "2025-08-08 10:51:19,140 - INFO - [PDF 15] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:51:19,148 - INFO - [PDF 15] Procesamiento del PDF completado\n",
      "2025-08-08 10:51:19,366 - INFO - [PDF 12] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:51:19,373 - INFO - [PDF 12] Procesamiento del PDF completado\n",
      "Batch 3/15:  17%|█▋        | 1/6 [00:00<00:02,  2.09it/s]2025-08-08 10:51:21,603 - INFO - [PDF 16] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:51:21,615 - INFO - [PDF 16] Procesamiento del PDF completado\n",
      "Batch 3/15: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s]\n",
      "2025-08-08 10:51:21,619 - INFO - Lote 3/15 completado\n",
      "2025-08-08 10:51:21,620 - INFO - Procesando lote 4/15 con 6 archivos\n",
      "Batch 4/15:   0%|          | 0/6 [00:00<?, ?it/s]2025-08-08 10:51:21,622 - INFO - [PDF 19] Inicio del procesamiento del PDF: BASILEA II CONVERGENCIA INTERNACINOAL bcbs128_es.pdf\n",
      "2025-08-08 10:51:21,622 - INFO - [PDF 18] Inicio del procesamiento del PDF: manual-prevencion-lavado-activos-Ecuador.pdf\n",
      "2025-08-08 10:51:21,623 - INFO - [PDF 20] Inicio del procesamiento del PDF: L1_IX_cap_II_PROVISION.pdf\n",
      "2025-08-08 10:51:21,624 - INFO - [PDF 21] Inicio del procesamiento del PDF: manual_control_inversiones_3_mar_17.pdf\n",
      "2025-08-08 10:51:21,624 - INFO - [PDF 23] Inicio del procesamiento del PDF: L1_XIV_cap_III.pdf\n",
      "2025-08-08 10:51:21,624 - INFO - [PDF 22] Inicio del procesamiento del PDF: Spanish-Lavado-de-Activos-Basado-en-el-Comercio-Indicadores-de-Riesgo.pdf.coredownload.inline (1).pdf\n",
      "2025-08-08 10:51:21,625 - INFO - [PDF 19] Procesando PDF: BASILEA II CONVERGENCIA INTERNACINOAL bcbs128_es.pdf\n",
      "2025-08-08 10:51:21,625 - INFO - [PDF 20] Procesando PDF: L1_IX_cap_II_PROVISION.pdf\n",
      "2025-08-08 10:51:21,625 - INFO - [PDF 18] Procesando PDF: manual-prevencion-lavado-activos-Ecuador.pdf\n",
      "2025-08-08 10:51:21,627 - INFO - [PDF 21] Procesando PDF: manual_control_inversiones_3_mar_17.pdf\n",
      "2025-08-08 10:51:21,627 - INFO - [PDF 22] Procesando PDF: Spanish-Lavado-de-Activos-Basado-en-el-Comercio-Indicadores-de-Riesgo.pdf.coredownload.inline (1).pdf\n",
      "2025-08-08 10:51:21,627 - INFO - [PDF 23] Procesando PDF: L1_XIV_cap_III.pdf\n",
      "2025-08-08 10:51:21,638 - INFO - [PDF 22] Procesando 12 fragmentos\n",
      "2025-08-08 10:51:21,640 - INFO - [PDF 23] Procesando 9 fragmentos\n",
      "2025-08-08 10:51:21,647 - INFO - [PDF 18] Procesando 149 fragmentos\n",
      "2025-08-08 10:51:21,657 - INFO - [PDF 21] Procesando 92 fragmentos\n",
      "2025-08-08 10:51:21,659 - INFO - [PDF 20] Procesando 148 fragmentos\n",
      "2025-08-08 10:51:21,683 - INFO - [PDF 23] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:51:21,687 - INFO - [PDF 23] Procesamiento del PDF completado\n",
      "2025-08-08 10:51:21,694 - INFO - [PDF 22] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:51:21,697 - INFO - [PDF 22] Procesamiento del PDF completado\n",
      "2025-08-08 10:51:21,790 - INFO - [PDF 19] Procesando 1448 fragmentos\n",
      "2025-08-08 10:51:22,391 - INFO - [PDF 21] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:51:22,399 - INFO - [PDF 21] Procesamiento del PDF completado\n",
      "2025-08-08 10:51:22,858 - INFO - [PDF 18] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:51:22,866 - INFO - [PDF 18] Procesamiento del PDF completado\n",
      "Batch 4/15:  17%|█▋        | 1/6 [00:01<00:06,  1.24s/it]2025-08-08 10:51:23,211 - INFO - [PDF 20] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:51:23,221 - INFO - [PDF 20] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:15,177 - INFO - [PDF 19] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:15,192 - INFO - [PDF 19] Procesamiento del PDF completado\n",
      "Batch 4/15: 100%|██████████| 6/6 [00:53<00:00,  8.93s/it]\n",
      "2025-08-08 10:52:15,198 - INFO - Lote 4/15 completado\n",
      "2025-08-08 10:52:15,199 - INFO - Procesando lote 5/15 con 6 archivos\n",
      "Batch 5/15:   0%|          | 0/6 [00:00<?, ?it/s]2025-08-08 10:52:15,211 - INFO - [PDF 24] Inicio del procesamiento del PDF: manual_operaciones_activas_contingentes_24_feb_17.pdf\n",
      "2025-08-08 10:52:15,211 - INFO - [PDF 25] Inicio del procesamiento del PDF: L1_XVIII_cap_V.pdf\n",
      "2025-08-08 10:52:15,212 - INFO - [PDF 27] Inicio del procesamiento del PDF: L1_VII_cap_V.pdf\n",
      "2025-08-08 10:52:15,212 - INFO - [PDF 26] Inicio del procesamiento del PDF: SUPERBANCOS L1_XIII_cap_IV.pdf\n",
      "2025-08-08 10:52:15,212 - INFO - [PDF 29] Inicio del procesamiento del PDF: metodologia-SBR-lavado-activos-2.pdf\n",
      "2025-08-08 10:52:15,212 - INFO - [PDF 28] Inicio del procesamiento del PDF: 9 Recomendaciones Especiales GAFI.pdf\n",
      "2025-08-08 10:52:15,215 - INFO - [PDF 25] Procesando PDF: L1_XVIII_cap_V.pdf\n",
      "2025-08-08 10:52:15,214 - INFO - [PDF 24] Procesando PDF: manual_operaciones_activas_contingentes_24_feb_17.pdf\n",
      "2025-08-08 10:52:15,215 - INFO - [PDF 27] Procesando PDF: L1_VII_cap_V.pdf\n",
      "2025-08-08 10:52:15,216 - INFO - [PDF 29] Procesando PDF: metodologia-SBR-lavado-activos-2.pdf\n",
      "2025-08-08 10:52:15,216 - INFO - [PDF 28] Procesando PDF: 9 Recomendaciones Especiales GAFI.pdf\n",
      "2025-08-08 10:52:15,216 - INFO - [PDF 26] Procesando PDF: SUPERBANCOS L1_XIII_cap_IV.pdf\n",
      "2025-08-08 10:52:15,229 - INFO - [PDF 28] Procesando 4 fragmentos\n",
      "2025-08-08 10:52:15,230 - INFO - [PDF 27] Procesando 12 fragmentos\n",
      "2025-08-08 10:52:15,232 - INFO - [PDF 29] Procesando 22 fragmentos\n",
      "2025-08-08 10:52:15,235 - INFO - [PDF 25] Procesando 66 fragmentos\n",
      "2025-08-08 10:52:15,237 - INFO - [PDF 26] Procesando 55 fragmentos\n",
      "2025-08-08 10:52:15,247 - INFO - [PDF 28] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:15,251 - INFO - [PDF 28] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:15,258 - INFO - [PDF 24] Procesando 150 fragmentos\n",
      "2025-08-08 10:52:15,284 - INFO - [PDF 27] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:15,288 - INFO - [PDF 27] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:15,344 - INFO - [PDF 29] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:15,351 - INFO - [PDF 29] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:15,886 - INFO - [PDF 26] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:15,894 - INFO - [PDF 26] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:16,027 - INFO - [PDF 25] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:16,037 - INFO - [PDF 25] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:18,275 - INFO - [PDF 24] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:18,282 - INFO - [PDF 24] Procesamiento del PDF completado\n",
      "Batch 5/15: 100%|██████████| 6/6 [00:03<00:00,  1.95it/s]\n",
      "2025-08-08 10:52:18,287 - INFO - Lote 5/15 completado\n",
      "2025-08-08 10:52:18,288 - INFO - Procesando lote 6/15 con 6 archivos\n",
      "Batch 6/15:   0%|          | 0/6 [00:00<?, ?it/s]2025-08-08 10:52:18,292 - INFO - [PDF 31] Inicio del procesamiento del PDF: L1_XIX_cap_V.pdf\n",
      "2025-08-08 10:52:18,291 - INFO - [PDF 30] Inicio del procesamiento del PDF: L1_XIX_cap_I.pdf\n",
      "2025-08-08 10:52:18,292 - INFO - [PDF 32] Inicio del procesamiento del PDF: VISION GENERAL ACUERDO.pdf\n",
      "2025-08-08 10:52:18,292 - INFO - [PDF 33] Inicio del procesamiento del PDF: resol_JB-2011-1897.pdf\n",
      "2025-08-08 10:52:18,292 - INFO - [PDF 35] Inicio del procesamiento del PDF: L1_VI_cap_V.pdf\n",
      "2025-08-08 10:52:18,292 - INFO - [PDF 34] Inicio del procesamiento del PDF: L1_XIII_cap_II.pdf\n",
      "2025-08-08 10:52:18,294 - INFO - [PDF 31] Procesando PDF: L1_XIX_cap_V.pdf\n",
      "2025-08-08 10:52:18,295 - INFO - [PDF 32] Procesando PDF: VISION GENERAL ACUERDO.pdf\n",
      "2025-08-08 10:52:18,294 - INFO - [PDF 30] Procesando PDF: L1_XIX_cap_I.pdf\n",
      "2025-08-08 10:52:18,295 - INFO - [PDF 33] Procesando PDF: resol_JB-2011-1897.pdf\n",
      "2025-08-08 10:52:18,296 - INFO - [PDF 35] Procesando PDF: L1_VI_cap_V.pdf\n",
      "2025-08-08 10:52:18,296 - INFO - [PDF 34] Procesando PDF: L1_XIII_cap_II.pdf\n",
      "2025-08-08 10:52:18,314 - INFO - [PDF 31] Procesando 41 fragmentos\n",
      "2025-08-08 10:52:18,316 - INFO - [PDF 30] Procesando 10 fragmentos\n",
      "2025-08-08 10:52:18,317 - INFO - [PDF 35] Procesando 47 fragmentos\n",
      "2025-08-08 10:52:18,319 - INFO - [PDF 34] Procesando 37 fragmentos\n",
      "2025-08-08 10:52:18,325 - INFO - [PDF 32] Procesando 177 fragmentos\n",
      "2025-08-08 10:52:18,326 - INFO - [PDF 33] Procesando 101 fragmentos\n",
      "2025-08-08 10:52:18,378 - INFO - [PDF 30] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:18,384 - INFO - [PDF 30] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:18,575 - INFO - [PDF 35] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:18,575 - INFO - [PDF 31] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:18,580 - INFO - [PDF 35] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:18,585 - INFO - [PDF 31] Procesamiento del PDF completado\n",
      "Batch 6/15:  33%|███▎      | 2/6 [00:00<00:00,  6.78it/s]2025-08-08 10:52:18,589 - INFO - [PDF 34] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:18,593 - INFO - [PDF 34] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:19,024 - INFO - [PDF 33] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:19,031 - INFO - [PDF 33] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:21,266 - INFO - [PDF 32] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:21,275 - INFO - [PDF 32] Procesamiento del PDF completado\n",
      "Batch 6/15: 100%|██████████| 6/6 [00:02<00:00,  2.01it/s]\n",
      "2025-08-08 10:52:21,283 - INFO - Lote 6/15 completado\n",
      "2025-08-08 10:52:21,285 - INFO - Procesando lote 7/15 con 6 archivos\n",
      "2025-08-08 10:52:21,290 - INFO - [PDF 36] Inicio del procesamiento del PDF: L1_XI_cap_VII.pdf\n",
      "Batch 7/15:   0%|          | 0/6 [00:00<?, ?it/s]2025-08-08 10:52:21,291 - INFO - [PDF 37] Inicio del procesamiento del PDF: L1_X_cap_II.pdf\n",
      "2025-08-08 10:52:21,291 - INFO - [PDF 40] Inicio del procesamiento del PDF: L1_XVIII_cap_III.pdf\n",
      "2025-08-08 10:52:21,291 - INFO - [PDF 38] Inicio del procesamiento del PDF: L1_X_cap_III.pdf\n",
      "2025-08-08 10:52:21,292 - INFO - [PDF 36] Procesando PDF: L1_XI_cap_VII.pdf\n",
      "2025-08-08 10:52:21,291 - INFO - [PDF 39] Inicio del procesamiento del PDF: L1_XII_cap_II.pdf\n",
      "2025-08-08 10:52:21,291 - INFO - [PDF 41] Inicio del procesamiento del PDF: L1_IX_cap_I.pdf\n",
      "2025-08-08 10:52:21,294 - INFO - [PDF 40] Procesando PDF: L1_XVIII_cap_III.pdf\n",
      "2025-08-08 10:52:21,294 - INFO - [PDF 37] Procesando PDF: L1_X_cap_II.pdf\n",
      "2025-08-08 10:52:21,294 - INFO - [PDF 38] Procesando PDF: L1_X_cap_III.pdf\n",
      "2025-08-08 10:52:21,295 - INFO - [PDF 41] Procesando PDF: L1_IX_cap_I.pdf\n",
      "2025-08-08 10:52:21,295 - INFO - [PDF 39] Procesando PDF: L1_XII_cap_II.pdf\n",
      "2025-08-08 10:52:21,312 - INFO - [PDF 36] Procesando 15 fragmentos\n",
      "2025-08-08 10:52:21,316 - INFO - [PDF 40] Procesando 12 fragmentos\n",
      "2025-08-08 10:52:21,314 - INFO - [PDF 37] Procesando 6 fragmentos\n",
      "2025-08-08 10:52:21,316 - INFO - [PDF 38] Procesando 5 fragmentos\n",
      "2025-08-08 10:52:21,319 - INFO - [PDF 41] Procesando 24 fragmentos\n",
      "2025-08-08 10:52:21,321 - INFO - [PDF 39] Procesando 21 fragmentos\n",
      "2025-08-08 10:52:21,341 - INFO - [PDF 38] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:21,343 - INFO - [PDF 37] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:21,348 - INFO - [PDF 38] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:21,349 - INFO - [PDF 37] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:21,366 - INFO - [PDF 40] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:21,371 - INFO - [PDF 36] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:21,373 - INFO - [PDF 40] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:21,377 - INFO - [PDF 36] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:21,514 - INFO - [PDF 39] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:21,521 - INFO - [PDF 39] Procesamiento del PDF completado\n",
      "Batch 7/15:  67%|██████▋   | 4/6 [00:00<00:00, 17.24it/s]2025-08-08 10:52:21,530 - INFO - [PDF 41] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:21,538 - INFO - [PDF 41] Procesamiento del PDF completado\n",
      "Batch 7/15: 100%|██████████| 6/6 [00:00<00:00, 24.16it/s]\n",
      "2025-08-08 10:52:21,542 - INFO - Lote 7/15 completado\n",
      "2025-08-08 10:52:21,544 - INFO - Procesando lote 8/15 con 6 archivos\n",
      "Batch 8/15:   0%|          | 0/6 [00:00<?, ?it/s]2025-08-08 10:52:21,550 - INFO - [PDF 42] Inicio del procesamiento del PDF: L1_XIX_cap_II.pdf\n",
      "2025-08-08 10:52:21,551 - INFO - [PDF 43] Inicio del procesamiento del PDF: L1_VIII_cap_II.pdf\n",
      "2025-08-08 10:52:21,552 - INFO - [PDF 44] Inicio del procesamiento del PDF: L1_VIII_cap_VI.pdf\n",
      "2025-08-08 10:52:21,553 - INFO - [PDF 45] Inicio del procesamiento del PDF: Spanish-Lavado-de-Activos-Basado-en-el-Comercio-Indicadores-de-Riesgo.pdf.coredownload.inline.pdf\n",
      "2025-08-08 10:52:21,553 - INFO - [PDF 47] Inicio del procesamiento del PDF: L1_XVII_cap_V.pdf\n",
      "2025-08-08 10:52:21,553 - INFO - [PDF 46] Inicio del procesamiento del PDF: titulo cap_II CALIFICACION DE ACTIVOS DE RIESGO.pdf\n",
      "2025-08-08 10:52:21,553 - INFO - [PDF 43] Procesando PDF: L1_VIII_cap_II.pdf\n",
      "2025-08-08 10:52:21,554 - INFO - [PDF 42] Procesando PDF: L1_XIX_cap_II.pdf\n",
      "2025-08-08 10:52:21,556 - INFO - [PDF 44] Procesando PDF: L1_VIII_cap_VI.pdf\n",
      "2025-08-08 10:52:21,557 - INFO - [PDF 45] Procesando PDF: Spanish-Lavado-de-Activos-Basado-en-el-Comercio-Indicadores-de-Riesgo.pdf.coredownload.inline.pdf\n",
      "2025-08-08 10:52:21,558 - INFO - [PDF 47] Procesando PDF: L1_XVII_cap_V.pdf\n",
      "2025-08-08 10:52:21,558 - INFO - [PDF 46] Procesando PDF: titulo cap_II CALIFICACION DE ACTIVOS DE RIESGO.pdf\n",
      "2025-08-08 10:52:21,568 - INFO - [PDF 43] Procesando 10 fragmentos\n",
      "2025-08-08 10:52:21,571 - INFO - [PDF 47] Procesando 10 fragmentos\n",
      "2025-08-08 10:52:21,570 - INFO - [PDF 44] Procesando 3 fragmentos\n",
      "2025-08-08 10:52:21,571 - INFO - [PDF 42] Procesando 5 fragmentos\n",
      "2025-08-08 10:52:21,572 - INFO - [PDF 45] Procesando 12 fragmentos\n",
      "2025-08-08 10:52:21,581 - INFO - [PDF 46] Procesando 50 fragmentos\n",
      "2025-08-08 10:52:21,590 - INFO - [PDF 44] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:21,595 - INFO - [PDF 44] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:21,599 - INFO - [PDF 42] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:21,609 - INFO - [PDF 42] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:21,621 - INFO - [PDF 47] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:21,625 - INFO - [PDF 47] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:21,626 - INFO - [PDF 43] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:21,642 - INFO - [PDF 43] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:21,814 - INFO - [PDF 45] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:21,826 - INFO - [PDF 45] Procesamiento del PDF completado\n",
      "Batch 8/15:  67%|██████▋   | 4/6 [00:00<00:00, 14.38it/s]2025-08-08 10:52:22,265 - INFO - [PDF 46] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:22,270 - INFO - [PDF 46] Procesamiento del PDF completado\n",
      "Batch 8/15: 100%|██████████| 6/6 [00:00<00:00,  8.31it/s]\n",
      "2025-08-08 10:52:22,275 - INFO - Lote 8/15 completado\n",
      "2025-08-08 10:52:22,276 - INFO - Procesando lote 9/15 con 6 archivos\n",
      "Batch 9/15:   0%|          | 0/6 [00:00<?, ?it/s]2025-08-08 10:52:22,280 - INFO - [PDF 48] Inicio del procesamiento del PDF: L1_VIII_cap_V.pdf\n",
      "2025-08-08 10:52:22,280 - INFO - [PDF 52] Inicio del procesamiento del PDF: cap_III ADMINISTRACION RIESGO MERCADO.pdf\n",
      "2025-08-08 10:52:22,280 - INFO - [PDF 50] Inicio del procesamiento del PDF: L1_III_cap_IV.pdf\n",
      "2025-08-08 10:52:22,280 - INFO - [PDF 49] Inicio del procesamiento del PDF: L1_III_cap_II.pdf\n",
      "2025-08-08 10:52:22,280 - INFO - [PDF 51] Inicio del procesamiento del PDF: L1_II_cap_I.pdf\n",
      "2025-08-08 10:52:22,280 - INFO - [PDF 53] Inicio del procesamiento del PDF: L1_IX_cap_IX.pdf\n",
      "2025-08-08 10:52:22,283 - INFO - [PDF 48] Procesando PDF: L1_VIII_cap_V.pdf\n",
      "2025-08-08 10:52:22,283 - INFO - [PDF 50] Procesando PDF: L1_III_cap_IV.pdf\n",
      "2025-08-08 10:52:22,283 - INFO - [PDF 52] Procesando PDF: cap_III ADMINISTRACION RIESGO MERCADO.pdf\n",
      "2025-08-08 10:52:22,284 - INFO - [PDF 51] Procesando PDF: L1_II_cap_I.pdf\n",
      "2025-08-08 10:52:22,284 - INFO - [PDF 49] Procesando PDF: L1_III_cap_II.pdf\n",
      "2025-08-08 10:52:22,284 - INFO - [PDF 53] Procesando PDF: L1_IX_cap_IX.pdf\n",
      "2025-08-08 10:52:22,293 - INFO - [PDF 48] Procesando 18 fragmentos\n",
      "2025-08-08 10:52:22,298 - INFO - [PDF 50] Procesando 34 fragmentos\n",
      "2025-08-08 10:52:22,298 - INFO - [PDF 49] Procesando 10 fragmentos\n",
      "2025-08-08 10:52:22,307 - INFO - [PDF 51] Procesando 25 fragmentos\n",
      "2025-08-08 10:52:22,307 - INFO - [PDF 52] Procesando 45 fragmentos\n",
      "2025-08-08 10:52:22,312 - INFO - [PDF 53] Procesando 53 fragmentos\n",
      "2025-08-08 10:52:22,359 - INFO - [PDF 48] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:22,359 - INFO - [PDF 49] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:22,363 - INFO - [PDF 48] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:22,364 - INFO - [PDF 49] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:22,417 - INFO - [PDF 51] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:22,423 - INFO - [PDF 51] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:22,499 - INFO - [PDF 50] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:22,507 - INFO - [PDF 50] Procesamiento del PDF completado\n",
      "Batch 9/15:  50%|█████     | 3/6 [00:00<00:00, 13.11it/s]2025-08-08 10:52:22,650 - INFO - [PDF 52] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:22,658 - INFO - [PDF 52] Procesamiento del PDF completado\n",
      "Batch 9/15:  83%|████████▎ | 5/6 [00:00<00:00, 13.15it/s]2025-08-08 10:52:22,726 - INFO - [PDF 53] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:22,731 - INFO - [PDF 53] Procesamiento del PDF completado\n",
      "Batch 9/15: 100%|██████████| 6/6 [00:00<00:00, 13.23it/s]\n",
      "2025-08-08 10:52:22,735 - INFO - Lote 9/15 completado\n",
      "2025-08-08 10:52:22,737 - INFO - Procesando lote 10/15 con 6 archivos\n",
      "Batch 10/15:   0%|          | 0/6 [00:00<?, ?it/s]2025-08-08 10:52:22,740 - INFO - [PDF 54] Inicio del procesamiento del PDF: L1_IX_cap_X.pdf\n",
      "2025-08-08 10:52:22,741 - INFO - [PDF 55] Inicio del procesamiento del PDF: L1_VII_cap_IV.pdf\n",
      "2025-08-08 10:52:22,741 - INFO - [PDF 56] Inicio del procesamiento del PDF: L1_XI_cap_III.pdf\n",
      "2025-08-08 10:52:22,741 - INFO - [PDF 59] Inicio del procesamiento del PDF: cap_V ADMINISTRACION RIESGO OPERATIVO.pdf\n",
      "2025-08-08 10:52:22,741 - INFO - [PDF 58] Inicio del procesamiento del PDF: L1_VI_cap_II.pdf\n",
      "2025-08-08 10:52:22,741 - INFO - [PDF 57] Inicio del procesamiento del PDF: L1_VI_cap_VI.pdf\n",
      "2025-08-08 10:52:22,743 - INFO - [PDF 54] Procesando PDF: L1_IX_cap_X.pdf\n",
      "2025-08-08 10:52:22,744 - INFO - [PDF 59] Procesando PDF: cap_V ADMINISTRACION RIESGO OPERATIVO.pdf\n",
      "2025-08-08 10:52:22,744 - INFO - [PDF 55] Procesando PDF: L1_VII_cap_IV.pdf\n",
      "2025-08-08 10:52:22,744 - INFO - [PDF 56] Procesando PDF: L1_XI_cap_III.pdf\n",
      "2025-08-08 10:52:22,745 - INFO - [PDF 58] Procesando PDF: L1_VI_cap_II.pdf\n",
      "2025-08-08 10:52:22,745 - INFO - [PDF 57] Procesando PDF: L1_VI_cap_VI.pdf\n",
      "2025-08-08 10:52:22,757 - INFO - [PDF 54] Procesando 19 fragmentos\n",
      "2025-08-08 10:52:22,759 - INFO - [PDF 59] Procesando 31 fragmentos\n",
      "2025-08-08 10:52:22,760 - INFO - [PDF 55] Procesando 18 fragmentos\n",
      "2025-08-08 10:52:22,760 - INFO - [PDF 57] Procesando 11 fragmentos\n",
      "2025-08-08 10:52:22,760 - INFO - [PDF 56] Procesando 3 fragmentos\n",
      "2025-08-08 10:52:22,761 - INFO - [PDF 58] Procesando 4 fragmentos\n",
      "2025-08-08 10:52:22,779 - INFO - [PDF 56] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:22,782 - INFO - [PDF 58] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:22,784 - INFO - [PDF 56] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:22,788 - INFO - [PDF 58] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:22,818 - INFO - [PDF 57] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:22,822 - INFO - [PDF 57] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:22,831 - INFO - [PDF 55] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:22,835 - INFO - [PDF 55] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:22,850 - INFO - [PDF 54] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:22,856 - INFO - [PDF 54] Procesamiento del PDF completado\n",
      "Batch 10/15:  17%|█▋        | 1/6 [00:00<00:00,  8.49it/s]2025-08-08 10:52:22,891 - INFO - [PDF 59] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:22,896 - INFO - [PDF 59] Procesamiento del PDF completado\n",
      "Batch 10/15: 100%|██████████| 6/6 [00:00<00:00, 38.44it/s]\n",
      "2025-08-08 10:52:22,898 - INFO - Lote 10/15 completado\n",
      "2025-08-08 10:52:22,900 - INFO - Procesando lote 11/15 con 6 archivos\n",
      "Batch 11/15:   0%|          | 0/6 [00:00<?, ?it/s]2025-08-08 10:52:22,908 - INFO - [PDF 62] Inicio del procesamiento del PDF: L1_XV_cap_I.pdf\n",
      "2025-08-08 10:52:22,908 - INFO - [PDF 60] Inicio del procesamiento del PDF: L1_VII_cap_I.pdf\n",
      "2025-08-08 10:52:22,909 - INFO - [PDF 61] Inicio del procesamiento del PDF: L1_XVIII_cap_I.pdf\n",
      "2025-08-08 10:52:22,909 - INFO - [PDF 63] Inicio del procesamiento del PDF: guia-manual-lavado-activos-sept-20.pdf\n",
      "2025-08-08 10:52:22,909 - INFO - [PDF 64] Inicio del procesamiento del PDF: L1_VI_cap_III.pdf\n",
      "2025-08-08 10:52:22,909 - INFO - [PDF 65] Inicio del procesamiento del PDF: L1_XIX_cap_IV.pdf\n",
      "2025-08-08 10:52:22,910 - INFO - [PDF 62] Procesando PDF: L1_XV_cap_I.pdf\n",
      "2025-08-08 10:52:22,911 - INFO - [PDF 60] Procesando PDF: L1_VII_cap_I.pdf\n",
      "2025-08-08 10:52:22,912 - INFO - [PDF 61] Procesando PDF: L1_XVIII_cap_I.pdf\n",
      "2025-08-08 10:52:22,912 - INFO - [PDF 63] Procesando PDF: guia-manual-lavado-activos-sept-20.pdf\n",
      "2025-08-08 10:52:22,913 - INFO - [PDF 65] Procesando PDF: L1_XIX_cap_IV.pdf\n",
      "2025-08-08 10:52:22,912 - INFO - [PDF 64] Procesando PDF: L1_VI_cap_III.pdf\n",
      "2025-08-08 10:52:22,925 - INFO - [PDF 62] Procesando 5 fragmentos\n",
      "2025-08-08 10:52:22,927 - INFO - [PDF 65] Procesando 6 fragmentos\n",
      "2025-08-08 10:52:22,928 - INFO - [PDF 61] Procesando 6 fragmentos\n",
      "2025-08-08 10:52:22,928 - INFO - [PDF 64] Procesando 3 fragmentos\n",
      "2025-08-08 10:52:22,929 - INFO - [PDF 60] Procesando 9 fragmentos\n",
      "2025-08-08 10:52:22,929 - INFO - [PDF 63] Procesando 8 fragmentos\n",
      "2025-08-08 10:52:22,941 - INFO - [PDF 62] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:22,941 - INFO - [PDF 64] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:22,944 - INFO - [PDF 65] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:22,944 - INFO - [PDF 62] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:22,947 - INFO - [PDF 65] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:22,946 - INFO - [PDF 64] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:22,951 - INFO - [PDF 61] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:22,956 - INFO - [PDF 61] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:22,963 - INFO - [PDF 63] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:22,963 - INFO - [PDF 60] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:22,968 - INFO - [PDF 63] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:22,968 - INFO - [PDF 60] Procesamiento del PDF completado\n",
      "Batch 11/15: 100%|██████████| 6/6 [00:00<00:00, 96.88it/s]\n",
      "2025-08-08 10:52:22,972 - INFO - Lote 11/15 completado\n",
      "2025-08-08 10:52:22,974 - INFO - Procesando lote 12/15 con 6 archivos\n",
      "Batch 12/15:   0%|          | 0/6 [00:00<?, ?it/s]2025-08-08 10:52:22,976 - INFO - [PDF 67] Inicio del procesamiento del PDF: L1_IV_cap_I.pdf\n",
      "2025-08-08 10:52:22,976 - INFO - [PDF 70] Inicio del procesamiento del PDF: Riesgo de Crédito.pdf\n",
      "2025-08-08 10:52:22,976 - INFO - [PDF 69] Inicio del procesamiento del PDF: L1_VII_cap_II.pdf\n",
      "2025-08-08 10:52:22,976 - INFO - [PDF 66] Inicio del procesamiento del PDF: L1_XIX_cap_III.pdf\n",
      "2025-08-08 10:52:22,977 - INFO - [PDF 67] Procesando PDF: L1_IV_cap_I.pdf\n",
      "2025-08-08 10:52:22,976 - INFO - [PDF 71] Inicio del procesamiento del PDF: L1_II_cap_II.pdf\n",
      "2025-08-08 10:52:22,976 - INFO - [PDF 68] Inicio del procesamiento del PDF: cap_IV ADMINISTRACION RIESGO DE LIQUIDEZ.pdf\n",
      "2025-08-08 10:52:22,978 - INFO - [PDF 70] Procesando PDF: Riesgo de Crédito.pdf\n",
      "2025-08-08 10:52:22,979 - INFO - [PDF 66] Procesando PDF: L1_XIX_cap_III.pdf\n",
      "2025-08-08 10:52:22,979 - INFO - [PDF 69] Procesando PDF: L1_VII_cap_II.pdf\n",
      "2025-08-08 10:52:22,980 - INFO - [PDF 71] Procesando PDF: L1_II_cap_II.pdf\n",
      "2025-08-08 10:52:22,980 - INFO - [PDF 68] Procesando PDF: cap_IV ADMINISTRACION RIESGO DE LIQUIDEZ.pdf\n",
      "2025-08-08 10:52:22,986 - INFO - [PDF 67] Procesando 3 fragmentos\n",
      "2025-08-08 10:52:22,987 - INFO - [PDF 66] Procesando 3 fragmentos\n",
      "2025-08-08 10:52:22,990 - INFO - [PDF 69] Procesando 3 fragmentos\n",
      "2025-08-08 10:52:22,993 - INFO - [PDF 71] Procesando 13 fragmentos\n",
      "2025-08-08 10:52:22,994 - INFO - [PDF 70] Procesando 11 fragmentos\n",
      "2025-08-08 10:52:22,996 - INFO - [PDF 68] Procesando 16 fragmentos\n",
      "2025-08-08 10:52:22,999 - INFO - [PDF 67] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:23,007 - INFO - [PDF 66] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:23,009 - INFO - [PDF 67] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:23,010 - INFO - [PDF 66] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:23,013 - INFO - [PDF 69] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:23,018 - INFO - [PDF 69] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:23,060 - INFO - [PDF 71] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:23,061 - INFO - [PDF 70] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:23,066 - INFO - [PDF 71] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:23,067 - INFO - [PDF 70] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:23,088 - INFO - [PDF 68] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:23,092 - INFO - [PDF 68] Procesamiento del PDF completado\n",
      "Batch 12/15: 100%|██████████| 6/6 [00:00<00:00, 51.04it/s]\n",
      "2025-08-08 10:52:23,096 - INFO - Lote 12/15 completado\n",
      "2025-08-08 10:52:23,098 - INFO - Procesando lote 13/15 con 6 archivos\n",
      "Batch 13/15:   0%|          | 0/6 [00:00<?, ?it/s]2025-08-08 10:52:23,100 - INFO - [PDF 72] Inicio del procesamiento del PDF: L1_I_cap_V.pdf\n",
      "2025-08-08 10:52:23,101 - INFO - [PDF 73] Inicio del procesamiento del PDF: L1_XVIII_cap_IV.pdf\n",
      "2025-08-08 10:52:23,102 - INFO - [PDF 74] Inicio del procesamiento del PDF: L1_XI_cap_VI.pdf\n",
      "2025-08-08 10:52:23,102 - INFO - [PDF 77] Inicio del procesamiento del PDF: L1_XII_cap_III.pdf\n",
      "2025-08-08 10:52:23,102 - INFO - [PDF 75] Inicio del procesamiento del PDF: L1_VIII_cap_III.pdf\n",
      "2025-08-08 10:52:23,102 - INFO - [PDF 76] Inicio del procesamiento del PDF: L1_VIII_cap_IV.pdf\n",
      "2025-08-08 10:52:23,109 - INFO - [PDF 72] Procesando PDF: L1_I_cap_V.pdf\n",
      "2025-08-08 10:52:23,110 - INFO - [PDF 74] Procesando PDF: L1_XI_cap_VI.pdf\n",
      "2025-08-08 10:52:23,110 - INFO - [PDF 73] Procesando PDF: L1_XVIII_cap_IV.pdf\n",
      "2025-08-08 10:52:23,110 - INFO - [PDF 77] Procesando PDF: L1_XII_cap_III.pdf\n",
      "2025-08-08 10:52:23,111 - INFO - [PDF 76] Procesando PDF: L1_VIII_cap_IV.pdf\n",
      "2025-08-08 10:52:23,110 - INFO - [PDF 75] Procesando PDF: L1_VIII_cap_III.pdf\n",
      "2025-08-08 10:52:23,123 - INFO - [PDF 73] Procesando 1 fragmentos\n",
      "2025-08-08 10:52:23,123 - INFO - [PDF 72] Procesando 3 fragmentos\n",
      "2025-08-08 10:52:23,124 - INFO - [PDF 74] Procesando 2 fragmentos\n",
      "2025-08-08 10:52:23,125 - INFO - [PDF 77] Procesando 1 fragmentos\n",
      "2025-08-08 10:52:23,126 - INFO - [PDF 75] Procesando 5 fragmentos\n",
      "2025-08-08 10:52:23,128 - INFO - [PDF 76] Procesando 21 fragmentos\n",
      "2025-08-08 10:52:23,131 - INFO - [PDF 73] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:23,133 - INFO - [PDF 77] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:23,135 - INFO - [PDF 73] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:23,137 - INFO - [PDF 77] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:23,140 - INFO - [PDF 72] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:23,144 - INFO - [PDF 72] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:23,152 - INFO - [PDF 75] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:23,156 - INFO - [PDF 75] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:23,259 - INFO - [PDF 76] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:23,264 - INFO - [PDF 76] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:26,611 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:52:26,624 - INFO - [PDF 74] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:26,628 - INFO - [PDF 74] Procesamiento del PDF completado\n",
      "Batch 13/15: 100%|██████████| 6/6 [00:03<00:00,  1.70it/s]\n",
      "2025-08-08 10:52:26,634 - INFO - Lote 13/15 completado\n",
      "2025-08-08 10:52:26,635 - INFO - Procesando lote 14/15 con 6 archivos\n",
      "Batch 14/15:   0%|          | 0/6 [00:00<?, ?it/s]2025-08-08 10:52:26,639 - INFO - [PDF 78] Inicio del procesamiento del PDF: cap_I DE LA GESTION Y ADMINISTRACION DE RIESGOS.pdf\n",
      "2025-08-08 10:52:26,639 - INFO - [PDF 82] Inicio del procesamiento del PDF: L1_XVIII_cap_II.pdf\n",
      "2025-08-08 10:52:26,639 - INFO - [PDF 79] Inicio del procesamiento del PDF: L1_III_cap_VI.pdf\n",
      "2025-08-08 10:52:26,639 - INFO - [PDF 83] Inicio del procesamiento del PDF: L1_IX_cap_II.pdf\n",
      "2025-08-08 10:52:26,639 - INFO - [PDF 81] Inicio del procesamiento del PDF: L1_XI_cap_IV.pdf\n",
      "2025-08-08 10:52:26,639 - INFO - [PDF 80] Inicio del procesamiento del PDF: L1_IX_cap_VIII.pdf\n",
      "2025-08-08 10:52:26,641 - INFO - [PDF 82] Procesando PDF: L1_XVIII_cap_II.pdf\n",
      "2025-08-08 10:52:26,641 - INFO - [PDF 78] Procesando PDF: cap_I DE LA GESTION Y ADMINISTRACION DE RIESGOS.pdf\n",
      "2025-08-08 10:52:26,642 - INFO - [PDF 83] Procesando PDF: L1_IX_cap_II.pdf\n",
      "2025-08-08 10:52:26,642 - INFO - [PDF 79] Procesando PDF: L1_III_cap_VI.pdf\n",
      "2025-08-08 10:52:26,642 - INFO - [PDF 81] Procesando PDF: L1_XI_cap_IV.pdf\n",
      "2025-08-08 10:52:26,643 - INFO - [PDF 80] Procesando PDF: L1_IX_cap_VIII.pdf\n",
      "2025-08-08 10:52:26,652 - INFO - [PDF 82] Procesando 1 fragmentos\n",
      "2025-08-08 10:52:26,653 - INFO - [PDF 78] Procesando 15 fragmentos\n",
      "2025-08-08 10:52:26,653 - INFO - [PDF 81] Procesando 2 fragmentos\n",
      "2025-08-08 10:52:26,654 - INFO - [PDF 79] Procesando 5 fragmentos\n",
      "2025-08-08 10:52:26,657 - INFO - [PDF 82] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:26,658 - INFO - [PDF 83] Procesando 25 fragmentos\n",
      "2025-08-08 10:52:26,661 - INFO - [PDF 82] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:26,660 - INFO - [PDF 80] Procesando 28 fragmentos\n",
      "2025-08-08 10:52:26,664 - INFO - [PDF 81] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:26,668 - INFO - [PDF 81] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:26,679 - INFO - [PDF 79] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:26,685 - INFO - [PDF 79] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:26,715 - INFO - [PDF 78] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:26,719 - INFO - [PDF 78] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:26,800 - INFO - [PDF 80] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:26,818 - INFO - [PDF 83] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:26,821 - INFO - [PDF 80] Procesamiento del PDF completado\n",
      "Batch 14/15:  50%|█████     | 3/6 [00:00<00:00, 16.33it/s]2025-08-08 10:52:26,823 - INFO - [PDF 83] Procesamiento del PDF completado\n",
      "Batch 14/15: 100%|██████████| 6/6 [00:00<00:00, 32.19it/s]\n",
      "2025-08-08 10:52:26,827 - INFO - Lote 14/15 completado\n",
      "2025-08-08 10:52:26,828 - INFO - Procesando lote 15/15 con 5 archivos\n",
      "Batch 15/15:   0%|          | 0/5 [00:00<?, ?it/s]2025-08-08 10:52:26,831 - INFO - [PDF 84] Inicio del procesamiento del PDF: L1_XI_cap_I.pdf\n",
      "2025-08-08 10:52:26,832 - INFO - [PDF 86] Inicio del procesamiento del PDF: L1_XII_cap_I.pdf\n",
      "2025-08-08 10:52:26,832 - INFO - [PDF 85] Inicio del procesamiento del PDF: L1_XIII_cap_VI.pdf\n",
      "2025-08-08 10:52:26,832 - INFO - [PDF 88] Inicio del procesamiento del PDF: L1_I_cap_III.pdf\n",
      "2025-08-08 10:52:26,832 - INFO - [PDF 87] Inicio del procesamiento del PDF: cap_II ADMINISTRACION RIESGO DE CREDITO.pdf\n",
      "2025-08-08 10:52:26,834 - INFO - [PDF 86] Procesando PDF: L1_XII_cap_I.pdf\n",
      "2025-08-08 10:52:26,834 - INFO - [PDF 84] Procesando PDF: L1_XI_cap_I.pdf\n",
      "2025-08-08 10:52:26,834 - INFO - [PDF 85] Procesando PDF: L1_XIII_cap_VI.pdf\n",
      "2025-08-08 10:52:26,835 - INFO - [PDF 87] Procesando PDF: cap_II ADMINISTRACION RIESGO DE CREDITO.pdf\n",
      "2025-08-08 10:52:26,835 - INFO - [PDF 88] Procesando PDF: L1_I_cap_III.pdf\n",
      "2025-08-08 10:52:26,844 - INFO - [PDF 86] Procesando 6 fragmentos\n",
      "2025-08-08 10:52:26,846 - INFO - [PDF 84] Procesando 1 fragmentos\n",
      "2025-08-08 10:52:26,847 - INFO - [PDF 88] Procesando 6 fragmentos\n",
      "2025-08-08 10:52:26,849 - INFO - [PDF 87] Procesando 9 fragmentos\n",
      "2025-08-08 10:52:26,849 - INFO - [PDF 85] Procesando 11 fragmentos\n",
      "2025-08-08 10:52:26,852 - INFO - [PDF 84] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:26,857 - INFO - [PDF 84] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:26,874 - INFO - [PDF 88] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:26,878 - INFO - [PDF 88] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:26,897 - INFO - [PDF 87] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:26,908 - INFO - [PDF 85] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:26,914 - INFO - [PDF 85] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:26,918 - INFO - [PDF 87] Procesamiento del PDF completado\n",
      "2025-08-08 10:52:31,066 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 10:52:31,082 - INFO - [PDF 86] Dataset conversacional generado y guardado en JSONL\n",
      "2025-08-08 10:52:31,088 - INFO - [PDF 86] Procesamiento del PDF completado\n",
      "Batch 15/15: 100%|██████████| 5/5 [00:04<00:00,  1.17it/s]\n",
      "2025-08-08 10:52:31,093 - INFO - Lote 15/15 completado\n",
      "2025-08-08 10:52:31,143 - INFO - Generación de datasets finalizada\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 550 ms, sys: 439 ms, total: 989 ms\n",
      "Wall time: 5min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "files = get_files()\n",
    "generate(files,max_workers=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a4ebf-86b7-4ce0-a89a-975ee4a86092",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79e48dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.11/site-packages (4.0.0)\n",
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.11/site-packages (0.34.3)\n",
      "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.11/site-packages (8.1.7)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.11/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.11/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in ./.venv/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (1.1.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (9.4.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./.venv/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade datasets huggingface_hub ipywidgets pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1685376-51e6-4287-a74e-26a1e3deffb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/fine-tunning-models/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading data: 100%|██████████| 64/64 [00:00<00:00, 374909.85files/s]\n",
      "Generating train split: 4682 examples [00:00, 17601.63 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages', 'topic'],\n",
       "        num_rows: 4682\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"./outputs/data/*.jsonl\")\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "151e67bb-0111-4991-b87d-e724b183ea9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 55.02ba/s]\n",
      "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            \n",
      "\u001b[A\n",
      "Processing Files (0 / 1)                :  14%|█▎        |  524kB / 3.86MB, 1.31MB/s  \n",
      "Processing Files (0 / 1)                :  27%|██▋       | 1.05MB / 3.86MB, 1.75MB/s  \n",
      "Processing Files (0 / 1)                :  95%|█████████▌| 3.67MB / 3.86MB, 4.59MB/s  \n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████| 3.86MB / 3.86MB, 3.21MB/s  \n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████| 3.86MB / 3.86MB, 2.76MB/s  \n",
      "New Data Upload                         : 100%|██████████| 3.86MB / 3.86MB, 2.76MB/s  \n",
      "                                        : 100%|██████████| 3.86MB / 3.86MB            \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.87s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/jeanmcm/b_risks/commit/6f625bb43e6b588893a70d8769613ed11130179a', commit_message='Upload dataset', commit_description='', oid='6f625bb43e6b588893a70d8769613ed11130179a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/jeanmcm/b_risks', endpoint='https://huggingface.co', repo_type='dataset', repo_id='jeanmcm/b_risks'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "hf_token = os.environ.get(\"HUGGING_FACE_KEY\")\n",
    "dataset.push_to_hub(\"jeanmcm/b_risks\",token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f0891",
   "metadata": {},
   "source": [
    "# Testing RAG with Open WebUI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ce0bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Respuesta en streaming:</b> El riesgo financiero se refiere a las posibles pérdidas o daños económicos que pueden afectar a una empresa, individuo u organización debido a sus actividades financieras. Esto puede incluir la incertidumbre sobre los rendimientos de los activos financieros, el valor de los bienes y servicios, o la capacidad de un individuo o organización para generar ingresos.\n",
       "\n",
       "En el contexto del sistema financiero, las instituciones deben conocer a fondo las características particulares de las actividades económicas de sus clientes y las operaciones que realizan para identificar y gestionar los riesgos financieros asociados. Esto es fundamental para prevenir la ocurrencia de incumplimientos en las regulaciones financieras y minimizar el impacto de cualquier pérdida o daño.\n",
       "\n",
       "Según el artículo 43 del \"Bsoft Documents\", las instituciones del sistema financiero deben contar con una unidad de cumplimiento dirigida por un oficial de cumplimiento, que debe tener formación profesional en áreas como administración, contaduría, derecho o economía. Este equipo es responsable de garantizar que las políticas y procedimientos financieros sean adecuados y cumplan con los requisitos legales.\n",
       "\n",
       "Por lo tanto, el riesgo financiero se considera un aspecto crítico para las instituciones del sistema financiero, ya que puede tener graves consecuencias en su estabilidad y solvencia. Es importante que estas entidades tomen medidas proactivas para identificar, evaluar y gestionar los riesgos financieros asociados con sus actividades y operaciones.\n",
       "\n",
       "No se incluye ninguna cita porque no hay un id atributo en el source tag correspondiente a esta información, pero la respuesta se basa en el contexto proporcionado.."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "from IPython.display import display, clear_output, HTML\n",
    "\n",
    "url = \"https://vwlppjjfa98c9x-8080.proxy.runpod.net\"\n",
    "api_key =\"sk-05568562f28844fe997cadf960a346cd\"\n",
    "\n",
    "messages =  [{\"role\": \"user\", \"content\": \"Que es el Riesgo Financiero?\"}]\n",
    "\n",
    "try:\n",
    "    # Realizar la solicitud con stream=True\n",
    "    with requests.post(f\"{url}/api/chat/completions\", stream=True,headers={\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"Authorization\": f\"Bearer {api_key}\"\n",
    "    },json={\n",
    "      \"model\":\"bosft-riesgos-rag-model\",\n",
    "      \"messages\":messages,\n",
    "      \"stream\":True\n",
    "      }) as response:\n",
    "        response.raise_for_status()\n",
    "        # Variable para almacenar la salida acumulada\n",
    "        accumulated_output = \"\"\n",
    "\n",
    "        # Iterar sobre las líneas de la respuesta\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                # Decodificar la línea\n",
    "                decoded_line = line.decode('utf-8').strip()\n",
    "                # Si la línea comienza con \"data:\", extraer el contenido\n",
    "                if decoded_line.startswith(\"data:\"):\n",
    "                    decoded_line = decoded_line[5:].strip()  # Quitar \"data: \"\n",
    "\n",
    "                # Ignorar líneas vacías o marcadores de fin como \"[DONE]\"\n",
    "                if not decoded_line or decoded_line == \"[DONE]\" :\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    # Parsear si es JSON\n",
    "                    data = json.loads(decoded_line)\n",
    "                    if \"choices\" not in data: continue\n",
    "                    \n",
    "                    delta = data['choices'][0]['delta']\n",
    "                    if \"content\" in delta: new_data = delta['content']\n",
    "                except json.JSONDecodeError:\n",
    "                    # Si no es JSON, usar la línea como texto\n",
    "                    new_data = decoded_line\n",
    "\n",
    "                # Acumular y mostrar la salida dinámicamente\n",
    "                if new_data:\n",
    "                    accumulated_output += new_data\n",
    "                    # Limpiar la salida anterior y mostrar la nueva\n",
    "                    clear_output(wait=True)\n",
    "                    display(HTML(f\"<b>Respuesta en streaming:</b> {accumulated_output}\"))\n",
    "                    time.sleep(0.1)  # Pequeña pausa para visibilidad\n",
    "                    \n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"\\nError en la solicitud: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e64806a-ac56-417a-8dd9-175c3a8447a1",
   "metadata": {},
   "source": [
    "# Testing with Flowise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797cb1c0-bb15-4c97-9e0c-0156e8b90df1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
