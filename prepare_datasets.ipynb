{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "067529bb",
   "metadata": {},
   "source": [
    "# Generacion de Datasets utilizando RLHF para fine tunning\n",
    "\n",
    "## Procedimiento\n",
    "\n",
    "- Recoleccion de Datos: Obtener Informacion mediante documentos de textos seleccionables (no escaneados), mediante web, redes sociales, scrapping,etc\n",
    "- Extraer Texto: Usa PyMuPDF para extraer texto de PDFs no escaneados, procesando en paralelo con ProcessPoolExecutor para manejar grandes volúmenes (>100, >10,000).\n",
    "- Limpiar Datos: Elimina espacios múltiples y caracteres no deseados con expresiones regulares, asegurando texto coherente.\n",
    "- Validar: Verifica que los fragmentos extraídos no estén vacíos y tengan una longitud mínima (por ejemplo, 50 caracteres).\n",
    "- Generar Conversaciones: Divide el texto en párrafos, usa un modelo como meta-llama/Llama-3.2-1B-Instruct para generar diálogos conversacionales específicos al contenido, con al menos 4 intercambios por conversación, en formato {\"messages\": [{\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}], \"topic\": \"...\"}.\n",
    "- Estructurar Dataset: Guarda las conversaciones en archivos JSONL con dos columnas: \"messages\" y \"topic\".\n",
    "Cargar y Subir: Carga el dataset con load_dataset(\"json\", data_files=\"path/*.jsonl\") y súbelo a un repositorio privado en Hugging Face Hub con push_to_hub(\"username/dataset_name\", private=True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fa351d6-c527-423f-9d2e-417d0ef2b8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45295ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.26.3-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting ollama\n",
      "  Downloading ollama-0.5.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.7.34-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting httpx>=0.27 (from ollama)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydantic>=2.9 (from ollama)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting anyio (from httpx>=0.27->ollama)\n",
      "  Downloading anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting certifi (from httpx>=0.27->ollama)\n",
      "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.27->ollama)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx>=0.27->ollama)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.27->ollama)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9->ollama)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2.9->ollama)\n",
      "  Downloading pydantic_core-2.33.2-cp310-cp310-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\cesar burbano\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=2.9->ollama) (4.12.2)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.9->ollama)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\cesar burbano\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\cesar burbano\\appdata\\roaming\\python\\python310\\site-packages (from anyio->httpx>=0.27->ollama) (1.3.0)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx>=0.27->ollama)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Downloading pymupdf-1.26.3-cp39-abi3-win_amd64.whl (18.7 MB)\n",
      "   ---------------------------------------- 0.0/18.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/18.7 MB 4.8 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 2.9/18.7 MB 8.4 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 5.5/18.7 MB 9.6 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 8.1/18.7 MB 10.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 10.0/18.7 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 12.1/18.7 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 14.4/18.7 MB 10.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 16.5/18.7 MB 10.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.6/18.7 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.7/18.7 MB 10.1 MB/s eta 0:00:00\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 11.4 MB/s eta 0:00:00\n",
      "Downloading ollama-0.5.1-py3-none-any.whl (13 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 10.9 MB/s eta 0:00:00\n",
      "Downloading regex-2025.7.34-cp310-cp310-win_amd64.whl (276 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading anyio-4.10.0-py3-none-any.whl (107 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: typing-inspection, tqdm, sniffio, regex, pymupdf, pydantic-core, joblib, idna, h11, click, certifi, annotated-types, pydantic, nltk, httpcore, anyio, httpx, ollama\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.10.0 certifi-2025.8.3 click-8.2.1 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 joblib-1.5.1 nltk-3.9.1 ollama-0.5.1 pydantic-2.11.7 pydantic-core-2.33.2 pymupdf-1.26.3 regex-2025.7.34 sniffio-1.3.1 tqdm-4.67.1 typing-inspection-0.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pymupdf nltk ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4760f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "MIN_FRAGMENT_LENGTH = 500  # Longitud mínima de un fragmento\n",
    "MAX_FRAGMENT_LENGTH = 2000  # Longitud máxima de un fragmento\n",
    "REPEAT_THRESHOLD = 0.3  # Umbral para considerar un bloque como repetitivo\n",
    "\n",
    "OLLAMA_MODEL = \"llama3.1:8b\"\n",
    "TEMPERATURE = 0.7  # Equilibrio entre creatividad y coherencia\n",
    "TOP_P = 0.9  # Filtrado de núcleo para diversidad\n",
    "MIN_CONVERSATION_LENGTH = 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62be033f",
   "metadata": {},
   "source": [
    "### Recoleccion de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b45b0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Folder\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Verificar si la carpeta existe\n",
    "\n",
    "folder_url = \"./docs\"\n",
    "folder = Path(folder_url)\n",
    "\n",
    "\n",
    "if folder.exists() and folder.is_dir():\n",
    "    print(\"Valid Folder\")\n",
    "    \n",
    "# Obtener todos los archivos de la carpeta\n",
    "files = [f for f in folder.rglob(\"*\") if f.is_file()]\n",
    "\n",
    "files\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce4f060",
   "metadata": {},
   "source": [
    "### Extraccion de Texto y Limpieza de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ed080bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import re\n",
    "import os\n",
    "import hashlib\n",
    "from collections import Counter\n",
    "\n",
    "# Constantes\n",
    "MIN_FRAGMENT_LENGTH = 500\n",
    "MAX_FRAGMENT_LENGTH = 2000\n",
    "REPEAT_THRESHOLD = 0.3\n",
    "MIN_BLOCK_LENGTH = 30  # Reducido de 50 a 30 para incluir bloques más cortos\n",
    "\n",
    "def clean_text(text, repeated_blocks=None):\n",
    "    \"\"\"Limpia el texto, eliminando caracteres repetitivos, texto redundante y caracteres no deseados.\"\"\"\n",
    "    # Eliminar caracteres de control no deseados (excepto \\n)\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]', '', text)\n",
    "    # Eliminar patrones repetitivos como ----, ...., ****\n",
    "    text = re.sub(r'([^\\w\\s])\\1{2,}|\\s*[.]{3,}\\s*', '', text)\n",
    "    # Normalizar espacios múltiples\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    # Eliminar espacios al inicio y final de cada línea\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    # Filtrar líneas duplicadas, números, correos y contenido administrativo\n",
    "    seen_lines = set()\n",
    "    unique_lines = []\n",
    "    for line in lines:\n",
    "        if line not in seen_lines and \\\n",
    "           not re.match(r'^\\d+$', line) and \\\n",
    "           not re.match(r'.*@(.*\\.)+.*', line) and \\\n",
    "           not re.match(r'^(Tel|Fax|E-mail|www\\.).*', line, re.IGNORECASE) and \\\n",
    "           (repeated_blocks is None or line not in repeated_blocks):\n",
    "            unique_lines.append(line)\n",
    "            seen_lines.add(line)\n",
    "    return '\\n'.join(unique_lines)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extrae texto de un PDF en una sola pasada, preservando el texto del inicio de la página,\n",
    "    eliminando encabezados, pies de página y contenido irrelevante,\n",
    "    dividiendo en fragmentos de 500 a 2000 caracteres con metadata corregida.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = pymupdf.open(pdf_path)\n",
    "        total_pages = len(doc)\n",
    "        chunks = []\n",
    "        current_chunk = []  # Lista de (párrafo, página)\n",
    "        current_chunk_length = 0\n",
    "        filename = os.path.basename(pdf_path)\n",
    "        block_counter = Counter()\n",
    "\n",
    "        for page_number in range(1, total_pages + 1):\n",
    "            page = doc[page_number - 1]\n",
    "            page_height = page.rect.height\n",
    "            blocks = page.get_text(\"blocks\")\n",
    "            page_text = []\n",
    "\n",
    "            # Procesar bloques y filtrar encabezados/pies de página\n",
    "            for block in blocks:\n",
    "                text = block[4]\n",
    "                y0, y1 = block[1], block[3]\n",
    "                # Excluir pies de página (parte inferior de la página)\n",
    "                if y1 > 0.95 * page_height:  # Relajado de 0.95 a 0.9\n",
    "                    continue\n",
    "                # Excluir encabezados solo si son repetitivos\n",
    "                if y0 < 0.05 * page_height:  # Relajado de 0.05 a 0.1\n",
    "                    block_counter[text] += 1\n",
    "                    if block_counter[text] > total_pages * REPEAT_THRESHOLD:\n",
    "                        continue\n",
    "                if text and len(text) >= MIN_BLOCK_LENGTH:\n",
    "                    block_counter[text] += 1\n",
    "                    page_text.append((text, page_number))\n",
    "\n",
    "            # Si no hay texto válido en la página, continuar\n",
    "            if not page_text:\n",
    "                continue\n",
    "\n",
    "            # Acumular párrafos con su número de página\n",
    "            for paragraph, page in page_text:\n",
    "                current_chunk.append((paragraph, page))\n",
    "                current_chunk_length += len(paragraph) + 2  # +2 por \"\\n\\n\"\n",
    "\n",
    "                # Si el fragmento alcanza la longitud mínima, procesarlo\n",
    "                if current_chunk_length >= MIN_FRAGMENT_LENGTH:\n",
    "                    chunk_text = \"\\n\\n\".join(p for p, _ in current_chunk)\n",
    "                    cleaned_chunk = clean_text(chunk_text, None)\n",
    "                    cleaned_length = len(cleaned_chunk)\n",
    "\n",
    "                    # Obtener el rango de páginas del fragmento\n",
    "                    page_numbers = sorted(set(page for _, page in current_chunk))\n",
    "                    start_page = page_numbers[0] if page_numbers else page_number\n",
    "                    end_page = page_numbers[-1] if page_numbers else page_number\n",
    "\n",
    "                    # Dividir fragmentos largos\n",
    "                    while cleaned_length > MAX_FRAGMENT_LENGTH:\n",
    "                        sub_chunk = cleaned_chunk[:MAX_FRAGMENT_LENGTH]\n",
    "                        last_paragraph_end = sub_chunk.rfind(\"\\n\\n\")\n",
    "                        if last_paragraph_end == -1:\n",
    "                            last_paragraph_end = MAX_FRAGMENT_LENGTH\n",
    "                        chunk_to_add = cleaned_chunk[:last_paragraph_end].strip()\n",
    "\n",
    "                        # Calcular el número de páginas para el subfragmento\n",
    "                        chars_so_far = 0\n",
    "                        sub_chunk_pages = []\n",
    "                        for paragraph, page in current_chunk:\n",
    "                            chars_so_far += len(paragraph) + 2\n",
    "                            if chars_so_far <= last_paragraph_end:\n",
    "                                sub_chunk_pages.append(page)\n",
    "                            else:\n",
    "                                break\n",
    "                        sub_start_page = min(sub_chunk_pages) if sub_chunk_pages else page_number\n",
    "                        sub_end_page = max(sub_chunk_pages) if sub_chunk_pages else page_number\n",
    "\n",
    "                        metadata = (\n",
    "                            f\"# FILENAME: {filename} | CHARACTERS: {len(chunk_to_add)} | \"\n",
    "                            f\"PAGES: {sub_start_page}-{sub_end_page}/{total_pages}\\n\\n\"\n",
    "                        )\n",
    "                        chunks.append((metadata + chunk_to_add, hashlib.md5(chunk_to_add.encode()).hexdigest()))\n",
    "                        cleaned_chunk = cleaned_chunk[last_paragraph_end:].strip()\n",
    "                        cleaned_length = len(cleaned_chunk)\n",
    "\n",
    "                        # Actualizar current_chunk para los párrafos restantes\n",
    "                        remaining_chunk = []\n",
    "                        chars_so_far = 0\n",
    "                        for paragraph, page in current_chunk:\n",
    "                            chars_so_far += len(paragraph) + 2\n",
    "                            if chars_so_far > last_paragraph_end:\n",
    "                                remaining_chunk.append((paragraph, page))\n",
    "                        current_chunk = remaining_chunk\n",
    "                        current_chunk_length = cleaned_length\n",
    "                        page_numbers = sorted(set(page for _, page in current_chunk))\n",
    "                        start_page = page_numbers[0] if page_numbers else page_number\n",
    "\n",
    "                    # Añadir el fragmento completo\n",
    "                    if cleaned_length >= MIN_FRAGMENT_LENGTH:\n",
    "                        metadata = (\n",
    "                            f\"# FILENAME: {filename} | CHARACTERS: {cleaned_length} | \"\n",
    "                            f\"PAGES: {start_page}-{end_page}/{total_pages}\\n\\n\"\n",
    "                        )\n",
    "                        chunks.append((metadata + cleaned_chunk, hashlib.md5(cleaned_chunk.encode()).hexdigest()))\n",
    "                        current_chunk = []\n",
    "                        current_chunk_length = 0\n",
    "\n",
    "                    else:\n",
    "                        current_chunk = [(cleaned_chunk, page_numbers[-1])] if page_numbers else []\n",
    "                        current_chunk_length = cleaned_length\n",
    "\n",
    "        # Añadir el fragmento final si cumple con la longitud mínima\n",
    "        if current_chunk and current_chunk_length >= MIN_FRAGMENT_LENGTH:\n",
    "            chunk_text = \"\\n\\n\".join(p for p, _ in current_chunk)\n",
    "            cleaned_chunk = clean_text(chunk_text, None)\n",
    "            cleaned_length = len(cleaned_chunk)\n",
    "            if cleaned_length >= MIN_FRAGMENT_LENGTH:\n",
    "                page_numbers = sorted(set(page for _, page in current_chunk))\n",
    "                start_page = page_numbers[0] if page_numbers else total_pages\n",
    "                end_page = page_numbers[-1] if page_numbers else total_pages\n",
    "                metadata = (\n",
    "                    f\"# FILENAME: {filename} | CHARACTERS: {cleaned_length} | \"\n",
    "                    f\"PAGES: {start_page}-{end_page}/{total_pages}\\n\\n\"\n",
    "                )\n",
    "                chunks.append((metadata + cleaned_chunk, hashlib.md5(cleaned_chunk.encode()).hexdigest()))\n",
    "\n",
    "        doc.close()\n",
    "\n",
    "        # Filtrar bloques repetitivos y duplicados\n",
    "        repeated_blocks = {text for text, count in block_counter.items() if count > total_pages * REPEAT_THRESHOLD}\n",
    "        final_chunks = []\n",
    "        seen_hashes = set()\n",
    "\n",
    "        for chunk, chunk_hash in chunks:\n",
    "            chunk_text = '\\n'.join(line for line in chunk.splitlines() if not line.startswith('#'))\n",
    "            cleaned_chunk = clean_text(chunk_text, repeated_blocks)\n",
    "            if len(cleaned_chunk) >= MIN_FRAGMENT_LENGTH and chunk_hash not in seen_hashes:\n",
    "                # Actualizar la longitud en la metadata después de la limpieza final\n",
    "                metadata_lines = chunk.splitlines()[0]\n",
    "                metadata = re.sub(r'CHARACTERS: \\d+', f'CHARACTERS: {len(cleaned_chunk)}', metadata_lines)\n",
    "                final_chunks.append(f\"{metadata}\\n\\n{cleaned_chunk}\")\n",
    "                seen_hashes.add(chunk_hash)\n",
    "\n",
    "        return final_chunks if final_chunks else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {pdf_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6e30814",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks=extract_text_from_pdf(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f9ea3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# FILENAME: 9 Recomendaciones Especiales GAFI.pdf | CHARACTERS: 2000 | PAGES: 1-1/3\\n\\nFinancial Action Task Force on Money Laundering\\nGrupo de Acción Financiera sobre el lavado de activos\\nGroupe d’action financière sur le blanchiment de capitaux\\nLAS 9 RECOMENDACIONES ESPECIALES\\nCONTRA EL FINANCIAMIENTO DEL TERRORISMO\\nGrupo de Acción Financiera de Sudamérica\\nGrupo de Açâo Financeira da América do Sul\\nI.\\nRatificación y ejecución de los instrumentos de las NU\\nTodos los países deben dar pasos inmediatos para ratificar y ejecutar plenamente la Convención\\nInternacional de las Naciones Unidas de 1999 para la Eliminación del Financiamiento del\\nTerrorismo.\\nLos países deben también poner en práctica de forma inmediata las resoluciones de las Naciones\\nUnidas relacionadas con la prevención y eliminación del financiamiento de actos terroristas,\\nparticularmente la Resolución 1373 del Consejo de Seguridad de las Naciones Unidas.\\nII.\\nTipificación del financiamiento del terrorismo y el lavado de dinero asociado\\nTodos los países deben tipificar el financiamiento del terrorismo, de los actos terroristas y de las\\norganizaciones terroristas. Los países deben asegurar que estos delitos sean designados como\\ndelitos precedentes del lavado de dinero.\\nIII. Congelamiento y decomiso de activos terroristas\\nLos países deben poner en práctica medidas para congelar, sin demora alguna, los fondos u otros\\nactivos de los terroristas, de aquellos que financian el terrorismo y de las organizaciones terroristas,\\nde acuerdo con las resoluciones de las Naciones Unidas relacionadas con la prevención y\\neliminación del financiamiento de los actos terroristas.\\nLos países deben también adoptar y ejecutar medidas, incluyendo medidas legislativas, que les\\npermitan a las autoridades competentes capturar y confiscar la propiedad que constituye el activo\\nde, o que ha sido utilizada en, o que se pretende utilizar o está destinada para ser utilizada en, el\\nfinanciamiento del terrorismo, de actos terroristas o de organizaciones terroristas.\\nIV.\\nReporte de transacciones sospechosas relacionadas con el t',\n",
       " '# FILENAME: 9 Recomendaciones Especiales GAFI.pdf | CHARACTERS: 1593 | PAGES: 2-2/3\\n\\nerrorismo\\nSi las instituciones financieras u otros negocios o entidades sujetas a las obligaciones contra el\\nlavado de dinero, sospechan o tienen argumentos razonables para sospechar que los fondos están\\nligados o relacionados con, o van a ser utilizados para el terrorismo, actos terroristas o por\\norganizaciones terroristas, a estas debe exigírseles que reporten con premura sus sospechas a las\\nautoridades competentes.\\nV.\\nCooperación Internacional\\nLos países deben permitirle a los demás países, sobre la base de un tratado, acuerdo u otro\\nmecanismo para la prestación de ayuda legal mutua o intercambio de información, la mayor ayuda\\nposible con respecto a la ejecución penal, civil e investigaciones administrativas, pesquisas y\\nprocedimientos relacionados con el financiamiento del terrorismo, actos terroristas y organizaciones\\nterroristas.\\nLos países deben también tomar todas las medidas posibles para asegurar que no ofrezcan paraísos\\nseguros para individuos acusados de financiar el terrorismo, actos terroristas u organizaciones\\nterroristas, y deben tener establecidos procedimientos para extraditar a tales individuos, cuando\\nsea posible.\\nVI.\\nSistemas alternativos de envíos de fondos\\nLos países deben tomar medidas para asegurar que las personas naturales o jurídicas, incluyendo a\\nlos agentes, que prestan un servicio para la transmisión de dinero o valor, incluyendo la transmisión\\na través de un sistema o red informal de transferencia de dinero o valor, reciban licencia o sean\\nregistrados y estén sujetos a todas las Recomendaciones del GAFI que se apliquen a los bancos y a',\n",
       " '# FILENAME: 9 Recomendaciones Especiales GAFI.pdf | CHARACTERS: 1333 | PAGES: 3-3/3\\n\\nlas instituciones financieras no bancarias. Cada país debe asegurar que las personas o entidades\\nlegales que lleven a cabo este servicio ilícitamente estén sujetas a sanciones administrativas, civiles\\no penales.\\nVII. Transferencias electrónicas\\nLos países deben tomar medidas para exigirle a las instituciones financieras, incluyendo a los que\\nenvían dinero, que incorporen información precisa y significativa del autor (nombre, dirección y\\nnúmero de la cuenta) sobre las transferencias de fondos y los mensajes relacionados enviados, y la\\ninformación debe permanecer con la transferencia o mensaje relacionado a través de la cadena de\\npago.\\nLos países deben tomar medidas para asegurar que las instituciones financieras, incluyendo a los\\nque envían dinero, lleven a cabo una investigación más profunda y un control más severo de las\\ntransferencias de fondos sospechosas que no estén acompañadas por una información completa\\nsobre el autor (nombre, dirección y número de la cuenta).\\nVIII. Organizaciones sin fines de lucro.\\nLos países deben revisar la adecuación de las leyes y reglamentos referidas a entidades que\\npueden ser utilizadas indebidamente para la financiación del terrorismo. Las organizaciones sin\\nfines de lucro son particularmente vulnerables y los países deben asegurar que las mismas no sean\\nutilizadas ilegalmente:',\n",
       " '# FILENAME: 9 Recomendaciones Especiales GAFI.pdf | CHARACTERS: 1496 | PAGES: 3-3/3\\n\\n(i) por organizaciones terroristas que aparezcan como entidades legales;\\n(ii) para explotar entidades legales como conducto para la financiación del terrorismo,\\nincluyendo el propósito de evitar las medidas de congelamiento de activos y\\n(iii) para esconder y ocultar el desvío clandestino de fondos destinados a propósitos legales\\nhacia organizaciones terroristas.\\nIX. Correos de efectivo (cash courriers)\\nLos países deberían tener medidas para detectar el transporte físico transfronterizo de dinero en\\nefectivo e instrumentos negociables al portador, incluyendo un sistema de declaración u otra\\nobligación de revelación.\\nLos países deberían asegurarse que sus autoridades competentes tienen la atribución legal para\\ndetener o retener dinero en efectivo e instrumentos negociables al portador que se sospecha están\\nrelacionados con el financiamiento del terrorismo o lavado de activos, o que son falsamente\\ndeclarados o revelados.\\nLos países deberían asegurarse que sanciones efectivas, proporcionadas y disuasivas estén\\ndisponibles para ser aplicadas a las personas que realizan una falsa declaración o revelación. En\\naquellos casos que el dinero en efectivo o los instrumentos negociables al portador estén\\nrelacionados con el financiamiento del terrorismo o lavado de activos, los países también deberían\\nadoptar medidas, incluyendo las legislativas, consistentes con la Recomendación 3 y la\\nRecomendación Especial III, que habilitarían el decomiso de dicho dinero en efectivo o\\ninstrumentos.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a912cd27",
   "metadata": {},
   "source": [
    "### Generar Conversaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ab2c9167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import AsyncClient\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# Definir el esquema Pydantic para la salida estructurada\n",
    "class Message(BaseModel):\n",
    "    role: str = Field(..., pattern=\"^(system|user|assistant)$\")\n",
    "    content: str\n",
    "\n",
    "class Conversation(BaseModel):\n",
    "    messages: List[Message] = Field(..., min_items=MIN_CONVERSATION_LENGTH)\n",
    "    topic: str\n",
    "\n",
    "ollama_client = AsyncClient()\n",
    "\n",
    "async def generate_conversation(fragment, temperature=TEMPERATURE, top_p=TOP_P):\n",
    "    \"\"\"\n",
    "    Genera una conversación estructurada en formato JSON usando Ollama, basada en un fragmento de texto.\n",
    "    Optimizado para múltiples iteraciones en la generación de datasets para fine-tuning.\n",
    "    \"\"\"\n",
    "    if not fragment or len(fragment) < MIN_FRAGMENT_LENGTH:\n",
    "        print(f\"Error: Fragmento demasiado corto ({len(fragment)} caracteres).\")\n",
    "        return None\n",
    "\n",
    "    # Extraer el texto sin la metadata (líneas que comienzan con '#')\n",
    "    fragment_content = '\\n'.join(line for line in fragment.splitlines() if not line.startswith('#')).strip()\n",
    "    if len(fragment_content) < MIN_FRAGMENT_LENGTH:\n",
    "        print(f\"Error: Contenido útil del fragmento demasiado corto ({len(fragment_content)} caracteres).\")\n",
    "        return None\n",
    "\n",
    "    # Prompt optimizado para múltiples iteraciones\n",
    "    prompt = f\"\"\"\n",
    "Basado en el siguiente contexto:\n",
    "<context>\n",
    "{fragment}\n",
    "</context>\n",
    "\n",
    "<instructions>\n",
    "Genera una conversación entre un usuario y un asistente basada **exclusivamente** en el contenido de <context>...</context>. La conversación debe seguir este formato y estilo:\n",
    "\n",
    "1. **Formato de salida**:\n",
    "   - Devuelve un objeto JSON con dos campos:\n",
    "     - \"messages\": lista de objetos con la forma {{ \"role\": \"user\" | \"assistant\", \"content\": \"string\" }}\n",
    "     - \"topic\": cadena corta (máximo 50 caracteres) que resume el tema del fragmento.\n",
    "\n",
    "2. **Estructura de la conversación**:\n",
    "   - Longitud: entre **1 y 2 intercambios** (2 a 4 mensajes).\n",
    "   - Usa UNA de estas dos opciones:\n",
    "     - **Opción A**: 1 intercambio (2 mensajes). Ambos deben ser **largos y detallados**.\n",
    "     - **Opción B**: 2 intercambios (4 mensajes). Cada respuesta debe ser extensa y cubrir bien el contenido.\n",
    "   - cada intercambio (par de mensajes) debe cumplir lo siguiente: empieza con el role: \"user\" y termina con el role: \"assistant\", en otras palabras, formato conversacional cumpliento el formato SFT (Supervices Fine Tunning)\n",
    "\n",
    "3. **Intervenciones del usuario**:\n",
    "   - **No incluyas frases como**:\n",
    "     - “según el texto”, “en el fragmento”, “respecto al documento”, “basado en este artículo”, etc.\n",
    "   - Deben ser naturales, humanas y relevantes.\n",
    "   - Pueden adoptar estas formas:\n",
    "     - Preguntas específicas o inferenciales.\n",
    "     - Frases incompletas para completar.\n",
    "     - Comentarios o afirmaciones parcialmente desarrolladas.\n",
    "   - Las intervenciones deben seguir un flujo lógico y coherente.\n",
    "\n",
    "4. **Respuestas del asistente**:\n",
    "   - Claras, profesionales y bien estructuradas.\n",
    "   - Basadas **únicamente en el contenido del contexto**.\n",
    "   - Extensas, ricas en detalles y sin añadir información inventada.\n",
    "   - Usa un tono conversacional técnico o divulgativo según el tema.\n",
    "\n",
    "5. **Cobertura y contenido**:\n",
    "   - Cubre **todo el contenido** del fragmento.\n",
    "   - No ignores secciones importantes ni temas mencionados.\n",
    "   - Si el texto es poco claro, genera igualmente un solo intercambio lo más plausible posible.\n",
    "\n",
    "6. **Objetivo de esta conversación**:\n",
    "   - Este formato será usado para entrenar modelos mediante *Supervised Fine-Tuning (SFT)* usando `SFTTrainer` de Hugging Face.\n",
    "   - Busca generar conversaciones verosímiles y útiles para entrenar modelos en comprensión lectora, generación coherente y natural.\n",
    "\n",
    "<example>\n",
    "**Fragmento técnico**:\n",
    "PERIODO DE TRANSICIÓN RELATIVO A LA IMPLEMENTACIÓN GENERAL DEL ACUERDO  \n",
    "215. El Nuevo Acuerdo será aplicable a todos los bancos internacionalmente activos en cada nivel del grupo bancario. Aquellos países en los que la subconsolidación no es actualmente un requisito, tendrán un periodo de transición de tres años, a partir de la fecha de implementación, para aplicarla.  \n",
    "\n",
    "PERIODO DE TRANSICIÓN RELATIVO AL MÉTODO FUNDADO EN LA CALIFICACIÓN INTERNA  \n",
    "216. El Comité reconoce que un respeto completo e inmediato de ciertos requisitos relacionados con datos podría no ser posible, particularmente para exposiciones frente a empresas, bancos, soberanos y retail. Para estos casos, se prevé un periodo de transición limitado durante el cual las entidades podrán utilizar aproximaciones alternativas.\n",
    "\n",
    "**Salida esperada**:\n",
    "```json\n",
    "{{\n",
    "  \"messages\": [\n",
    "    {{\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"¿Qué ocurre en los países donde la subconsolidación no es obligatoria y cómo afecta esto a la aplicación del acuerdo?\"\n",
    "    }},\n",
    "    {{\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"En esos países, se establece un periodo de transición de tres años a partir de la implementación del acuerdo. Esto les permite adaptarse gradualmente a los nuevos requisitos regulatorios sin enfrentar impactos abruptos. La subconsolidación es esencial para asegurar una visión completa del riesgo dentro de los grupos bancarios, y este periodo transitorio facilita su adopción progresiva.\"\n",
    "    }},\n",
    "    {{\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"¿Qué flexibilidades se contemplan para las entidades financieras en relación con los datos requeridos por el método basado en calificación interna?\"\n",
    "    }},\n",
    "    {{\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"El texto reconoce que ciertos requisitos de datos pueden ser difíciles de cumplir de inmediato, especialmente en exposiciones a empresas, bancos, gobiernos y clientes minoristas. Por ello, se permite un periodo de transición limitado durante el cual las instituciones pueden aplicar aproximaciones alternativas. Esto proporciona margen para ajustar los sistemas internos y recopilar datos adecuados sin comprometer la supervisión del riesgo.\"\n",
    "    }},\n",
    "    {{\n",
    "      \"role\": \"user\",\n",
    "      \"content\":\"acuerdo 215\"\n",
    "    }},\n",
    "    {{\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\":\"El Nuevo Acuerdo se aplicará a todos los bancos con actividad internacional en cada nivel de su grupo bancario. Los países donde la subconsolidación no sea actualmente obligatoria dispondrán de un período de transición de tres años, a partir de la fecha de implementación, para adoptarla.\"\n",
    "    }}\n",
    "  ],\n",
    "  \"topic\": \"Transición en la implementación del acuerdo\"\n",
    "}}\n",
    "```\n",
    "\n",
    "**Fragmento simple**:\n",
    "El cielo es azul porque las moléculas en la atmósfera dispersan la luz azul del sol en todas direcciones. Esta dispersión se llama dispersión de Rayleigh.\n",
    "\n",
    "**Salida esperada**:\n",
    "```json\n",
    "{{\n",
    "  \"messages\": [\n",
    "    {{\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"¿Por qué el cielo tiene ese color azul tan característico durante el día?\"\n",
    "    }},\n",
    "    {{\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"El color azul del cielo se debe a un fenómeno llamado dispersión de Rayleigh. Cuando la luz del sol entra en la atmósfera terrestre, interactúa con las moléculas de aire. La luz azul, que tiene una longitud de onda más corta, se dispersa mucho más fácilmente que otros colores. Esta dispersión hace que el azul se difunda en todas direcciones, lo cual es lo que vemos desde cualquier punto del planeta durante el día. Por eso el cielo parece predominantemente azul cuando el sol está alto.\"\n",
    "    }}\n",
    "  ],\n",
    "  \"topic\": \"Dispersión de la luz en la atmósfera\"\n",
    "}}\n",
    "```\n",
    "</example>\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Llama a Ollama con roles system y user\n",
    "        response = await ollama_client.chat(\n",
    "            model=OLLAMA_MODEL,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"Eres un experto en el dominio del fragmento proporcionado (por ejemplo, regulaciones financieras, riesgos financieros, finanzas). Genera respuestas claras, concisas y basadas únicamente en el fragmento, manteniendo un tono profesional y conversacional, se especifico y detalla una respuestas en el intercambio de asistente.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            options={\n",
    "                \"temperature\": temperature,\n",
    "                \"top_p\": top_p,\n",
    "            },\n",
    "            format=Conversation.model_json_schema()  # Especifica el esquema JSON\n",
    "        )\n",
    "        conversation = Conversation.model_validate_json(response.message.content)\n",
    "        # Validar que la conversación tenga al menos un intercambio completo\n",
    "        if len(conversation.messages) < 2:\n",
    "            print(f\"Error: Conversación inválida, número de mensajes insuficiente: {len(conversation.messages)}\")\n",
    "            return None\n",
    "        # Si el número de mensajes es impar, eliminar el último (asumiendo que es del usuario)\n",
    "        if len(conversation.messages) % 2 != 0 and len(conversation.messages) > 2:\n",
    "            conversation.messages = conversation.messages[:-1]\n",
    "        return conversation\n",
    "    except Exception as e:\n",
    "        print(f\"Error generando conversación con Ollama: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0a7f8506-c9ff-46cb-971c-4a01f32aac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "async def process_pdf(index, pdf_path, output_dir):\n",
    "    \"\"\"Procesa un PDF, genera conversaciones y las guarda en JSONL.\"\"\"\n",
    "    try:\n",
    "        \n",
    "        pages_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "        if not pages_text:\n",
    "            return\n",
    "    \n",
    "        jsonl_file = os.path.join(output_dir, f\"pdf_{index:04d}.jsonl\")\n",
    "        with open(jsonl_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            tasks = [generate_conversation(fragment) for fragment in pages_text if len(fragment) > 20]\n",
    "    \n",
    "            conversations = await asyncio.gather(*tasks,)\n",
    "            if conversations:\n",
    "                output = [messages.model_dump() for messages in conversations if messages is not None]\n",
    "                output_str = \"\\n\".join(json.dumps(messages, ensure_ascii=False) for messages in output)\n",
    "                f.write(output_str + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error el pdf {pdf_path}: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "29a179d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "await process_pdf(0,files[0],\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "741cdd88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fdbb014-2ca2-4f7f-bdfe-1430423e7d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import asyncio\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def process_pdf_wrapper(args):\n",
    "    index, pdf_path, output_dir = args\n",
    "    # Ejecutar la función asíncrona process_pdf dentro de un bucle de eventos\n",
    "    asyncio.run(process_pdf(index, pdf_path, output_dir))\n",
    "\n",
    "def generate():\n",
    "    pdf_files = files\n",
    "    output_dir=\"outputs\"\n",
    "    output_folder_path=Path(output_dir)\n",
    "    if not os.path.exists(output_folder_path):\n",
    "        os.makedirs(output_folder_path)\n",
    "    \n",
    "    num_workers = int(min(multiprocessing.cpu_count() * 0.8,len(pdf_files)))\n",
    "    with ProcessPoolExecutor(max_workers=8) as executor:\n",
    "        list(tqdm(executor.map(process_pdf_wrapper, [(i, p, output_dir) for i, p in enumerate(pdf_files)]), total=len(pdf_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dfe45b6-1566-44ee-bd25-001e9f76ea39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/89 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed eval>:1\u001b[0m\n",
      "Cell \u001b[1;32mIn[18], line 22\u001b[0m, in \u001b[0;36mgenerate\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m num_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mmin\u001b[39m(multiprocessing\u001b[38;5;241m.\u001b[39mcpu_count() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.8\u001b[39m,\u001b[38;5;28mlen\u001b[39m(pdf_files)))\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProcessPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_pdf_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpdf_files\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpdf_files\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Cesar Burbano\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Cesar Burbano\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\process.py:567\u001b[0m, in \u001b[0;36m_chain_from_iterable_of_lists\u001b[1;34m(iterable)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[0;32m    562\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;124;03m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;124;03m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 567\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m    568\u001b[0m         element\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[0;32m    569\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m element:\n",
      "File \u001b[1;32mc:\\Users\\Cesar Burbano\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:608\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[0;32m    606\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 608\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    610\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mresult(end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[1;32mc:\\Users\\Cesar Burbano\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:445\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\Cesar Burbano\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    392\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a4ebf-86b7-4ce0-a89a-975ee4a86092",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1685376-51e6-4287-a74e-26a1e3deffb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4bf20d9d5d429a87a1522c0622cab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37cc3b1f5a8d41188412819e28b55e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/96 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b78dc506ac400fbfcf72115ca01365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages', 'topic'],\n",
       "        num_rows: 664\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"./outputs/*.jsonl\")\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa783a63-83da-4cd0-97a3-dc50d15df2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0302c5a93ee748c19fce35827aaa637e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "151e67bb-0111-4991-b87d-e724b183ea9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f927e39d7bc4dfc9d55557c21ac7edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2593e683346a4d229b5e9c6d5c0481b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fdb167115d14860a59947184dcab9b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0569c44f34e6422d9c78ce1f9e5eeba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1d7e93c0804ab8aab28fb1266739cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                        : 100%|##########|  270kB /  270kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2027803f4d423bb6cb7c5ceb339b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/376 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/jeanmcm/b_risks/commit/13e6a6ad8f5d4eb1efc3bf36fe4e0b58c6e237fb', commit_message='Upload dataset', commit_description='', oid='13e6a6ad8f5d4eb1efc3bf36fe4e0b58c6e237fb', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/jeanmcm/b_risks', endpoint='https://huggingface.co', repo_type='dataset', repo_id='jeanmcm/b_risks'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.push_to_hub(\"jeanmcm/b_risks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f0891",
   "metadata": {},
   "source": [
    "# Testing RAG with Open WebUI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01ce0bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Respuesta en streaming:</b> El riesgo financiero se refiere a las posibles pérdidas o daños económicos que pueden afectar a una empresa, individuo u organización debido a sus actividades financieras. Esto puede incluir la incertidumbre sobre los rendimientos de los activos financieros, el valor de los bienes y servicios, o la capacidad de un individuo o organización para generar ingresos.\n",
       "\n",
       "En el contexto del sistema financiero, las instituciones deben conocer a fondo las características particulares de las actividades económicas de sus clientes y las operaciones que realizan para identificar y gestionar los riesgos financieros asociados. Esto es fundamental para prevenir la ocurrencia de incumplimientos en las regulaciones financieras y minimizar el impacto de cualquier pérdida o daño.\n",
       "\n",
       "Según el artículo 43 del \"Bsoft Documents\", las instituciones del sistema financiero deben contar con una unidad de cumplimiento dirigida por un oficial de cumplimiento, que debe tener formación profesional en áreas como administración, contaduría, derecho o economía. Este equipo es responsable de garantizar que las políticas y procedimientos financieros sean adecuados y cumplan con los requisitos legales.\n",
       "\n",
       "Por lo tanto, el riesgo financiero se considera un aspecto crítico para las instituciones del sistema financiero, ya que puede tener graves consecuencias en su estabilidad y solvencia. Es importante que estas entidades tomen medidas proactivas para identificar, evaluar y gestionar los riesgos financieros asociados con sus actividades y operaciones.\n",
       "\n",
       "No se incluye ninguna cita porque no hay un id atributo en el source tag correspondiente a esta información, pero la respuesta se basa en el contexto proporcionado.."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "from IPython.display import display, clear_output, HTML\n",
    "\n",
    "url = \"https://vwlppjjfa98c9x-8080.proxy.runpod.net\"\n",
    "api_key =\"sk-05568562f28844fe997cadf960a346cd\"\n",
    "\n",
    "messages =  [{\"role\": \"user\", \"content\": \"Que es el Riesgo Financiero?\"}]\n",
    "\n",
    "try:\n",
    "    # Realizar la solicitud con stream=True\n",
    "    with requests.post(f\"{url}/api/chat/completions\", stream=True,headers={\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"Authorization\": f\"Bearer {api_key}\"\n",
    "    },json={\n",
    "      \"model\":\"bosft-riesgos-rag-model\",\n",
    "      \"messages\":messages,\n",
    "      \"stream\":True\n",
    "      }) as response:\n",
    "        response.raise_for_status()\n",
    "        # Variable para almacenar la salida acumulada\n",
    "        accumulated_output = \"\"\n",
    "\n",
    "        # Iterar sobre las líneas de la respuesta\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                # Decodificar la línea\n",
    "                decoded_line = line.decode('utf-8').strip()\n",
    "                # Si la línea comienza con \"data:\", extraer el contenido\n",
    "                if decoded_line.startswith(\"data:\"):\n",
    "                    decoded_line = decoded_line[5:].strip()  # Quitar \"data: \"\n",
    "\n",
    "                # Ignorar líneas vacías o marcadores de fin como \"[DONE]\"\n",
    "                if not decoded_line or decoded_line == \"[DONE]\" :\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    # Parsear si es JSON\n",
    "                    data = json.loads(decoded_line)\n",
    "                    if \"choices\" not in data: continue\n",
    "                    \n",
    "                    delta = data['choices'][0]['delta']\n",
    "                    if \"content\" in delta: new_data = delta['content']\n",
    "                except json.JSONDecodeError:\n",
    "                    # Si no es JSON, usar la línea como texto\n",
    "                    new_data = decoded_line\n",
    "\n",
    "                # Acumular y mostrar la salida dinámicamente\n",
    "                if new_data:\n",
    "                    accumulated_output += new_data\n",
    "                    # Limpiar la salida anterior y mostrar la nueva\n",
    "                    clear_output(wait=True)\n",
    "                    display(HTML(f\"<b>Respuesta en streaming:</b> {accumulated_output}\"))\n",
    "                    time.sleep(0.1)  # Pequeña pausa para visibilidad\n",
    "                    \n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"\\nError en la solicitud: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e64806a-ac56-417a-8dd9-175c3a8447a1",
   "metadata": {},
   "source": [
    "# Testing with Flowise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797cb1c0-bb15-4c97-9e0c-0156e8b90df1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
