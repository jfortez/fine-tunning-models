{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "067529bb",
   "metadata": {},
   "source": [
    "# Generacion de Datasets utilizando RLHF para fine tunning\n",
    "\n",
    "## Procedimiento\n",
    "\n",
    "- Recoleccion de Datos: Obtener Informacion mediante documentos de textos seleccionables (no escaneados), mediante web, redes sociales, scrapping,etc\n",
    "- Extraer Texto: Usa PyMuPDF para extraer texto de PDFs no escaneados, procesando en paralelo con ProcessPoolExecutor para manejar grandes volúmenes (>100, >10,000).\n",
    "- Limpiar Datos: Elimina espacios múltiples y caracteres no deseados con expresiones regulares, asegurando texto coherente.\n",
    "- Validar: Verifica que los fragmentos extraídos no estén vacíos y tengan una longitud mínima (por ejemplo, 50 caracteres).\n",
    "- Generar Conversaciones: Divide el texto en párrafos, usa un modelo como meta-llama/Llama-3.2-1B-Instruct para generar diálogos conversacionales específicos al contenido, con al menos 4 intercambios por conversación, en formato {\"messages\": [{\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}], \"topic\": \"...\"}.\n",
    "- Estructurar Dataset: Guarda las conversaciones en archivos JSONL con dos columnas: \"messages\" y \"topic\".\n",
    "Cargar y Subir: Carga el dataset con load_dataset(\"json\", data_files=\"path/*.jsonl\") y súbelo a un repositorio privado en Hugging Face Hub con push_to_hub(\"username/dataset_name\", private=True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fa351d6-c527-423f-9d2e-417d0ef2b8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "45295ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in ./venv/lib/python3.11/site-packages (1.26.3)\n",
      "Requirement already satisfied: nltk in ./venv/lib/python3.11/site-packages (3.9.1)\n",
      "Collecting ollama\n",
      "  Downloading ollama-0.5.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.11/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.11/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./venv/lib/python3.11/site-packages (from nltk) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: httpx>=0.27 in ./venv/lib/python3.11/site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in ./venv/lib/python3.11/site-packages (from ollama) (2.11.7)\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.11/site-packages (from httpx>=0.27->ollama) (4.9.0)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.11/site-packages (from httpx>=0.27->ollama) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.11/site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in ./venv/lib/python3.11/site-packages (from httpx>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.11/site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./venv/lib/python3.11/site-packages (from pydantic>=2.9->ollama) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./venv/lib/python3.11/site-packages (from pydantic>=2.9->ollama) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./venv/lib/python3.11/site-packages (from pydantic>=2.9->ollama) (0.4.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./venv/lib/python3.11/site-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
      "Downloading ollama-0.5.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: ollama\n",
      "Successfully installed ollama\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pymupdf nltk ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "4760f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "NUM_WORKERS = min(os.cpu_count(), 8)  # Máximo de 8 trabajadores para evitar sobrecarga\n",
    "BATCH_SIZE = 10  # Fragmentos por trabajador\n",
    "MAX_LENGTH = 500  # Longitud máxima de la generación\n",
    "TEMPERATURE = 0.7  # Equilibrio entre creatividad y coherencia\n",
    "TOP_P = 0.9  # Filtrado de núcleo para diversidad\n",
    "NUM_RETURN_SEQUENCES = 1  # Una secuencia por fragmento\n",
    "MIN_FRAGMENT_LENGTH = 50  # Longitud mínima del fragmento\n",
    "MIN_CONVERSATION_LENGTH = 4  # Mínimo 4 mensajes por conversación\n",
    "MAX_JSONL_SIZE = 1000  # Máximo de entradas por archivo JSONL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62be033f",
   "metadata": {},
   "source": [
    "### Recoleccion de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6b45b0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Folder\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Verificar si la carpeta existe\n",
    "\n",
    "folder_url = \"/workspace/data/uploads\"\n",
    "folder = Path(folder_url)\n",
    "\n",
    "\n",
    "if folder.exists() and folder.is_dir():\n",
    "    print(\"Valid Folder\")\n",
    "    \n",
    "# Obtener todos los archivos de la carpeta\n",
    "files = [f for f in folder.rglob(\"*\") if f.is_file()]\n",
    "\n",
    "files\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce4f060",
   "metadata": {},
   "source": [
    "### Extraccion de Texto y Limpieza de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4ed080bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import re\n",
    "\n",
    "output_folder= \"extracted\"\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Limpia el texto extraído eliminando espacios múltiples y caracteres no deseados.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Reemplaza múltiples espacios por uno solo\n",
    "    text = re.sub(r'[^\\x20-\\x7E]', '', text)  # Elimina caracteres no imprimibles\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "NORMALIZE_TEXT = False\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extrae texto de cada página de un PDF y valida que no esté vacío.\"\"\"\n",
    "    try:\n",
    "        doc = pymupdf.open(pdf_path)\n",
    "        pages_text = []\n",
    "        for page in doc:\n",
    "            text = page.get_text()\n",
    "            cleaned_text =  clean_text(text) if NORMALIZE_TEXT else  text\n",
    "            if cleaned_text and len(cleaned_text) >= MIN_FRAGMENT_LENGTH:\n",
    "                pages_text.append(cleaned_text)\n",
    "        doc.close()\n",
    "        return pages_text if pages_text else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {pdf_path}: {e}\")\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4a686020-8ff9-4bc6-80c9-9ba941c3d079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Comité de Supervisión\\nBancaria de Basilea\\nDocumento Consultivo\\nVisión General del Nuevo\\nAcuerdo de Capital de\\nBasilea\\nEmitido para consulta hasta el 31 de mayo de 2001\\nTraducción realizada por la Asociación de Supervisores Bancarios de\\nlas Américas (ASBA). En caso de dudas consultar el texto original en\\ninglés\\nEnero 2001\\n'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = extract_text_from_pdf(files[0])\n",
    "\n",
    "chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a912cd27",
   "metadata": {},
   "source": [
    "### Generar Conversaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "60c5b856-d8ec-4f37-8bf6-64bb8c39a75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "# Descarga recursos de NLTK para segmentación\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ab2c9167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "import ollama\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "OLLAMA_MODEL=\"llama3.1:8b\"\n",
    "\n",
    "# Definir el esquema Pydantic para la salida estructurada\n",
    "class Message(BaseModel):\n",
    "    role: str = Field(..., pattern=\"^(user|assistant)$\")\n",
    "    content: str\n",
    "\n",
    "class Conversation(BaseModel):\n",
    "    messages: List[Message] = Field(..., min_items=MIN_CONVERSATION_LENGTH)\n",
    "    topic: str\n",
    "\n",
    "\n",
    "def generate_conversation(fragment):\n",
    "    \"\"\"Genera una conversación estructurada en formato JSON usando Ollama con el parámetro format.\"\"\"\n",
    "    if not fragment or len(fragment) < MIN_FRAGMENT_LENGTH:\n",
    "        return None\n",
    "\n",
    "    # Divide el fragmento en oraciones\n",
    "    sentences = sent_tokenize(fragment)\n",
    "    if not sentences:\n",
    "        return None\n",
    "\n",
    "    # Usa la primera oración como tema inicial\n",
    "    topic = sentences[0][:50] + \"...\" if len(sentences[0]) > 50 else sentences[0]\n",
    "\n",
    "    # Prompt para guiar la generación\n",
    "    prompt = f\"\"\"\n",
    "    Basándote únicamente en el siguiente texto:\n",
    "    \n",
    "    '{fragment[:500]}'\n",
    "    \n",
    "    Crea una conversación entre un usuario y un asistente con el siguiente comportamiento:\n",
    "    \n",
    "    1. La conversación debe tener entre 2 y 8 intercambios, dependiendo de la cantidad de contexto útil disponible en el texto. Si el texto tiene poco contexto, limita la conversación a solo 1 intercambio (usuario + asistente). Si es más informativo, genera una conversación de entre 4 a 8 mensajes.\n",
    "    2. Las intervenciones del usuario pueden incluir:\n",
    "        - Preguntas específicas y relevantes sobre el contenido (no genéricas como “¿Cuál es el tema del texto?” ni “¿Qué organización se menciona?”).\n",
    "        - Frases incompletas o afirmaciones a medio terminar para que el asistente las complete, como: “El fenómeno descrito es...” o “La causa principal de esto es...”.\n",
    "    3. Las respuestas del asistente deben ser claras, coherentes y basarse únicamente en el contenido del texto.\n",
    "    4. El hilo de la conversación debe seguir una lógica progresiva: cada mensaje debe conectar naturalmente con el anterior.\n",
    "    5. Identifica el **tema principal** del texto y agrégalo como valor del campo `topic`.\n",
    "    \n",
    "    Este resultado está orientado a la **generación de datasets para aprendizaje por refuerzo** (Reinforcement Learning) o aprendizaje supervisado. Por tanto:\n",
    "    - La conversación debe reflejar interacciones verosímiles y útiles entre humanos y asistentes conversacionales.\n",
    "    - Debe permitir entrenar modelos que comprendan el contexto, anticipen respuestas coherentes y mantengan un flujo natural.\n",
    "    \n",
    "    Ejemplo de interacción válida:\n",
    "    \n",
    "    user: \"El experimento de dispersión muestra que...\"\n",
    "    \n",
    "    assistant: \"…la luz blanca se divide en varios colores debido a su paso por un prisma.\"\n",
    "    \n",
    "    Otro ejemplo:\n",
    "    \n",
    "    user: \"¿Por qué se utilizan prismas en el experimento?\"\n",
    "    \n",
    "    assistant: \"Porque permiten observar cómo se separan los diferentes componentes del espectro de luz blanca.\"\n",
    "    \n",
    "    Asegúrate de que las preguntas no sean genéricas, que la conversación tenga coherencia, y que se respete el formato.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Llama a Ollama con el esquema Pydantic\n",
    "        response = ollama.chat(\n",
    "            model=OLLAMA_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            options={\n",
    "                \"temperature\": TEMPERATURE,\n",
    "                \"top_p\": TOP_P,\n",
    "            },\n",
    "            format=Conversation.model_json_schema()  # Especifica el esquema JSON\n",
    "        )\n",
    "        return Conversation.model_validate_json(response.message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generando conversación con Ollama: {e}\")\n",
    "        # Fallback en caso de error\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0a7f8506-c9ff-46cb-971c-4a01f32aac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def process_pdf(index, pdf_path, output_dir):\n",
    "    \"\"\"Procesa un PDF, genera conversaciones y las guarda en JSONL.\"\"\"\n",
    "    pages_text = extract_text_from_pdf(pdf_path)\n",
    "    if not pages_text:\n",
    "        return\n",
    "\n",
    "    jsonl_file = os.path.join(output_dir, f\"pdf_{index:04d}.jsonl\")\n",
    "    with open(jsonl_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for page_text in pages_text:\n",
    "            # Divide el texto de la página en fragmentos (párrafos)\n",
    "            fragments = page_text.split('\\n\\n')  # Asume que los párrafos están separados por líneas en blanco\n",
    "            for fragment in fragments:\n",
    "                if len(fragment) > 20:  # Ignora fragmentos cortos\n",
    "                    conversation = generate_conversation(fragment)\n",
    "                    if conversation:\n",
    "                        f.write(json.dumps(conversation.model_dump(), ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "2e73b0c6-b672-4061-8383-1cbd14d7c073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdbb014-2ca2-4f7f-bdfe-1430423e7d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 5/5 [12:29<00:00, 149.82s/it]\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "pdf_files = files[:5]\n",
    "output_dir=\"outputs\"\n",
    "\n",
    "num_workers = int(min(multiprocessing.cpu_count() * 0.8,len(pdf_files)))\n",
    "\n",
    "def process_pdf_wrapper(args):\n",
    "    index, pdf_path, output_dir = args\n",
    "    process_pdf(index, pdf_path, output_dir)\n",
    "\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "    list(tqdm(executor.map(process_pdf_wrapper, [(i, p, output_dir) for i, p in enumerate(pdf_files)]), total=len(pdf_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a4ebf-86b7-4ce0-a89a-975ee4a86092",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b1685376-51e6-4287-a74e-26a1e3deffb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a41c61b0befc400e996a8d1672d64e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages', 'topic'],\n",
       "        num_rows: 225\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"./outputs/*.jsonl\")\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "aa783a63-83da-4cd0-97a3-dc50d15df2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90858f0527d549c494b10c780bf963c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5fc025448fc41e0a0a5f4b08e9fa93a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31a1c882b554d1599c43c7a5926a313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f9067858284e34952dd65217ad5d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d26cf071d049aaa820e79f306f77c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e0281287fa4e26ac9bb4f6b0437180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                        : 100%|##########| 71.7kB / 71.7kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/jeanmcm/b_risks/commit/a534f757b5961e3c0f354726c09c8b960018bd7e', commit_message='Upload dataset', commit_description='', oid='a534f757b5961e3c0f354726c09c8b960018bd7e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/jeanmcm/b_risks', endpoint='https://huggingface.co', repo_type='dataset', repo_id='jeanmcm/b_risks'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2a5f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.push_to_hub(\"jeanmcm/b_risks\", private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f0891",
   "metadata": {},
   "source": [
    "# Testing RAG with Open WebUI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01ce0bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Respuesta en streaming:</b> El riesgo financiero se refiere a las posibles pérdidas o daños económicos que pueden afectar a una empresa, individuo u organización debido a sus actividades financieras. Esto puede incluir la incertidumbre sobre los rendimientos de los activos financieros, el valor de los bienes y servicios, o la capacidad de un individuo o organización para generar ingresos.\n",
       "\n",
       "En el contexto del sistema financiero, las instituciones deben conocer a fondo las características particulares de las actividades económicas de sus clientes y las operaciones que realizan para identificar y gestionar los riesgos financieros asociados. Esto es fundamental para prevenir la ocurrencia de incumplimientos en las regulaciones financieras y minimizar el impacto de cualquier pérdida o daño.\n",
       "\n",
       "Según el artículo 43 del \"Bsoft Documents\", las instituciones del sistema financiero deben contar con una unidad de cumplimiento dirigida por un oficial de cumplimiento, que debe tener formación profesional en áreas como administración, contaduría, derecho o economía. Este equipo es responsable de garantizar que las políticas y procedimientos financieros sean adecuados y cumplan con los requisitos legales.\n",
       "\n",
       "Por lo tanto, el riesgo financiero se considera un aspecto crítico para las instituciones del sistema financiero, ya que puede tener graves consecuencias en su estabilidad y solvencia. Es importante que estas entidades tomen medidas proactivas para identificar, evaluar y gestionar los riesgos financieros asociados con sus actividades y operaciones.\n",
       "\n",
       "No se incluye ninguna cita porque no hay un id atributo en el source tag correspondiente a esta información, pero la respuesta se basa en el contexto proporcionado.."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "from IPython.display import display, clear_output, HTML\n",
    "\n",
    "url = \"https://vwlppjjfa98c9x-8080.proxy.runpod.net\"\n",
    "api_key =\"sk-05568562f28844fe997cadf960a346cd\"\n",
    "\n",
    "messages =  [{\"role\": \"user\", \"content\": \"Que es el Riesgo Financiero?\"}]\n",
    "\n",
    "try:\n",
    "    # Realizar la solicitud con stream=True\n",
    "    with requests.post(f\"{url}/api/chat/completions\", stream=True,headers={\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"Authorization\": f\"Bearer {api_key}\"\n",
    "    },json={\n",
    "      \"model\":\"bosft-riesgos-rag-model\",\n",
    "      \"messages\":messages,\n",
    "      \"stream\":True\n",
    "      }) as response:\n",
    "        response.raise_for_status()\n",
    "        # Variable para almacenar la salida acumulada\n",
    "        accumulated_output = \"\"\n",
    "\n",
    "        # Iterar sobre las líneas de la respuesta\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                # Decodificar la línea\n",
    "                decoded_line = line.decode('utf-8').strip()\n",
    "                # Si la línea comienza con \"data:\", extraer el contenido\n",
    "                if decoded_line.startswith(\"data:\"):\n",
    "                    decoded_line = decoded_line[5:].strip()  # Quitar \"data: \"\n",
    "\n",
    "                # Ignorar líneas vacías o marcadores de fin como \"[DONE]\"\n",
    "                if not decoded_line or decoded_line == \"[DONE]\" :\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    # Parsear si es JSON\n",
    "                    data = json.loads(decoded_line)\n",
    "                    if \"choices\" not in data: continue\n",
    "                    \n",
    "                    delta = data['choices'][0]['delta']\n",
    "                    if \"content\" in delta: new_data = delta['content']\n",
    "                except json.JSONDecodeError:\n",
    "                    # Si no es JSON, usar la línea como texto\n",
    "                    new_data = decoded_line\n",
    "\n",
    "                # Acumular y mostrar la salida dinámicamente\n",
    "                if new_data:\n",
    "                    accumulated_output += new_data\n",
    "                    # Limpiar la salida anterior y mostrar la nueva\n",
    "                    clear_output(wait=True)\n",
    "                    display(HTML(f\"<b>Respuesta en streaming:</b> {accumulated_output}\"))\n",
    "                    time.sleep(0.1)  # Pequeña pausa para visibilidad\n",
    "                    \n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"\\nError en la solicitud: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e64806a-ac56-417a-8dd9-175c3a8447a1",
   "metadata": {},
   "source": [
    "# Testing with Flowise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fc0618-06dd-4591-b5d7-9e7fa560a99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
